"""
ç®€åŒ–æµ‹è¯•ï¼šåªæµ‹è¯•è§†è§‰ç¼–ç å’Œæ—¶åºå»ºæ¨¡éƒ¨åˆ†
ç»•è¿‡æ–‡æœ¬ç¼–ç å™¨çš„å¤æ‚åˆå§‹åŒ–
"""
import torch
import sys
sys.path.insert(0, '/data/zyy/wsvad/2026CVPR/FG-CLIP')

def test_temporal_modules():
    """æµ‹è¯•æ—¶åºæ¨¡å—æ˜¯å¦æ­£ç¡®æ·»åŠ """
    print("=" * 60)
    print("æµ‹è¯•1: æ—¶åºæ¨¡å—åˆå§‹åŒ–")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    text_config = CLIPTextConfig()
    vision_config = CLIPVisionConfig()
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config)
    
    # æ£€æŸ¥æ—¶åºæ¨¡å—
    assert hasattr(model, 'temporal_transformer'), "âŒ ç¼ºå°‘ temporal_transformer"
    assert hasattr(model, 'temporal_attention'), "âŒ ç¼ºå°‘ temporal_attention"
    
    print("âœ… æ—¶åºæ¨¡å—å·²æ­£ç¡®æ·»åŠ :")
    print(f"   - temporal_transformer: 2å±‚TransformerEncoder, 8å¤´æ³¨æ„åŠ›")
    print(f"   - temporal_attention: æ³¨æ„åŠ›åŠ æƒæ¨¡å—")
    print()
    return True


def test_vision_encoding_video():
    """æµ‹è¯•è§†è§‰ç¼–ç å™¨å¯¹è§†é¢‘è¾“å…¥çš„å¤„ç†"""
    print("=" * 60)
    print("æµ‹è¯•2: è§†é¢‘è§†è§‰ç¼–ç ï¼ˆé€å¸§ï¼‰")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    text_config = CLIPTextConfig()
    vision_config = CLIPVisionConfig()
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config).eval()
    
    # æ¨¡æ‹Ÿè§†é¢‘è¾“å…¥
    bs, T, C, H, W = 2, 8, 3, 224, 224
    video = torch.randn(bs, T, C, H, W)
    
    # å±•å¹³ä¸ºé€å¸§
    video_flat = video.view(bs * T, C, H, W)
    
    print(f"è§†é¢‘è¾“å…¥: {video.shape}")
    print(f"å±•å¹³å: {video_flat.shape}")
    
    try:
        with torch.no_grad():
            # æµ‹è¯•vision_model
            vision_outputs = model.vision_model(pixel_values=video_flat)
            pooled_output = vision_outputs[1]  # (B*T, D_vision)
            
            # æµ‹è¯•æŠ•å½±
            image_features = model.visual_projection(pooled_output)  # (B*T, D_proj)
            
            # é‡å¡‘ä¸º (B, T, D)
            image_features = image_features.view(bs, T, -1)
            
            print(f"âœ… è§†è§‰ç¼–ç æˆåŠŸ")
            print(f"   - è¾“å‡ºå½¢çŠ¶: {image_features.shape}")
            print(f"   - æ¯å¸§ç‰¹å¾ç»´åº¦: {image_features.shape[-1]}")
            print()
            return True, image_features
    except Exception as e:
        print(f"âŒ è§†è§‰ç¼–ç å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, None


def test_temporal_transformer(image_features):
    """æµ‹è¯•æ—¶åºTransformer"""
    print("=" * 60)
    print("æµ‹è¯•3: æ—¶åºTransformer")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    text_config = CLIPTextConfig()
    vision_config = CLIPVisionConfig()
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config).eval()
    
    print(f"è¾“å…¥ç‰¹å¾: {image_features.shape}")
    
    try:
        with torch.no_grad():
            # æµ‹è¯•Transformerç¼–ç 
            temporal_features = model.temporal_transformer(image_features)
            
            print(f"âœ… æ—¶åºTransformeræˆåŠŸ")
            print(f"   - è¾“å‡ºå½¢çŠ¶: {temporal_features.shape}")
            print()
            return True, temporal_features
    except Exception as e:
        print(f"âŒ æ—¶åºTransformerå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, None


def test_attention_pooling(temporal_features):
    """æµ‹è¯•æ³¨æ„åŠ›åŠ æƒæ± åŒ–"""
    print("=" * 60)
    print("æµ‹è¯•4: æ³¨æ„åŠ›åŠ æƒæ± åŒ–")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    text_config = CLIPTextConfig()
    vision_config = CLIPVisionConfig()
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config).eval()
    
    print(f"è¾“å…¥ç‰¹å¾: {temporal_features.shape}")
    
    try:
        with torch.no_grad():
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attn_weights = model.temporal_attention(temporal_features)  # (B, T, 1)
            attn_weights = torch.softmax(attn_weights, dim=1)
            
            # åŠ æƒèšåˆ
            video_features = (temporal_features * attn_weights).sum(dim=1)  # (B, D)
            
            print(f"âœ… æ³¨æ„åŠ›åŠ æƒæ± åŒ–æˆåŠŸ")
            print(f"   - æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
            print(f"   - è§†é¢‘ç‰¹å¾å½¢çŠ¶: {video_features.shape}")
            print(f"   - ç¬¬ä¸€ä¸ªè§†é¢‘çš„æ³¨æ„åŠ›æƒé‡: {attn_weights[0].squeeze().tolist()}")
            print()
            return True
    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›åŠ æƒæ± åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_with_attention_mask():
    """æµ‹è¯•å¸¦attention_maskçš„æ—¶åºå»ºæ¨¡"""
    print("=" * 60)
    print("æµ‹è¯•5: å¸¦Attention Maskçš„æ—¶åºå»ºæ¨¡")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    text_config = CLIPTextConfig()
    vision_config = CLIPVisionConfig()
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config).eval()
    
    # æ¨¡æ‹Ÿå¡«å……è§†é¢‘
    bs, T = 2, 8
    features = torch.randn(bs, T, 512)
    
    # ç¬¬ä¸€ä¸ªè§†é¢‘5å¸§ï¼Œç¬¬äºŒä¸ª8å¸§
    mask = torch.zeros(bs, T, dtype=torch.bool)
    mask[0, :5] = True
    mask[1, :] = True
    
    print(f"ç‰¹å¾å½¢çŠ¶: {features.shape}")
    print(f"Mask: Video0={mask[0].sum().item()}å¸§, Video1={mask[1].sum().item()}å¸§")
    
    try:
        with torch.no_grad():
            # Transformerå¤„ç†
            temporal_mask = ~mask
            temporal_features = model.temporal_transformer(
                features,
                src_key_padding_mask=temporal_mask
            )
            
            # æ³¨æ„åŠ›åŠ æƒ
            attn_weights = model.temporal_attention(temporal_features)
            attn_weights = attn_weights.masked_fill(~mask.unsqueeze(-1), -1e9)
            attn_weights = torch.softmax(attn_weights, dim=1)
            
            # èšåˆ
            video_features = (temporal_features * attn_weights).sum(dim=1)
            
            print(f"âœ… å¸¦Maskçš„æ—¶åºå»ºæ¨¡æˆåŠŸ")
            print(f"   - è¾“å‡ºå½¢çŠ¶: {video_features.shape}")
            print(f"   - Video0æ³¨æ„åŠ›æƒé‡: {attn_weights[0].squeeze().tolist()}")
            print(f"   - Video1æ³¨æ„åŠ›æƒé‡: {attn_weights[1].squeeze().tolist()}")
            print()
            return True
    except Exception as e:
        print(f"âŒ å¸¦Maskçš„æ—¶åºå»ºæ¨¡å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("è§†é¢‘æ—¶åºå»ºæ¨¡ç®€åŒ–æµ‹è¯•")
    print("=" * 60 + "\n")
    
    results = []
    
    # æµ‹è¯•1: æ¨¡å—åˆå§‹åŒ–
    results.append(("æ—¶åºæ¨¡å—åˆå§‹åŒ–", test_temporal_modules()))
    
    # æµ‹è¯•2: è§†è§‰ç¼–ç 
    success, image_features = test_vision_encoding_video()
    results.append(("è§†é¢‘è§†è§‰ç¼–ç ", success))
    
    if success and image_features is not None:
        # æµ‹è¯•3: æ—¶åºTransformer
        success, temporal_features = test_temporal_transformer(image_features)
        results.append(("æ—¶åºTransformer", success))
        
        if success and temporal_features is not None:
            # æµ‹è¯•4: æ³¨æ„åŠ›æ± åŒ–
            results.append(("æ³¨æ„åŠ›åŠ æƒæ± åŒ–", test_attention_pooling(temporal_features)))
    
    # æµ‹è¯•5: å¸¦Maskçš„å¤„ç†
    results.append(("Attention Maskå¤„ç†", test_with_attention_mask()))
    
    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:<25}: {status}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\næ ¸å¿ƒåŠŸèƒ½éªŒè¯ï¼š")
        print("  âœ“ æ—¶åºTransformeræ¨¡å—æ­£ç¡®æ·»åŠ ")
        print("  âœ“ è§†é¢‘é€å¸§ç¼–ç åŠŸèƒ½æ­£å¸¸")
        print("  âœ“ æ—¶åºå»ºæ¨¡æ•æ‰å¸§é—´å…³ç³»")
        print("  âœ“ æ³¨æ„åŠ›åŠ æƒæ± åŒ–å·¥ä½œæ­£å¸¸")
        print("  âœ“ Attention Maskæ­£ç¡®å¤„ç†å¡«å……å¸§")
        print("\nâœ¨ æ¨¡å‹å·²æˆåŠŸæ”¯æŒè§†é¢‘è¾“å…¥å’Œæ—¶åºå»ºæ¨¡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    print("=" * 60 + "\n")
