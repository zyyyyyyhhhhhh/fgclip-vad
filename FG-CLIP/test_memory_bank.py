#!/usr/bin/env python3
"""
æµ‹è¯•Memory BankåŠŸèƒ½
éªŒè¯ï¼š
1. Memory Bankåˆå§‹åŒ–
2. é˜Ÿåˆ—æ›´æ–°é€»è¾‘
3. Regionå¯¹æ¯”å­¦ä¹ ç»´åº¦åŒ¹é…
"""

import torch
import torch.nn.functional as F

def test_memory_bank_logic():
    """æµ‹è¯•Memory Bankçš„æ ¸å¿ƒé€»è¾‘"""
    print("="*80)
    print("æµ‹è¯•Memory Bankæ ¸å¿ƒé€»è¾‘")
    print("="*80)
    
    # æ¨¡æ‹Ÿå‚æ•°
    projection_dim = 512
    memory_bank_size = 128
    batch_size = 4  # æ¨¡æ‹Ÿ4ä¸ªæœ‰æ•ˆregions
    
    # åˆå§‹åŒ–é˜Ÿåˆ—
    region_image_queue = torch.randn(projection_dim, memory_bank_size)
    region_text_queue = torch.randn(projection_dim, memory_bank_size)
    region_image_queue = F.normalize(region_image_queue, dim=0)
    region_text_queue = F.normalize(region_text_queue, dim=0)
    queue_ptr = torch.zeros(1, dtype=torch.long)
    queue_is_full = torch.zeros(1, dtype=torch.bool)
    
    print(f"âœ“ é˜Ÿåˆ—åˆå§‹åŒ–æˆåŠŸ")
    print(f"  - Image Queue: {region_image_queue.shape}")
    print(f"  - Text Queue: {region_text_queue.shape}")
    print(f"  - Queue Ptr: {queue_ptr.item()}")
    
    # æ¨¡æ‹Ÿå¤šä¸ªbatchçš„æ›´æ–°
    for i in range(3):
        # ç”Ÿæˆå½“å‰batchçš„ç‰¹å¾
        bbox_image_embeds = torch.randn(batch_size, projection_dim)
        bbox_text_embeds = torch.randn(batch_size, projection_dim)
        bbox_image_embeds = F.normalize(bbox_image_embeds, dim=1)
        bbox_text_embeds = F.normalize(bbox_text_embeds, dim=1)
        
        # æ›´æ–°é˜Ÿåˆ—
        ptr = int(queue_ptr)
        if ptr + batch_size <= memory_bank_size:
            region_image_queue[:, ptr:ptr + batch_size] = bbox_image_embeds.T
            region_text_queue[:, ptr:ptr + batch_size] = bbox_text_embeds.T
        else:
            remain_space = memory_bank_size - ptr
            region_image_queue[:, ptr:] = bbox_image_embeds[:remain_space].T
            region_text_queue[:, ptr:] = bbox_text_embeds[:remain_space].T
            
            overflow_size = batch_size - remain_space
            region_image_queue[:, :overflow_size] = bbox_image_embeds[remain_space:].T
            region_text_queue[:, :overflow_size] = bbox_text_embeds[remain_space:].T
        
        ptr = (ptr + batch_size) % memory_bank_size
        queue_ptr[0] = ptr
        
        if not queue_is_full and ptr < batch_size:
            queue_is_full[0] = True
        
        print(f"\nâœ“ Batch {i+1} æ›´æ–°å®Œæˆ")
        print(f"  - Batch Size: {batch_size}")
        print(f"  - Queue Ptr: {queue_ptr.item()}")
        print(f"  - Queue Full: {queue_is_full.item()}")
    
    # æµ‹è¯•å¯¹æ¯”å­¦ä¹ ç»´åº¦
    print("\n" + "="*80)
    print("æµ‹è¯•å¯¹æ¯”å­¦ä¹ ç»´åº¦åŒ¹é…")
    print("="*80)
    
    # å½“å‰batch
    bbox_image_embeds = torch.randn(batch_size, projection_dim)
    bbox_text_embeds = torch.randn(batch_size, projection_dim)
    bbox_image_embeds = F.normalize(bbox_image_embeds, dim=1)
    bbox_text_embeds = F.normalize(bbox_text_embeds, dim=1)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    logits_i2t_batch = torch.matmul(bbox_image_embeds, bbox_text_embeds.T)
    logits_i2t_queue = torch.matmul(bbox_image_embeds, region_text_queue)
    logits_i2t = torch.cat([logits_i2t_batch, logits_i2t_queue], dim=1)
    
    print(f"âœ“ ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—æˆåŠŸ")
    print(f"  - Batchå†…å¯¹æ¯”: {logits_i2t_batch.shape} (æœŸæœ›: {batch_size}Ã—{batch_size})")
    print(f"  - Queueå¯¹æ¯”: {logits_i2t_queue.shape} (æœŸæœ›: {batch_size}Ã—{memory_bank_size})")
    print(f"  - æ€»å¯¹æ¯”çŸ©é˜µ: {logits_i2t.shape} (æœŸæœ›: {batch_size}Ã—{batch_size+memory_bank_size})")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    labels = torch.arange(batch_size, dtype=torch.long)
    loss = F.cross_entropy(logits_i2t, labels)
    
    print(f"\nâœ“ æŸå¤±è®¡ç®—æˆåŠŸ")
    print(f"  - Labels: {labels}")
    print(f"  - Loss: {loss.item():.4f}")
    
    # è®¡ç®—è´Ÿæ ·æœ¬å¢å¼ºæ¯”ä¾‹
    original_negatives = batch_size - 1  # batchå†…å…¶ä»–æ ·æœ¬
    enhanced_negatives = (batch_size - 1) + memory_bank_size  # batch + queue
    enhancement_ratio = enhanced_negatives / original_negatives
    
    print(f"\n" + "="*80)
    print(f"ğŸ“Š è´Ÿæ ·æœ¬å¢å¼ºç»Ÿè®¡")
    print("="*80)
    print(f"  - åŸå§‹è´Ÿæ ·æœ¬æ•°: {original_negatives}")
    print(f"  - å¢å¼ºåè´Ÿæ ·æœ¬æ•°: {enhanced_negatives}")
    print(f"  - å¢å¼ºå€æ•°: {enhancement_ratio:.1f}x")
    print(f"  - æ¯ä¸ªæ ·æœ¬å¯¹æ¯”è§„æ¨¡: 1æ­£æ ·æœ¬ + {enhanced_negatives}è´Ÿæ ·æœ¬")
    
    print(f"\n{'='*80}")
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Memory Banké€»è¾‘æ­£ç¡®")
    print(f"{'='*80}\n")


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("="*80)
    print("æµ‹è¯•è¾¹ç•Œæƒ…å†µ")
    print("="*80)
    
    projection_dim = 512
    memory_bank_size = 128
    
    # æµ‹è¯•1: è®­ç»ƒåˆæœŸé˜Ÿåˆ—æœªæ»¡
    print("\næµ‹è¯•1: è®­ç»ƒåˆæœŸï¼ˆé˜Ÿåˆ—æœªæ»¡ï¼‰")
    bbox_image_embeds = torch.randn(2, projection_dim)
    bbox_text_embeds = torch.randn(2, projection_dim)
    region_text_queue = torch.randn(projection_dim, memory_bank_size)
    queue_ptr = torch.tensor([5])  # é˜Ÿåˆ—åªæœ‰5ä¸ªæ ·æœ¬
    queue_is_full = torch.tensor([False])
    
    # åªä½¿ç”¨å·²å¡«å……éƒ¨åˆ†
    ptr = int(queue_ptr)
    logits_i2t_queue = torch.matmul(bbox_image_embeds, region_text_queue[:, :ptr])
    print(f"  âœ“ éƒ¨åˆ†é˜Ÿåˆ—å¯¹æ¯”: {logits_i2t_queue.shape} (æœŸæœ›: 2Ã—{ptr})")
    
    # æµ‹è¯•2: é˜Ÿåˆ—å¾ªç¯è¦†ç›–
    print("\næµ‹è¯•2: é˜Ÿåˆ—å¾ªç¯è¦†ç›–")
    queue_ptr = torch.tensor([126])  # æ¥è¿‘é˜Ÿåˆ—æœ«å°¾
    batch_size = 4
    print(f"  - é˜Ÿåˆ—æŒ‡é’ˆ: {queue_ptr.item()}")
    print(f"  - Batchå¤§å°: {batch_size}")
    print(f"  - å‰©ä½™ç©ºé—´: {memory_bank_size - queue_ptr.item()}")
    
    # æ¨¡æ‹Ÿå¾ªç¯å†™å…¥
    remain_space = memory_bank_size - queue_ptr.item()
    overflow_size = batch_size - remain_space
    print(f"  âœ“ éœ€è¦å¾ªç¯å†™å…¥: {remain_space}ä¸ªåˆ°æœ«å°¾ + {overflow_size}ä¸ªåˆ°å¼€å¤´")
    
    print(f"\n{'='*80}")
    print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡ï¼")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    test_memory_bank_logic()
    test_edge_cases()
