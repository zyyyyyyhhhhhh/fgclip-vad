"""
è¿½è¸ª train_fgclip.py çš„æ•°æ®æµ
å±•ç¤ºæ•°æ®ä» JSON æ–‡ä»¶åˆ°æœ€ç»ˆè¿”å›çš„å®Œæ•´è½¬æ¢è¿‡ç¨‹
"""

import json
import torch
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, '/data/zyy/wsvad/2026CVPR/FG-CLIP')

from transformers import AutoTokenizer, CLIPImageProcessor
from fgclip.train.train_fgclip import LazySupervisedBboxDataset, DataCollatorForSupervisedDataset
from dataclasses import dataclass

@dataclass
class MockDataArgs:
    data_path = '/data/zyy/dataset/UCF_Crimes_Videos/ucf_train_data_merged.json'
    image_folder = '/data/zyy/dataset/UCF_Crimes_Videos/UCF_Crimes'
    is_video = True
    num_frames = 8  # ä½¿ç”¨è¾ƒå°çš„å€¼ä¾¿äºå±•ç¤º
    add_box_loss = True
    use_hard_neg = False
    max_seq_length = 248
    base_seq_length = 77
    base_image_size = 224

def print_separator(title):
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)

def print_tensor_info(name, tensor):
    """æ‰“å°å¼ é‡çš„è¯¦ç»†ä¿¡æ¯"""
    if isinstance(tensor, torch.Tensor):
        print(f"  {name}:")
        print(f"    - Type: torch.Tensor")
        print(f"    - Shape: {tuple(tensor.shape)}")
        print(f"    - Dtype: {tensor.dtype}")
        print(f"    - Device: {tensor.device}")
        if tensor.numel() < 20:
            print(f"    - Values: {tensor.tolist()}")
        else:
            print(f"    - Sample values: {tensor.flatten()[:5].tolist()}...")
    else:
        print(f"  {name}: {tensor} (type: {type(tensor).__name__})")

def main():
    print_separator("æ•°æ®æµè¿½è¸ªï¼šä» JSON åˆ°æœ€ç»ˆè¿”å›")
    
    # æ­¥éª¤1: åŠ è½½åŸå§‹ JSON æ•°æ®
    print_separator("æ­¥éª¤1: åŸå§‹ JSON æ•°æ®")
    with open('/data/zyy/dataset/UCF_Crimes_Videos/ucf_train_data_merged.json', 'r') as f:
        raw_data = json.load(f)
    
    # æ‰¾ä¸€ä¸ªå¼‚å¸¸è§†é¢‘å’Œä¸€ä¸ªæ­£å¸¸è§†é¢‘ä½œä¸ºæ ·æœ¬
    abnormal_sample = None
    normal_sample = None
    
    for video_name, video_data in raw_data.items():
        if abnormal_sample is None and video_data.get('region') and len(video_data['region']) > 0:
            if 'keyframes' in video_data['region'][0]:
                abnormal_sample = (video_name, video_data)
        
        if normal_sample is None and video_data.get('region') and len(video_data['region']) > 0:
            if 'keyframes' not in video_data['region'][0]:
                normal_sample = (video_name, video_data)
        
        if abnormal_sample and normal_sample:
            break
    
    print("\nğŸ“Š å¼‚å¸¸è§†é¢‘æ ·æœ¬:")
    name, data = abnormal_sample
    print(f"  Video: {name}")
    print(f"  Global Caption: {data['global']['Caption'][:80]}...")
    print(f"  Region æ•°é‡: {len(data['region'])}")
    print(f"  Region[0] Caption: {data['region'][0]['caption'][:80]}...")
    print(f"  Region[0] Keyframes æ•°é‡: {len(data['region'][0]['keyframes'])}")
    print(f"  Keyframe æ ·ä¾‹: {data['region'][0]['keyframes'][0]}")
    
    print("\nğŸ“Š æ­£å¸¸è§†é¢‘æ ·æœ¬:")
    name, data = normal_sample
    print(f"  Video: {name}")
    print(f"  Global Caption: {data['global']['Caption']}")
    print(f"  Region æ•°é‡: {len(data['region'])}")
    print(f"  Region[0] Caption: {data['region'][0]['caption']}")
    print(f"  Region[0] æœ‰ keyframes: {'keyframes' in data['region'][0]}")
    
    # æ­¥éª¤2: åˆå§‹åŒ–æ•°æ®é›†
    print_separator("æ­¥éª¤2: åˆå§‹åŒ– Dataset")
    
    print("\n  åŠ è½½ Tokenizer å’Œ ImageProcessor...")
    # ä½¿ç”¨æœ¬åœ° CLIP tokenizer
    from fgclip.model.clip.simple_tokenizer import SimpleTokenizer
    tokenizer = SimpleTokenizer()
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ mock image processor
    class MockImageProcessor:
        def preprocess(self, image, return_tensors='pt'):
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],
                                   std=[0.26862954, 0.26130258, 0.27577711])
            ])
            tensor = transform(image)
            return {'pixel_values': tensor.unsqueeze(0)}
    
    image_processor = MockImageProcessor()
    
    data_args = MockDataArgs()
    
    print(f"\n  åˆ›å»º LazySupervisedBboxDataset...")
    print(f"    - data_path: {data_args.data_path}")
    print(f"    - is_video: {data_args.is_video}")
    print(f"    - num_frames: {data_args.num_frames}")
    print(f"    - add_box_loss: {data_args.add_box_loss}")
    print(f"    - use_hard_neg: {data_args.use_hard_neg}")
    
    dataset = LazySupervisedBboxDataset(
        data_path=data_args.data_path,
        data_args=data_args,
        img_preprocess=image_processor,
        tokenizer=tokenizer
    )
    
    print(f"\n  âœ“ Dataset åˆ›å»ºæˆåŠŸ")
    print(f"    - æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"    - æ­£å¸¸è§†é¢‘: {sum(1 for x in dataset.list_data_dict if not x['is_abnormal'])}")
    print(f"    - å¼‚å¸¸è§†é¢‘: {sum(1 for x in dataset.list_data_dict if x['is_abnormal'])}")
    
    # æ­¥éª¤3: _convert_dict_to_list è½¬æ¢
    print_separator("æ­¥éª¤3: _convert_dict_to_list() è½¬æ¢åçš„æ ¼å¼")
    
    # æ‰¾åˆ°å¯¹åº”çš„å†…éƒ¨æ•°æ®
    abnormal_internal = None
    normal_internal = None
    for item in dataset.list_data_dict:
        if abnormal_internal is None and item['is_abnormal']:
            abnormal_internal = item
        if normal_internal is None and not item['is_abnormal']:
            normal_internal = item
        if abnormal_internal and normal_internal:
            break
    
    print("\nğŸ“Š å¼‚å¸¸è§†é¢‘å†…éƒ¨è¡¨ç¤º:")
    print(f"  video_name: {abnormal_internal['video_name']}")
    print(f"  is_abnormal: {abnormal_internal['is_abnormal']}")
    print(f"  global: {dict(abnormal_internal['global'])}")
    print(f"  region[0] keys: {list(abnormal_internal['region'][0].keys())}")
    print(f"  region[0] æœ‰ keyframes: {'keyframes' in abnormal_internal['region'][0]}")
    
    print("\nğŸ“Š æ­£å¸¸è§†é¢‘å†…éƒ¨è¡¨ç¤º:")
    print(f"  video_name: {normal_internal['video_name']}")
    print(f"  is_abnormal: {normal_internal['is_abnormal']}")
    print(f"  global: {dict(normal_internal['global'])}")
    print(f"  region[0] keys: {list(normal_internal['region'][0].keys())}")
    print(f"  region[0] æœ‰ keyframes: {'keyframes' in normal_internal['region'][0]}")
    
    # æ­¥éª¤4: __getitem__ è¿”å›çš„å•ä¸ªæ ·æœ¬
    print_separator("æ­¥éª¤4: __getitem__() è¿”å›çš„å•ä¸ªæ ·æœ¬")
    
    # æ‰¾åˆ°å¼‚å¸¸è§†é¢‘å’Œæ­£å¸¸è§†é¢‘çš„ç´¢å¼•
    abnormal_idx = None
    normal_idx = None
    for i, item in enumerate(dataset.list_data_dict):
        if abnormal_idx is None and item['is_abnormal']:
            abnormal_idx = i
        if normal_idx is None and not item['is_abnormal']:
            normal_idx = i
        if abnormal_idx is not None and normal_idx is not None:
            break
    
    print(f"\n  æ­£åœ¨åŠ è½½å¼‚å¸¸è§†é¢‘æ ·æœ¬ (index={abnormal_idx})...")
    try:
        abnormal_sample_data = dataset[abnormal_idx]
        
        print("\nğŸ“Š å¼‚å¸¸è§†é¢‘æ ·æœ¬æ•°æ®ç»“æ„:")
        print(f"  Keys: {list(abnormal_sample_data.keys())}")
        print()
        
        for key, value in abnormal_sample_data.items():
            print_tensor_info(key, value)
        
        print("\n  ğŸ” å…³é”®å­—æ®µè¯¦è§£:")
        print(f"    video: è§†é¢‘å¸§å¼ é‡ {tuple(abnormal_sample_data['video'].shape)}")
        print(f"           - ç»´åº¦: (T={abnormal_sample_data['video'].shape[0]}, "
              f"C={abnormal_sample_data['video'].shape[1]}, "
              f"H={abnormal_sample_data['video'].shape[2]}, "
              f"W={abnormal_sample_data['video'].shape[3]})")
        
        print(f"\n    video_attention_mask: æœ‰æ•ˆå¸§æ©ç  {tuple(abnormal_sample_data['video_attention_mask'].shape)}")
        print(f"           - True è¡¨ç¤ºæœ‰æ•ˆå¸§, False è¡¨ç¤ºå¡«å……å¸§")
        print(f"           - æœ‰æ•ˆå¸§æ•°: {abnormal_sample_data['video_attention_mask'].sum().item()}")
        
        print(f"\n    text: Global Caption (é•¿æ–‡æœ¬) {tuple(abnormal_sample_data['text'].shape)}")
        print(f"           - Token IDs, é•¿åº¦ 77*4-60=248")
        
        print(f"\n    short_text: Region Caption (çŸ­æ–‡æœ¬) {tuple(abnormal_sample_data['short_text'].shape)}")
        print(f"           - Token IDs, é•¿åº¦ 77")
        
        if abnormal_sample_data['add_box_loss']:
            print(f"\n    box_texts: Region æè¿°æ–‡æœ¬ {tuple(abnormal_sample_data['box_texts'].shape)}")
            print(f"           - (max_anns={abnormal_sample_data['box_texts'].shape[0]}, seq_len=77)")
            
            print(f"\n    box_infos: Bbox åæ ‡ {tuple(abnormal_sample_data['box_infos'].shape)}")
            print(f"           - (max_anns={abnormal_sample_data['box_infos'].shape[0]}, 4)")
            print(f"           - æ ¼å¼: [x1, y1, x2, y2] (å½’ä¸€åŒ– 0-1)")
            print(f"           - æ ·ä¾‹: {abnormal_sample_data['box_infos'][0].tolist()}")
            
            print(f"\n    box_nums: æœ‰æ•ˆ bbox æ•°é‡ {tuple(abnormal_sample_data['box_nums'].shape)}")
            print(f"           - å€¼: {abnormal_sample_data['box_nums'].item()}")
        
    except Exception as e:
        print(f"  âš ï¸ åŠ è½½å¤±è´¥: {e}")
        print("  (è¿™å¯èƒ½æ˜¯å› ä¸ºè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†ä¸å½±å“ç†è§£æ•°æ®æ ¼å¼)")
    
    print(f"\n  æ­£åœ¨åŠ è½½æ­£å¸¸è§†é¢‘æ ·æœ¬ (index={normal_idx})...")
    try:
        normal_sample_data = dataset[normal_idx]
        
        print("\nğŸ“Š æ­£å¸¸è§†é¢‘æ ·æœ¬æ•°æ®ç»“æ„:")
        print(f"  Keys: {list(normal_sample_data.keys())}")
        print()
        
        for key, value in normal_sample_data.items():
            print_tensor_info(key, value)
        
        print("\n  ğŸ” ä¸å¼‚å¸¸è§†é¢‘çš„åŒºåˆ«:")
        print(f"    âœ“ video shape ç›¸åŒ: {tuple(normal_sample_data['video'].shape)}")
        print(f"    âœ“ æ‰€æœ‰å­—æ®µéƒ½ç›¸åŒ")
        print(f"    âœ“ å…³é”®åŒºåˆ«åœ¨ box_infos:")
        if normal_sample_data['add_box_loss']:
            print(f"      - æ­£å¸¸è§†é¢‘ box_infos: {normal_sample_data['box_infos'][0].tolist()}")
            print(f"      - è¿™æ˜¯è™šæ‹Ÿ bbox [0, 0, 1, 1]ï¼Œè¦†ç›–æ•´ä¸ªç”»é¢")
    
    except Exception as e:
        print(f"  âš ï¸ åŠ è½½å¤±è´¥: {e}")
    
    # æ­¥éª¤5: DataCollator åˆå¹¶ batch
    print_separator("æ­¥éª¤5: DataCollator åˆå¹¶æˆ Batch")
    
    print("\n  DataCollator çš„ä½œç”¨:")
    print("    - å°†å¤šä¸ªæ ·æœ¬å †å æˆ batch")
    print("    - æ·»åŠ  batch ç»´åº¦ (B)")
    
    try:
        collator = DataCollatorForSupervisedDataset()
        
        # æ¨¡æ‹Ÿä¸€ä¸ª batch (2ä¸ªæ ·æœ¬)
        batch_samples = [abnormal_sample_data, normal_sample_data]
        batch = collator(batch_samples)
        
        print("\nğŸ“Š Batch æ•°æ®ç»“æ„ (batch_size=2):")
        print(f"  Keys: {list(batch.keys())}")
        print()
        
        for key, value in batch.items():
            print_tensor_info(key, value)
        
        print("\n  ğŸ” Batch ç»´åº¦è¯´æ˜:")
        print(f"    video: {tuple(batch['video'].shape)}")
        print(f"           - (B=2, T={batch['video'].shape[1]}, C=3, H=224, W=224)")
        print(f"           - Batch[0]: å¼‚å¸¸è§†é¢‘, Batch[1]: æ­£å¸¸è§†é¢‘")
        
        print(f"\n    video_attention_mask: {tuple(batch['video_attention_mask'].shape)}")
        print(f"           - (B=2, T={batch['video_attention_mask'].shape[1]})")
        
        print(f"\n    text_long: {tuple(batch['text_long'].shape)}")
        print(f"           - (B=2, seq_len=248)")
        
        print(f"\n    text_short: {tuple(batch['text_short'].shape)}")
        print(f"           - (B=2, seq_len=77)")
        
        if batch['add_box_loss']:
            print(f"\n    box_texts: {tuple(batch['box_texts'].shape)}")
            print(f"           - (B*max_anns=2*4=8, seq_len=77)")
            
            print(f"\n    box_infos: {tuple(batch['box_infos'].shape)}")
            print(f"           - (B*max_anns=8, 4)")
            print(f"           - Batch[0] (å¼‚å¸¸): {batch['box_infos'][0].tolist()}")
            print(f"           - Batch[1] (æ­£å¸¸): {batch['box_infos'][4].tolist()}")
            
            print(f"\n    box_nums: {tuple(batch['box_nums'].shape)}")
            print(f"           - (B=2,)")
            print(f"           - å€¼: {batch['box_nums'].tolist()}")
    
    except Exception as e:
        print(f"  âš ï¸ Batch åˆ›å»ºå¤±è´¥: {e}")
    
    # æ­¥éª¤6: ä¼ å…¥æ¨¡å‹
    print_separator("æ­¥éª¤6: ä¼ å…¥ FGCLIPModel è¿›è¡Œå‰å‘ä¼ æ’­")
    
    print("""
  æ¨¡å‹ forward() æ¥æ”¶çš„å‚æ•°:
  
  model.forward(
      pixel_values=batch['video'],                    # (B, T, C, H, W)
      video_attention_mask=batch['video_attention_mask'],  # (B, T)
      input_ids_long=batch['text_long'],             # (B, 248)
      input_ids_short=batch['text_short'],           # (B, 77)
      add_box_loss=batch['add_box_loss'],            # bool
      box_texts=batch['box_texts'],                  # (B*max_anns, 77)
      box_infos=batch['box_infos'],                  # (B*max_anns, 4)
      box_nums=batch['box_nums'],                    # (B,)
      use_hard_neg=batch['use_hard_neg'],            # bool
  )
  
  æ¨¡å‹å†…éƒ¨å¤„ç†:
    1. è§†é¢‘ç¼–ç  (é€å¸§ + æ—¶åºå»ºæ¨¡)
       - video_attention_mask æ ‡è®°æœ‰æ•ˆå¸§
       - è¾“å‡º: video_embeds (B, 512)
    
    2. æ–‡æœ¬ç¼–ç 
       - text_long: Global Caption (B, 512)
       - text_short: Region Caption (B, 512)
    
    3. è®¡ç®—å…¨å±€å¯¹æ¯”æŸå¤±
       - loss_global = InfoNCE(video_embeds, text_embeds)
    
    4. å¦‚æœ add_box_loss=True:
       - æå– bbox åŒºåŸŸç‰¹å¾ (ä½¿ç”¨ RoI Align)
       - å¼‚å¸¸è§†é¢‘: çœŸå® bbox [x1, y1, x2, y2]
       - æ­£å¸¸è§†é¢‘: è™šæ‹Ÿ bbox [0, 0, 1, 1]
       - ç¼–ç  box_texts
       - loss_bbox = 0.1 * pairwise_contrastive_loss(...)
    
    5. è¿”å›æ€»æŸå¤±
       - total_loss = loss_global + loss_bbox
""")
    
    # æ€»ç»“
    print_separator("æ•°æ®æµæ€»ç»“")
    
    print("""
  å®Œæ•´æ•°æ®æµ:
  
  1. JSON æ–‡ä»¶
     â†“ _convert_dict_to_list()
  
  2. å†…éƒ¨åˆ—è¡¨æ ¼å¼ [{video_name, global, region, is_abnormal}, ...]
     â†“ __getitem__()
  
  3. å•ä¸ªæ ·æœ¬å­—å…¸
     {
       'video': (T, C, H, W),
       'video_attention_mask': (T,),
       'text': (1, 248),
       'short_text': (1, 77),
       'box_texts': (max_anns, 77),
       'box_infos': (max_anns, 4),
       'box_nums': (1,),
       'add_box_loss': bool,
       'use_hard_neg': bool
     }
     â†“ DataCollator
  
  4. Batch å­—å…¸
     {
       'video': (B, T, C, H, W),
       'video_attention_mask': (B, T),
       'text_long': (B, 248),
       'text_short': (B, 77),
       'box_texts': (B*max_anns, 77),
       'box_infos': (B*max_anns, 4),
       'box_nums': (B,),
       'add_box_loss': bool,
       'use_hard_neg': bool
     }
     â†“ FGCLIPModel.forward()
  
  5. æ¨¡å‹è¾“å‡º
     {
       'loss': scalar tensor (æ€»æŸå¤±)
     }
  
  å…³é”®ç‰¹å¾:
    âœ“ å¼‚å¸¸è§†é¢‘: box_infos æ˜¯çœŸå® bbox (ä» keyframes æ’å€¼)
    âœ“ æ­£å¸¸è§†é¢‘: box_infos æ˜¯è™šæ‹Ÿ bbox [0, 0, 1, 1]
    âœ“ video_attention_mask: æ ‡è®°æœ‰æ•ˆå¸§ï¼ˆéå¡«å……å¸§ï¼‰
    âœ“ æ‰€æœ‰è§†é¢‘éƒ½æœ‰ Global + Region Caption
    âœ“ add_box_loss=True æ—¶è®¡ç®—ç»†ç²’åº¦ bbox æŸå¤±
""")
    
    print("=" * 80)
    print("  æ•°æ®æµè¿½è¸ªå®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()
