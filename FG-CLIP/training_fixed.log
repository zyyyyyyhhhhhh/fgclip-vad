========================================
ğŸš€ FG-CLIP UCF æ­£å¼è®­ç»ƒå¯åŠ¨
========================================
ğŸ“Š æ•°æ®ç»Ÿè®¡:
  - è®­ç»ƒè§†é¢‘: 232ä¸ª
  - æ•°æ®æ–‡ä»¶: ucf_fgclip_train_final.json
  - æ•°æ®å¤§å°: 1.6M

ğŸ¯ è®­ç»ƒé…ç½®:
  - å¸§æ•°: 256
  - æ‰¹æ¬¡å¤§å°: 2
  - æ¢¯åº¦ç´¯ç§¯: 8
  - æœ‰æ•ˆæ‰¹æ¬¡: 16
  - è®­ç»ƒè½®æ•°: 10

ï¿½ æ¨¡å‹é…ç½®:
  - CLIPæ¨¡å‹: ViT-B/32 (æœ¬åœ°åŠ è½½)
  - æœ¬åœ°è·¯å¾„: ./fgclip/model/clip
  - ç½‘ç»œéœ€æ±‚: âŒ æ— éœ€è”ç½‘

ï¿½ğŸ’¾ è¾“å‡ºç›®å½•: ./checkpoints/fgclip_ucf_full
ğŸ–¥ï¸  GPUè®¾å¤‡: 0
========================================

â° è®­ç»ƒå¼€å§‹æ—¶é—´: 2025-10-12 22:42:22
/data/zyy/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
============================================================
Loading CLIP components (LOCAL MODE - No Internet Required)
============================================================
Loading tokenizer for: ViT-B/32
  âœ“ Tokenizer loaded (local CLIP)
Loading image processor for: ViT-B/32
  âœ“ Image processor loaded (local CLIP)
Initializing FG-CLIP model: ViT-B/32
  âœ“ Model initialized (random weights)
============================================================
Loading data from: /data/zyy/dataset/UCF_Crimes_Videos/ucf_fgclip_train_final.json
  â†’ Detected list format (new)
Total videos loaded: 232
  - Normal videos: 0
  - Abnormal videos: 232
  0%|          | 0/140 [00:00<?, ?it/s]/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:562: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  short_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  box_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:562: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  short_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  box_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:562: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  short_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  box_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:562: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:567: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  short_text = torch.tensor(
/data/zyy/wsvad/2026CVPR/FG-CLIP/fgclip/train/train_fgclip.py:650: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
  box_text = torch.tensor(
[LOSS] Total: 0.7553 | Global: 0.6826 | Region: 0.7266 | HardNeg: 0.0000
[LOSS] Total: 0.7867 | Global: 0.7173 | Region: 0.6943 | HardNeg: 0.0000
[LOSS] Total: 0.8389 | Global: 0.7695 | Region: 0.6934 | HardNeg: 0.0000
[LOSS] Total: 0.7992 | Global: 0.7295 | Region: 0.6973 | HardNeg: 0.0000
[LOSS] Total: nan | Global: 0.7891 | Region: nan | HardNeg: 0.0000
[LOSS] Total: 0.7471 | Global: 0.6768 | Region: 0.7031 | HardNeg: 0.0000
[LOSS] Total: 0.8256 | Global: 0.7554 | Region: 0.7021 | HardNeg: 0.0000
[LOSS] Total: 0.8321 | Global: 0.7612 | Region: 0.7090 | HardNeg: 0.0000
  1%|          | 1/140 [00:23<53:48, 23.23s/it][LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
[LOSS] Total: nan | Global: nan | Region: nan | HardNeg: 0.0000
  1%|â–         | 2/140 [00:58<1:10:18, 30.57s/it]