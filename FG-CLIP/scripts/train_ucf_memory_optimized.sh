#!/bin/bash

# ============================================
# FG-CLIP UCF å†…å­˜ä¼˜åŒ–è®­ç»ƒé…ç½®
# ä¸“é—¨é’ˆå¯¹ 24GB æ˜¾å­˜ï¼ˆå¦‚ RTX 3090/4090ï¼‰
# ============================================

# ç¯å¢ƒé…ç½®
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
# âœ… PyTorch å†…å­˜ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# ============================================
# æ ¸å¿ƒé…ç½® - å†…å­˜ä¼˜åŒ–
# ============================================

# æ•°æ®é…ç½®
DATA_PATH="/data/zyy/dataset/UCF_Crimes_Videos/ucf_fgclip_with_caption2_translated.json"
IMAGE_FOLDER="/data/zyy/dataset"
OUTPUT_DIR="./checkpoints/fgclip_ucf_memory_opt"

# æ¨¡å‹é…ç½®
BASE_MODEL="fgclip-weight"
LOCAL_CLIP_PATH="./fgclip/model/clip"

# ============================================
# è®­ç»ƒè¶…å‚æ•° - æé™å†…å­˜ä¼˜åŒ–
# ============================================

# è§†é¢‘é…ç½®
NUM_FRAMES=256              # Global: 256, Region: è‡ªåŠ¨é™åˆ° 64 (1/4)
BATCH_SIZE=2                # âœ… æå‡åˆ° 2ï¼ˆ64 å¸§ region åæ˜¾å­˜è¶³å¤Ÿï¼‰
GRAD_ACCUM=16               # âœ… æ¢¯åº¦ç´¯ç§¯ï¼ˆæœ‰æ•ˆbatch=2Ã—16=32ï¼‰

# è®­ç»ƒé…ç½®
EPOCHS=5
LEARNING_RATE=5e-6
TEXT_LR=2e-6
WARMUP_RATIO=0.1
MAX_GRAD_NORM=1.0

# ä¿å­˜ç­–ç•¥
SAVE_STEPS=100
SAVE_TOTAL_LIMIT=2          # âœ… åªä¿ç•™ 2 ä¸ª checkpointï¼ˆèŠ‚çœç£ç›˜ï¼‰
LOGGING_STEPS=10

# æ•ˆç‡ä¼˜åŒ–
NUM_WORKERS=1               # âœ… é™ä½åˆ° 1ï¼ˆå‡å°‘ CPU å†…å­˜ï¼‰
GRAD_CHECKPOINT=True
AUTO_RESUME=False

# LoRA é…ç½®ï¼ˆé»˜è®¤å…³é—­ï¼Œå¦‚éœ€å¯ç”¨è®¾ç½® USE_LORA=trueï¼‰
USE_LORA=True
LORA_R=8
LORA_ALPHA=16
LORA_DROPOUT=0.05
LORA_BIAS="none"
LORA_WEIGHT_PATH=""

echo "========================================"
echo "ğŸš€ FG-CLIP UCF å†…å­˜ä¼˜åŒ–è®­ç»ƒ"
echo "========================================"
echo "ğŸ’¾ å†…å­˜ä¼˜åŒ–ç­–ç•¥:"
echo "  - Batch Size: ${BATCH_SIZE} âœ… æå‡åˆ° 2ï¼ˆå¯¹æ¯”å­¦ä¹ éœ€è¦ >1ï¼‰"
echo "  - Gradient Accumulation: ${GRAD_ACCUM}"
echo "  - Global å¸§æ•°: 256 (å®Œæ•´æ—¶åº)"
echo "  - Region å¸§æ•°: 96 (3/8 Globalï¼Œå¹³è¡¡è¯­ä¹‰+æ˜¾å­˜)"
echo "  - æœ‰æ•ˆ Batch: $((BATCH_SIZE * GRAD_ACCUM))"
echo "  - Workers: ${NUM_WORKERS}"
echo "  - PyTorch å†…å­˜ç¢ç‰‡ä¼˜åŒ–: âœ…"
echo ""
echo "ğŸ“Š é¢„æœŸæ˜¾å­˜å ç”¨:"
echo "  - Global Video: ~2.4 GB (2Ã—256Ã—3Ã—224Ã—224)"
echo "  - Region Videos: ~6.5 GB (2Ã—15Ã—96Ã—3Ã—224Ã—224) âœ… æå‡åˆ° 96 å¸§"
echo "  - æ¨¡å‹å‚æ•° + æ¢¯åº¦: ~8 GB"
echo "  - æ€»è®¡: ~17 GB (24 GB æ˜¾å­˜å……è¶³ï¼Œå®‰å…¨è¾¹é™… 7 GB)"
echo "========================================"
echo ""

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
if [ ! -f "${DATA_PATH}" ]; then
    echo "âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: ${DATA_PATH}"
    exit 1
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p ${OUTPUT_DIR}

# æ¸…ç† GPU ç¼“å­˜
python3 -c "import torch; torch.cuda.empty_cache(); print('âœ… GPU ç¼“å­˜å·²æ¸…ç†')"

# æ„é€  LoRA ç›¸å…³å‚æ•°
LORA_ARGS="--lora_enable ${USE_LORA}"
if [ "${USE_LORA}" = true ]; then
    LORA_ARGS="${LORA_ARGS} --lora_r ${LORA_R} --lora_alpha ${LORA_ALPHA} --lora_dropout ${LORA_DROPOUT} --lora_bias ${LORA_BIAS}"
    if [ -n "${LORA_WEIGHT_PATH}" ]; then
        LORA_ARGS="${LORA_ARGS} --lora_weight_path ${LORA_WEIGHT_PATH}"
    fi
fi

# è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
START_TIME=$(date '+%Y-%m-%d %H:%M:%S')
echo "â° è®­ç»ƒå¼€å§‹æ—¶é—´: ${START_TIME}" | tee ${OUTPUT_DIR}/training_start.log

# ============================================
# å¯åŠ¨è®­ç»ƒ
# ============================================

python3 fgclip/train/train_fgclip.py \
    --model_name_or_path ${BASE_MODEL} \
    --base_model ${BASE_MODEL} \
    --data_path ${DATA_PATH} \
    --image_folder ${IMAGE_FOLDER} \
    --output_dir ${OUTPUT_DIR} \
    --overwrite_output_dir True \
    \
    --is_video True \
    --num_frames ${NUM_FRAMES} \
    --is_multimodal True \
    --add_box_loss True \
    --from_openai True \
    \
    --bf16 True \
    --num_train_epochs ${EPOCHS} \
    --per_device_train_batch_size ${BATCH_SIZE} \
    --gradient_accumulation_steps ${GRAD_ACCUM} \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps ${SAVE_STEPS} \
    --save_total_limit ${SAVE_TOTAL_LIMIT} \
    --learning_rate ${LEARNING_RATE} \
    --text_model_lr ${TEXT_LR} \
    --max_grad_norm ${MAX_GRAD_NORM} \
    --weight_decay 0.01 \
    --warmup_ratio ${WARMUP_RATIO} \
    --lr_scheduler_type "cosine" \
    --logging_steps ${LOGGING_STEPS} \
    --tf32 True \
    --gradient_checkpointing ${GRAD_CHECKPOINT} \
    --dataloader_num_workers ${NUM_WORKERS} \
    --auto_resume ${AUTO_RESUME} \
    --report_to tensorboard \
    --seed 42 \
    ${LORA_ARGS}

# è®°å½•è®­ç»ƒç»“æŸæ—¶é—´
TRAIN_EXIT_CODE=$?
END_TIME=$(date '+%Y-%m-%d %H:%M:%S')

echo ""
echo "========================================"
if [ ${TRAIN_EXIT_CODE} -eq 0 ]; then
    echo "âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼"
else
    echo "âŒ è®­ç»ƒå¼‚å¸¸é€€å‡º (Exit Code: ${TRAIN_EXIT_CODE})"
fi
echo "========================================"
echo "â° å¼€å§‹æ—¶é—´: ${START_TIME}"
echo "â° ç»“æŸæ—¶é—´: ${END_TIME}"
echo ""
echo "ğŸ“‚ è¾“å‡ºç›®å½•: ${OUTPUT_DIR}"
echo "ğŸ“Š TensorBoard: tensorboard --logdir ${OUTPUT_DIR} --port 6006"
echo "========================================"

exit ${TRAIN_EXIT_CODE}
