import os
import copy
from dataclasses import dataclass, field
import json
import logging
import pathlib
from typing import Dict, Optional, Sequence, List
from pathlib import Path

import torch
import random

import glob
import transformers

from torch.utils.data import Dataset, Sampler
from fgclip.train.clean_clip_trainer import CLIPTrainer


import torch.distributed as dist

import copy
import os
import json
import torch 
from torch.utils.data import Dataset
from torchvision.datasets.utils import download_url
from torchvision import transforms
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from torchvision.transforms.functional import InterpolationMode
from einops import rearrange

from random import choice
from PIL import Image
import cv2  # ç”¨äºè¯»å–è§†é¢‘
import numpy as np  # ç”¨äºå¤„ç†å¸§é‡‡æ ·

import gzip
from io import BytesIO
import base64
from torch.utils.data import  IterableDataset
import random
import math

from fgclip.model.clip_strc.fgclip import FGCLIPModel

# âœ… ä½¿ç”¨æœ¬åœ°CLIPï¼ˆä¸ä¾èµ–HuggingFaceåœ¨çº¿ä¸‹è½½ï¼‰
from fgclip.train.local_clip_loader import LocalCLIPWrapper

# Load pretrained model, tokenizer, and image processor
from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig, CLIPConfig
import numpy as np

from transformers import (
    AutoImageProcessor,
    AutoModel,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    set_seed,
)

import gc
from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, TaskType, PeftModel

local_rank = None


def rank0_print(*args):
    if local_rank == 0:
        print(*args)


# ç¯å¢ƒå˜é‡æ§åˆ¶çš„è¿è¡Œæ—¶è¯Šæ–­å¼€å…³
# ä½¿ç”¨æ–¹æ³•: åœ¨å¯åŠ¨è®­ç»ƒå‰ export ENABLE_RUNTIME_DIAG=1 æˆ–è€…å¯¼å‡º ENABLE_RUNTIME_DIAG=interval
# ä¾‹å¦‚: export ENABLE_RUNTIME_DIAG=1  (æ¯ä¸ªæœ‰regionçš„batchéƒ½æ‰“å°)
# æˆ–è€…: export ENABLE_RUNTIME_DIAG=10 (æ¯10ä¸ªæœ‰regionçš„batchæ‰“å°ä¸€æ¬¡)
def parse_runtime_diag_env():
    v = os.environ.get('ENABLE_RUNTIME_DIAG', '').strip()
    if v == '':
        return False, 1
    try:
        iv = int(v)
        return True, max(1, iv)
    except Exception:
        return True, 1


class BalancedNormalAbnormalSampler(Sampler):
    """
    ä¿è¯æ¯ä¸ªmini-batchä¸­normal/abnormalæ•°é‡ä¸€è‡´çš„é‡‡æ ·å™¨ã€‚
    """
    def __init__(self, dataset, batch_size: int, shuffle: bool = True):
        if batch_size % 2 != 0:
            raise ValueError("BalancedNormalAbnormalSampler requires an even per-device batch size.")
        if not hasattr(dataset, "normal_indices") or not hasattr(dataset, "abnormal_indices"):
            raise ValueError("Dataset must expose normal_indices and abnormal_indices for balanced sampling.")
        if len(dataset.normal_indices) == 0 or len(dataset.abnormal_indices) == 0:
            raise ValueError("Balanced sampling needs both normal and abnormal samples.")
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.half = max(1, batch_size // 2)

    def __iter__(self):
        normals = self.dataset.normal_indices.copy()
        abnormals = self.dataset.abnormal_indices.copy()
        if self.shuffle:
            random.shuffle(normals)
            random.shuffle(abnormals)

        num_batches = math.ceil(max(len(normals), len(abnormals)) / self.half)
        norm_idx = 0
        abn_idx = 0
        for _ in range(num_batches):
            for _ in range(self.half):
                yield normals[norm_idx % len(normals)]
                norm_idx += 1
            for _ in range(self.half):
                yield abnormals[abn_idx % len(abnormals)]
                abn_idx += 1

    def __len__(self):
        num_batches = math.ceil(max(len(self.dataset.normal_indices), len(self.dataset.abnormal_indices)) / self.half)
        return num_batches * self.batch_size


class BalancedCLIPTrainer(CLIPTrainer):
    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
        if self.train_dataset is None:
            return None
        try:
            return BalancedNormalAbnormalSampler(
                self.train_dataset,
                batch_size=self.args.per_device_train_batch_size,
                shuffle=True,
            )
        except ValueError as exc:
            rank0_print(f"[Sampler] {exc}ï¼Œfallbackåˆ°é»˜è®¤éšæœºé‡‡æ ·")
            return super()._get_train_sampler()


# ============ è§†é¢‘å¤„ç†å·¥å…·å‡½æ•° ============
def uniform_sample_frames(frames: list, frame_indices: list, target_len: int):
    """ä»å¸§åˆ—è¡¨ä¸­å‡åŒ€é‡‡æ · target_len å¸§ï¼Œå¹¶è¿”å›é‡‡æ ·åçš„å¸§ç´¢å¼•"""
    num_frames = len(frames)
    if num_frames == target_len:
        return frames, frame_indices
    indices = np.linspace(0, num_frames - 1, target_len, dtype=int)
    sampled_frames = [frames[i] for i in indices]
    sampled_indices = [frame_indices[i] for i in indices]
    return sampled_frames, sampled_indices


def pad_frames(frames: list, frame_indices: list, target_len: int):
    """ç”¨æœ€åä¸€å¸§å¡«å……åˆ° target_lenï¼Œå¹¶æ‰©å±•ç´¢å¼•åˆ—è¡¨"""
    num_padding = target_len - len(frames)
    if num_padding > 0:
        frames.extend([frames[-1]] * num_padding)
        # å¡«å……å¸§ä½¿ç”¨æœ€åä¸€å¸§çš„ç´¢å¼•
        frame_indices.extend([frame_indices[-1]] * num_padding)
    return frames, frame_indices


def find_closest_keyframe_bbox(keyframes: list, target_frame_idx: int, interpolate: bool = True):
    """
    æ‰¾åˆ°ä¸ç›®æ ‡å¸§æœ€æ¥è¿‘çš„ keyframe çš„ bbox
    
    ã€æ–¹æ¡ˆAä¿®æ”¹ã€‘åªåœ¨å¼‚å¸¸æ—¶æ®µå†…è¿”å›æœ‰æ•ˆbboxï¼Œéå¼‚å¸¸æ—¶æ®µè¿”å›å…¨é›¶bbox
    
    Args:
        keyframes: keyframe åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'frame': int, 'bbox': [x1,y1,x2,y2], 'enabled': bool}
        target_frame_idx: ç›®æ ‡å¸§ç´¢å¼•
        interpolate: æ˜¯å¦åœ¨ä¸¤ä¸ª keyframe ä¹‹é—´è¿›è¡Œçº¿æ€§æ’å€¼
    
    Returns:
        bbox: [x1, y1, x2, y2]
        is_valid: bool - è¯¥å¸§æ˜¯å¦åœ¨å¼‚å¸¸æ—¶æ®µå†…
    """
    if not keyframes:
        return [0.0, 0.0, 0.0, 0.0], False
    
    # å¦‚æœåªæœ‰ä¸€ä¸ª keyframeï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ—¶æ®µå†…
    if len(keyframes) == 1:
        kf = keyframes[0]
        # å•ä¸ªkeyframeè§†ä¸ºåªåœ¨è¯¥å¸§æœ‰æ•ˆ
        if target_frame_idx == kf['frame']:
            return kf['bbox'], True
        else:
            return [0.0, 0.0, 0.0, 0.0], False
    
    # æŒ‰å¸§å·æ’åº
    sorted_keyframes = sorted(keyframes, key=lambda x: x['frame'])
    
    # è·å–å¼‚å¸¸æ—¶æ®µçš„èµ·å§‹å’Œç»“æŸå¸§
    start_frame = sorted_keyframes[0]['frame']
    end_frame = sorted_keyframes[-1]['frame']
    
    # âœ… å…³é”®ä¿®æ”¹ï¼šåªåœ¨å¼‚å¸¸æ—¶æ®µå†…è¿”å›æœ‰æ•ˆbbox
    if target_frame_idx < start_frame or target_frame_idx > end_frame:
        # éå¼‚å¸¸æ—¶æ®µï¼šè¿”å›å…¨é›¶ + æ— æ•ˆæ ‡è®°
        return [0.0, 0.0, 0.0, 0.0], False
    
    # åœ¨å¼‚å¸¸æ—¶æ®µå†…ï¼šæ’å€¼è®¡ç®—
    for i in range(len(sorted_keyframes) - 1):
        kf1 = sorted_keyframes[i]
        kf2 = sorted_keyframes[i + 1]
        
        if kf1['frame'] <= target_frame_idx <= kf2['frame']:
            if not interpolate:
                # è¿”å›æœ€è¿‘çš„ keyframe
                if abs(target_frame_idx - kf1['frame']) < abs(target_frame_idx - kf2['frame']):
                    return kf1['bbox'], True
                else:
                    return kf2['bbox'], True
            else:
                # çº¿æ€§æ’å€¼
                t = (target_frame_idx - kf1['frame']) / (kf2['frame'] - kf1['frame'])
                bbox = [
                    kf1['bbox'][j] * (1 - t) + kf2['bbox'][j] * t
                    for j in range(4)
                ]
                return bbox, True
    
    # é»˜è®¤è¿”å›æœ€åä¸€ä¸ª keyframeï¼ˆåœ¨æ—¶æ®µå†…ï¼‰
    return sorted_keyframes[-1]['bbox'], True
# ==========================================


def load_video_frames(video_path: str, target_size: tuple = (224, 224), timestamps: tuple = None, frame_range: tuple = None):
    """
    è¯»å–è§†é¢‘å¹¶è¿”å›å¸§åˆ—è¡¨ï¼ˆPIL.Image æ ¼å¼ï¼‰å’Œå¸§ç´¢å¼•
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        target_size: ç›®æ ‡å°ºå¯¸ (height, width)
        timestamps: Optional[Tuple[float, float]] - (start_sec, end_sec) æ—¶é—´æ®µèŒƒå›´ï¼ˆç§’çº§ï¼‰
        frame_range: Optional[Tuple[int, int]] - (start_frame, end_frame) å¸§çº§èŒƒå›´ï¼Œä¼˜å…ˆçº§é«˜äº timestamps
    
    Returns:
        frames: PIL.Image å¯¹è±¡çš„åˆ—è¡¨
        frame_indices: å¯¹åº”çš„å¸§ç´¢å¼•åˆ—è¡¨ï¼ˆç›¸å¯¹äºå®Œæ•´è§†é¢‘çš„å¸§å·ï¼‰
    """
    # âœ… å¢å¼ºé”™è¯¯å¤„ç†
    if not os.path.exists(video_path):
        raise FileNotFoundError(
            f"âŒ Video file not found: {video_path}\n"
            f"   Please check:\n"
            f"   1. File path is correct\n"
            f"   2. File permissions are readable\n"
            f"   3. image_folder parameter points to correct directory"
        )
    
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise RuntimeError(
            f"âŒ Cannot open video file: {video_path}\n"
            f"   Possible reasons:\n"
            f"   1. Video file is corrupted\n"
            f"   2. Video codec not supported by OpenCV\n"
            f"   3. File permissions issue"
        )
    
    # âœ… ä½¿ç”¨try-finallyç¡®ä¿capä¸€å®šä¼šè¢«é‡Šæ”¾
    try:
        # âœ… è·å–è§†é¢‘å…ƒä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # âœ… ç¡®å®šé‡‡æ ·èŒƒå›´ï¼ˆä¼˜å…ˆä½¿ç”¨ frame_rangeï¼Œå…¶æ¬¡ timestampsï¼‰
        if frame_range is not None:
            start_frame, end_frame = frame_range
            start_frame = max(0, start_frame)
            end_frame = min(total_frames, end_frame)
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        elif timestamps is not None:
            start_sec, end_sec = timestamps
            start_frame = max(0, int(start_sec * fps))
            end_frame = min(total_frames, int(end_sec * fps))
            
            # ç²¾ç¡®seekåˆ°å¼€å§‹å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        else:
            start_frame = 0
            end_frame = total_frames
        
        frames = []
        frame_indices = []
        current_frame = start_frame
        
        # âœ… é™åˆ¶æœ€å¤§å¸§æ•°,é˜²æ­¢å†…å­˜çˆ†ç‚¸
        MAX_FRAMES = 512  # ç¡¬æ€§ä¸Šé™,é˜²æ­¢å¼‚å¸¸é•¿è§†é¢‘
        
        while cap.isOpened() and current_frame < end_frame and len(frames) < MAX_FRAMES:
            ret, frame = cap.read()
            if not ret:
                break
            
            # BGR -> RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸º PIL Image å¹¶ resize
            pil_frame = Image.fromarray(frame).resize(target_size, Image.BILINEAR)
            frames.append(pil_frame)
            frame_indices.append(current_frame)  # âœ… ä¿å­˜çš„æ˜¯ç›¸å¯¹äºå®Œæ•´è§†é¢‘çš„å¸§å·
            current_frame += 1
        
        # âœ… éªŒè¯è¯»å–ç»“æœ
        if len(frames) == 0:
            raise RuntimeError(
                f"âŒ No frames extracted from video: {video_path}\n"
                f"   Timestamps: {timestamps}\n"
                f"   Start frame: {start_frame}, End frame: {end_frame}\n"
                f"   Video might be corrupted or timestamp range invalid"
            )
        
        return frames, frame_indices
        
    finally:
        # âœ… æ— è®ºå¦‚ä½•éƒ½é‡Šæ”¾VideoCaptureèµ„æº
        cap.release()
# ==========================================

@dataclass
class ModelArguments:
    model_name_or_path: Optional[str] = field(default="facebook/opt-125m")
    version: Optional[str] = field(default="v0")
    freeze_backbone: bool = field(default=False)
    tune_mm_mlp_adapter: bool = field(default=False)
    vision_tower: Optional[str] = field(default=None)
    base_model: Optional[str] = field(default=None)
    download_root: Optional[str] = field(default=None)
    log_scale: float = 4.6052

@dataclass
class DataArguments:
    data_path: str = field(default=None,
                           metadata={"help": "Path to the training data."})
    lazy_preprocess: bool = False
    is_multimodal: bool = False
    image_folder: Optional[str] = field(default=None)
    image_aspect_ratio: str = 'square'
    image_grid_pinpoints: Optional[str] = field(default=None)
    max_seq_length: int = 77*4-60
    base_seq_length: int = 77
    base_image_size: int = 224
    add_box_loss: bool = field(default=False)
    use_hard_neg: bool = field(default=False)
    
    # æ–°å¢ï¼šè§†é¢‘ç›¸å…³å‚æ•°
    is_video: bool = field(default=False, metadata={"help": "Whether to process video instead of images"})
    num_frames: int = field(default=256, metadata={"help": "Number of frames to sample from each video"})
    
    # ğŸ”¬ å®éªŒå¼€å…³ï¼šæµ‹è¯•region_texté•¿åº¦å¯¹æ”¶æ•›çš„å½±å“
    use_simple_region_text: bool = field(
        default=False, 
        metadata={"help": "å®éªŒé€‰é¡¹ï¼šä½¿ç”¨ç®€åŒ–çš„region text ('Region: ' + global_caption) è€ŒéåŸå§‹çš„detailed region_captionsï¼Œç”¨äºæµ‹è¯•texté•¿åº¦æ˜¯å¦å¯¼è‡´æ”¶æ•›é—®é¢˜"}
    )


    

@dataclass
class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    optim: str = field(default="adamw_torch")
    remove_unused_columns: bool = field(default=False)
    freeze_mm_mlp_adapter: bool = field(default=False)
    mpt_attn_impl: Optional[str] = field(default="triton")
    model_max_length: int = field(
        default=512,
        metadata={
            "help":
            "Maximum sequence length. Sequences will be right padded (and possibly truncated)."
        },
    )
    double_quant: bool = field(
        default=True,
        metadata={"help": "Compress the quantization statistics through double quantization."}
    )
    quant_type: str = field(
        default="nf4",
        metadata={"help": "Quantization data type to use. Should be one of `fp4` or `nf4`."}
    )
    bits: int = field(
        default=16,
        metadata={"help": "How many bits to use."}
    )
    auto_resume: bool = field(
        default=True,
        metadata={"help": "Automatically resume from the latest checkpoint in output_dir if available."}
    )
    disable_global_loss: bool = field(
        default=False,
        metadata={"help": "If True, global contrastive loss is disabled (Region-only training)."}
    )
    lora_enable: bool = False
    lora_r: int = 64
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    lora_weight_path: str = ""
    lora_bias: str = "none"
    from_openai: bool = field(default=False)
    train_use_word_size: int = 8
    text_model_lr: Optional[float] = None


from datetime import datetime
    
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,
                                   output_dir: str):
    """Collects the state dict and dump to disk."""

    if trainer.deepspeed:
        torch.cuda.synchronize()
        trainer.save_model(output_dir)
        return

    model = trainer.model

    # LoRA åœºæ™¯ï¼šåªä¿å­˜é€‚é…å™¨æƒé‡
    if getattr(model, "peft_config", None) is not None:
        if trainer.args.should_save:
            trainer._save(output_dir, state_dict=get_peft_model_state_dict(model))
            if getattr(trainer, "tokenizer", None) is not None:
                trainer.tokenizer.save_pretrained(output_dir)
        return

    state_dict = model.state_dict()

    if trainer.args.should_save:
        cpu_state_dict = {
            key: value.cpu()
            for key, value in state_dict.items()
        }
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


class LazySupervisedBboxDataset(Dataset):
    """Dataset for VAD training, supports both normal and abnormal videos."""

    def __init__(self, data_path: str,
                 data_args: DataArguments,
                 img_preprocess=None,tokenizer=None):
        super(LazySupervisedBboxDataset, self).__init__()

        # ========== ä¿®æ”¹1+4+5: é‡æ„æ•°æ®åŠ è½½é€»è¾‘ï¼Œæ”¯æŒå¤šä¸ªJSONæ–‡ä»¶å’Œå­—å…¸/åˆ—è¡¨æ ¼å¼ ==========
        list_data_dict = []
        
        if data_path.endswith('.json'):
            # å•ä¸ªJSONæ–‡ä»¶
            rank0_print(f"Loading data from: {data_path}")
            data = json.load(open(data_path, "r", encoding="utf-8"))
            
            # âœ… è‡ªé€‚åº”ï¼šæ£€æµ‹æ˜¯åˆ—è¡¨è¿˜æ˜¯å­—å…¸æ ¼å¼
            if isinstance(data, list):
                # æ–°æ ¼å¼ (ucf_fgclip_train_final.json): åˆ—è¡¨æ ¼å¼
                rank0_print("  â†’ Detected list format (new)")
                list_data_dict = self._convert_list_format_to_internal(data)
            elif isinstance(data, dict):
                # æ—§æ ¼å¼: å­—å…¸æ ¼å¼
                rank0_print("  â†’ Detected dict format (legacy)")
                list_data_dict = self._convert_dict_to_list(data)
            else:
                raise ValueError(f"Unsupported data format: {type(data)}")
                
        elif data_path.endswith('.txt'):
            # txtæ–‡ä»¶ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªJSONæ–‡ä»¶è·¯å¾„
            lines = open(data_path, "r", encoding="utf-8").readlines()
            for line in lines:
                json_file = line.rstrip()
                rank0_print(f"Loading data from: {json_file}")
                data = json.load(open(json_file, "r", encoding="utf-8"))
                
                # è‡ªé€‚åº”å¤„ç†
                if isinstance(data, list):
                    list_data_dict += self._convert_list_format_to_internal(data)
                elif isinstance(data, dict):
                    list_data_dict += self._convert_dict_to_list(data)
                    
        else:
            # ç›®å½•è·¯å¾„ï¼ŒåŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
            json_files = glob.glob(os.path.join(data_path, '*.json'))
            for json_file in json_files:
                rank0_print(f"Loading data from: {json_file}")
                data = json.load(open(json_file, "r", encoding="utf-8"))
                
                # è‡ªé€‚åº”å¤„ç†
                if isinstance(data, list):
                    list_data_dict += self._convert_list_format_to_internal(data)
                elif isinstance(data, dict):
                    list_data_dict += self._convert_dict_to_list(data)

        rank0_print(f"Total videos loaded: {len(list_data_dict)}")
        rank0_print(f"  - Normal videos: {sum(1 for x in list_data_dict if not x['is_abnormal'])}")
        rank0_print(f"  - Abnormal videos: {sum(1 for x in list_data_dict if x['is_abnormal'])}")
        
        # ğŸ”¬ å®éªŒå¼€å…³æç¤º
        if data_args.use_simple_region_text:
            rank0_print("\n" + "="*80)
            rank0_print("ğŸ”¬ å®éªŒæ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–Region Text")
            rank0_print("   Region caption = 'Region: ' + global_caption")
            rank0_print("   ç›®çš„ï¼šæµ‹è¯•detailed region_captionsæ˜¯å¦å› è¿‡é•¿å¯¼è‡´æ”¶æ•›å›°éš¾")
            rank0_print("="*80 + "\n")
        else:
            rank0_print("\nâœ… æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨åŸå§‹çš„detailed region captions\n")

        self.tokenizer = tokenizer
        self.list_data_dict = list_data_dict
        self.max_anns = 4

        self.data_args = data_args
        self.preprocess = img_preprocess
        self.image_root = data_args.image_folder
        self.max_length = data_args.max_seq_length
        self.base_length = data_args.base_seq_length
        self.base_image_size = data_args.base_image_size
        self.add_box_loss = data_args.add_box_loss
        self.use_hard_neg = data_args.use_hard_neg
        
        # è§†é¢‘ç›¸å…³å±æ€§
        self.is_video = data_args.is_video
        self.num_frames = data_args.num_frames if self.is_video else 1

        # ç¼“å­˜æ­£å¸¸/å¼‚å¸¸æ ·æœ¬ç´¢å¼•ç”¨äºå¹³è¡¡é‡‡æ ·
        self.normal_indices = [idx for idx, item in enumerate(self.list_data_dict) if not item["is_abnormal"]]
        self.abnormal_indices = [idx for idx, item in enumerate(self.list_data_dict) if item["is_abnormal"]]
        if len(self.normal_indices) == 0 or len(self.abnormal_indices) == 0:
            rank0_print("âš ï¸ Balanced sampling requires both normal and abnormal videos. Current dataset may be imbalanced.")
    
    def _convert_dict_to_list(self, data_dict: dict) -> list:
        """
        å°†å­—å…¸æ ¼å¼çš„æ•°æ®è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        è¾“å…¥æ ¼å¼: {"video_name.mp4": {"global": {...}, "region": [...]}, ...}
        è¾“å‡ºæ ¼å¼: [{"video_name": "...", "global": {...}, "region": [...], "is_abnormal": bool}, ...]
        """
        result = []
        for video_name, video_data in data_dict.items():
            # è·³è¿‡ç©ºæ•°æ®
            if not video_data or not isinstance(video_data, dict):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ global å’Œ region å­—æ®µ
            if 'global' not in video_data or 'region' not in video_data:
                rank0_print(f"Warning: Video {video_name} missing 'global' or 'region' field, skipping...")
                continue
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚å¸¸è§†é¢‘
            # å¼‚å¸¸è§†é¢‘çš„ç‰¹å¾: region åˆ—è¡¨ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå…ƒç´ åŒ…å« keyframes å­—æ®µ
            is_abnormal = False
            region_list = video_data.get('region', [])
            if isinstance(region_list, list) and len(region_list) > 0:
                for region in region_list:
                    if isinstance(region, dict) and 'keyframes' in region:
                        is_abnormal = True
                        break
            
            result.append({
                'video_name': video_name,
                'global': video_data['global'],
                'region': video_data['region'],
                'is_abnormal': is_abnormal
            })
        
        return result
    
    def _convert_list_format_to_internal(self, data_list: list) -> list:
        """
        å°†æ–°çš„åˆ—è¡¨æ ¼å¼è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼
        
        è¾“å…¥æ ¼å¼ (ucf_fgclip_train_with_timestamps.json):
        [
          {
            "video": "UCF_Crimes_Videos/Anomaly-Videos-Part-1/Abuse001_x264.mp4",
            "global_caption": "å…¨å±€æè¿°...",
            "region_captions": ["åŒºåŸŸ1", "åŒºåŸŸ2", ...],
            "box_infos": [[{frame, bbox}, ...], [{frame, bbox}, ...], ...],
            "timestamps": null  # æˆ– [start_sec, end_sec]
          }
        ]
        
        è¾“å‡ºæ ¼å¼ (å†…éƒ¨æ ¼å¼):
        [
          {
            "video_path": "UCF_Crimes_Videos/Anomaly-Videos-Part-1/Abuse001_x264.mp4",
            "video_name": "Abuse001_x264.mp4",
            "global": {"Caption": "å…¨å±€æè¿°..."},
            "region": [{"caption": "åŒºåŸŸ1", "keyframes": [...]}, ...],
            "timestamps": null,
            "is_abnormal": True
          }
        ]
        """
        result = []
        
        for item in data_list:
            if not item or not isinstance(item, dict):
                continue
            
            # âœ… æ–°æ ¼å¼ï¼šç›´æ¥æå–videoå­—æ®µ
            video_path = item.get('video', item.get('f_path', ''))
            video_name = os.path.basename(video_path)
            
            # âœ… æå–å…¨å±€æè¿°
            global_caption = item.get('global_caption', '')
            
            # âœ… æå–timestampsï¼ˆæ–°å¢ï¼‰
            timestamps = item.get('timestamps', None)
            
            # âœ… æ„å»ºregionåˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ªregionï¼‰
            region_captions = item.get('region_captions', [])
            box_infos = item.get('box_infos', [])
            
            # å…¼å®¹æ—§æ ¼å¼çš„bbox_info
            if not region_captions and 'bbox_info' in item:
                bbox_info = item.get('bbox_info', [])
                region_list = bbox_info
            else:
                # æ–°æ ¼å¼ï¼šåˆå¹¶region_captionså’Œbox_infos
                region_list = []
                for i, caption in enumerate(region_captions):
                    keyframes = []
                    if i < len(box_infos):
                        # å°†box_infoè½¬æ¢ä¸ºkeyframesæ ¼å¼
                        for box_item in box_infos[i]:
                            if isinstance(box_item, dict):
                                keyframes.append({
                                    'frame': box_item.get('frame', 0),
                                    'bbox': box_item.get('bbox', [0, 0, 0, 0]),
                                    'enabled': True
                                })
                    
                    region_list.append({
                        'caption': caption,
                        'keyframes': keyframes
                    })
            
            # åˆ¤æ–­æ˜¯å¦å¼‚å¸¸ï¼ˆæœ‰keyframeså­—æ®µä¸”ä¸ä¸ºç©ºï¼‰
            is_abnormal = any(
                isinstance(region, dict) and 
                'keyframes' in region and 
                len(region.get('keyframes', [])) > 0
                for region in region_list
            )
            
            # æ„å»ºå†…éƒ¨æ ¼å¼
            result.append({
                'video_path': video_path,  # âœ… æ–°å¢ï¼šå®Œæ•´è·¯å¾„
                'video_name': video_name,
                'global': {'Caption': global_caption},
                'region': region_list,
                'timestamps': timestamps,  # âœ… æ–°å¢
                'is_abnormal': is_abnormal
            })
        
        return result

    def __len__(self):
        return len(self.list_data_dict)

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        item = self.list_data_dict[i]
        # print(item)

        # ========== ä¿®æ”¹1+3: ä»æ–°æ•°æ®æ ¼å¼ä¸­æå–ä¿¡æ¯ ==========
        video_name = item['video_name']
        global_data = item['global']
        region_data = item['region']
        is_abnormal = item['is_abnormal']
        video_path = item['video_path']
        
        # æå–æ–‡æœ¬æè¿°
        global_caption = global_data.get('Caption', global_data.get('caption', ''))
        
        # å¯¹äº region captionï¼Œå¦‚æœæœ‰å¤šä¸ª regionï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆç”¨äºshort_textï¼‰
        # æ­£å¸¸è§†é¢‘: region ä¸­åªæœ‰ caption å­—æ®µ
        # å¼‚å¸¸è§†é¢‘: region ä¸­æœ‰ caption å’Œ keyframes å­—æ®µ
        if isinstance(region_data, list) and len(region_data) > 0:
            region_caption = region_data[0].get('caption', region_data[0].get('Caption', ''))
        else:
            # å¦‚æœæ²¡æœ‰ region æ•°æ®ï¼Œä½¿ç”¨ global caption
            region_caption = global_caption
        
        # ç›´æ¥ä½¿ç”¨åŸå§‹çš„ region captionï¼Œä¸æ·»åŠ å‰ç¼€å’Œæˆªæ–­
        caption_short = region_caption

        # ========== ä¿®æ”¹3: æ„å»ºæ­£ç¡®çš„è§†é¢‘è·¯å¾„ ==========
        # æ•°æ®æ ¼å¼: f_path = "UCF_Crimes_Videos/Abuse001_x264.mp4"
        # å®é™…è·¯å¾„: /data/zyy/dataset/UCF_Crimes_Videos/UCF_Crimes/Videos/Abuse/Abuse001_x264.mp4
        
        # âœ… æ–°å¢ï¼šè·å–timestampsç”¨äºæ­£å¸¸è§†é¢‘çš„æ—¶é—´æ®µæˆªå–
        timestamps = item.get('timestamps', None)  # [start_sec, end_sec] or None

        folder = video_path.split('/')[-2]  # UCF_Crimes_Videos
        
        video_category = self._extract_category_from_filename(video_name, folder)
        # print(video_category)
        
        # âœ… ä¿®å¤ï¼šæ·»åŠ å®Œæ•´çš„è·¯å¾„ç»“æ„
        video_full_path = os.path.join(
            self.image_root,           # /data/zyy/dataset
            "UCF_Crimes_Videos",       # æ•°æ®é›†æ ¹ç›®å½•
            "UCF_Crimes",              # UCF_Crimes å­ç›®å½•
            "Videos",                  # Videos ç›®å½•
            video_category,            # ç±»åˆ«ç›®å½• (Abuse, Arrest, etc.)
            video_name                 # è§†é¢‘æ–‡ä»¶å
        )
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œè®­ç»ƒæ—¶å¯ä»¥æ³¨é‡Šæ‰ï¼‰
        # rank0_print(f"Video path: {video_full_path}")
        
        # ========== è§†é¢‘å¤„ç†ï¼ˆä¿®æ”¹ï¼šæ”¯æŒtimestampsæ—¶é—´æ®µæˆªå–ï¼‰==========
        if self.is_video:
            # 1. åŠ è½½è§†é¢‘å¸§å’Œå¸§ç´¢å¼•ï¼ˆâœ… æ–°å¢timestampså‚æ•°ï¼‰
            # print(video_full_path)
            frames, frame_indices = load_video_frames(
                video_full_path, 
                target_size=(self.base_image_size, self.base_image_size),
                timestamps=timestamps  # âœ… æ”¯æŒæ­£å¸¸è§†é¢‘çš„æ—¶é—´æ®µæˆªå–
            )
            original_num_frames = len(frames)
            
            # 2. é‡‡æ ·æˆ–å¡«å……åˆ°å›ºå®šå¸§æ•°
            if len(frames) > self.num_frames:
                frames, sampled_frame_indices = uniform_sample_frames(frames, frame_indices, self.num_frames)
            elif len(frames) < self.num_frames:
                frames, sampled_frame_indices = pad_frames(frames, frame_indices, self.num_frames)
            else:
                sampled_frame_indices = frame_indices
            
            # 3. é¢„å¤„ç†æ¯ä¸€å¸§å¹¶å †å 
            frame_tensors = []
            for frame in frames:
                tensor = self.preprocess.preprocess(frame, return_tensors='pt')['pixel_values'][0]
                frame_tensors.append(tensor)
            
            video_tensor = torch.stack(frame_tensors, dim=0)  # (T, C, H, W)
            
            # 4. åˆ›å»ºæ³¨æ„åŠ›æ©ç 
            attention_mask = torch.zeros(self.num_frames, dtype=torch.bool)
            attention_mask[:min(original_num_frames, self.num_frames)] = True
            
        else:
            # å›¾åƒå¤„ç†åˆ†æ”¯ï¼ˆä¿æŒå…¼å®¹ï¼‰
            image = Image.open(video_full_path).convert("RGB")
            image = image.resize((self.base_image_size, self.base_image_size))
            video_tensor = self.preprocess.preprocess(image, return_tensors='pt')['pixel_values'][0]
            video_tensor = video_tensor.unsqueeze(0)
            attention_mask = torch.ones(1, dtype=torch.bool)
            sampled_frame_indices = [0]
        
        # ========== æ–‡æœ¬å¤„ç† ==========
        text = torch.tensor(
            self.tokenizer([global_caption], max_length=self.max_length, 
                          padding="max_length", truncation=True).input_ids, 
            dtype=torch.long, device=video_tensor.device
        )
        short_text = torch.tensor(
            self.tokenizer([caption_short], max_length=self.base_length, 
                          padding="max_length", truncation=True).input_ids, 
            dtype=torch.long, device=video_tensor.device
        )

        # ========== Region-Aware è§†é¢‘é‡‡æ · ==========
        # ä¸ºæ¯ä¸ª region åœ¨å…¶æ—¶é—´æ®µå†…å•ç‹¬é‡‡æ ·å¸§
        # âš ï¸ å†…å­˜ä¼˜åŒ–ç­–ç•¥ï¼šRegion ä½¿ç”¨æ›´å°‘çš„å¸§æ•°ä»¥é€‚é… 24GB æ˜¾å­˜
        # - Global: 256 å¸§ï¼ˆå®Œæ•´æ—¶åºï¼‰
        # - Region: 96 å¸§ï¼ˆ3/8 å¸§æ•°ï¼Œå¹³è¡¡è¯­ä¹‰å®Œæ•´æ€§å’Œæ˜¾å­˜ï¼‰
        region_num_frames = max(64, self.num_frames // 8)  # âœ… 64 â†’ 96 å¸§ï¼ˆ+50%ï¼‰
        
        region_videos_list = []
        if self.add_box_loss and self.is_video:
            for region_idx in range(min(len(region_data), self.max_anns)):
                region_info = region_data[region_idx]
                keyframes = region_info.get("keyframes", [])
                
                if len(keyframes) > 1:
                    # æå– keyframes çš„æ—¶é—´èŒƒå›´
                    all_frame_nums = [kf['frame'] for kf in keyframes]
                    
                    # Region æ—¶é—´æ®µï¼šé¦–å°¾ keyframe
                    start_frame = min(all_frame_nums)
                    end_frame = max(all_frame_nums)
                    
                    # åœ¨æ—¶é—´æ®µå†…åŠ è½½å¹¶é‡‡æ ·å¸§
                    region_frames, region_frame_indices = load_video_frames(
                        video_full_path,
                        target_size=(self.base_image_size, self.base_image_size),
                        frame_range=(start_frame, end_frame)  # âœ… ä½¿ç”¨å¸§èŒƒå›´å‚æ•°
                    )
                    
                    # âœ… é‡‡æ ·æˆ–å¡«å……åˆ° region_num_framesï¼ˆè€Œé num_framesï¼‰
                    if len(region_frames) > region_num_frames:
                        region_frames, _ = uniform_sample_frames(region_frames, region_frame_indices, region_num_frames)
                    elif len(region_frames) < region_num_frames:
                        region_frames, _ = pad_frames(region_frames, region_frame_indices, region_num_frames)
                    
                    # é¢„å¤„ç†å¸§
                    region_frame_tensors = []
                    for frame in region_frames:
                        tensor = self.preprocess.preprocess(frame, return_tensors='pt')['pixel_values'][0]
                        region_frame_tensors.append(tensor)
                    
                    region_video = torch.stack(region_frame_tensors, dim=0)  # (region_num_frames, C, H, W)
                    region_videos_list.append(region_video)
                else:
                    # å•ä¸ª keyframe æˆ–æ²¡æœ‰ keyframesï¼šä½¿ç”¨é›¶å¡«å……
                    region_videos_list.append(torch.zeros((region_num_frames, 3, self.base_image_size, self.base_image_size), 
                                                          device=video_tensor.device))
            
            # å¡«å……åˆ° max_anns
            while len(region_videos_list) < self.max_anns:
                region_videos_list.append(torch.zeros((region_num_frames, 3, self.base_image_size, self.base_image_size),
                                                      device=video_tensor.device))
            
            # å †å ä¸º (max_anns, region_num_frames, C, H, W)
            region_videos = torch.stack(region_videos_list, dim=0)
        else:
            region_videos = None

        # ========== ä¿®æ”¹2+4: Bounding Box å¤„ç†ï¼ˆæ–¹æ¡ˆAï¼šæ”¯æŒåŠ¨æ€bbox + maskï¼‰==========
        if self.add_box_loss:
            total_num = self.max_anns
            valid_num = min(len(region_data), self.max_anns)
            
            # ========== æ–¹æ¡ˆAï¼šä¸ºæ¯ä¸€å¸§è®¡ç®— bbox å’Œ mask ==========
            if self.is_video:
                # ä¸ºæ¯ä¸€å¸§ç”Ÿæˆ bbox: (T, max_anns, 4)
                boxes_template = torch.zeros((self.num_frames, total_num, 4), device=video_tensor.device)
                # âœ… æ–°å¢ï¼šbbox_mask æ ‡è®°æœ‰æ•ˆæ€§ (T, max_anns)
                bbox_mask = torch.zeros((self.num_frames, total_num), dtype=torch.bool, device=video_tensor.device)
                
                for frame_idx, original_frame_idx in enumerate(sampled_frame_indices):
                    for i in range(total_num):
                        if i < valid_num:
                            region_item = region_data[i]
                            
                            # å…³é”®é€»è¾‘ï¼šåˆ¤æ–­æ˜¯å¦æœ‰ keyframes
                            if 'keyframes' in region_item and len(region_item['keyframes']) > 0:
                                # âœ… å¼‚å¸¸è§†é¢‘ï¼šä» keyframes ä¸­æ’å€¼è®¡ç®—å½“å‰å¸§çš„ bbox
                                box, is_valid = find_closest_keyframe_bbox(
                                    region_item['keyframes'], 
                                    original_frame_idx,  # ä½¿ç”¨çœŸå®çš„å¸§ç´¢å¼•
                                    interpolate=True
                                )
                                boxes_template[frame_idx, i] = torch.tensor(box[:4], dtype=torch.float32)
                                bbox_mask[frame_idx, i] = is_valid  # âœ… è®°å½•æœ‰æ•ˆæ€§
                            else:
                                # âœ… æ­£å¸¸è§†é¢‘ï¼šè™šæ‹Ÿ bboxï¼ˆè¦†ç›–æ•´ä¸ªç”»é¢ï¼‰
                                box = [0.0, 0.0, 1.0, 1.0]
                                boxes_template[frame_idx, i] = torch.tensor(box[:4], dtype=torch.float32)
                                bbox_mask[frame_idx, i] = True  # æ­£å¸¸è§†é¢‘çš„è™šæ‹Ÿbboxæ ‡è®°ä¸ºæœ‰æ•ˆ
                        else:
                            # å¡«å……æ— æ•ˆ box
                            boxes_template[frame_idx, i] = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                            bbox_mask[frame_idx, i] = False  # paddingåŒºåŸŸæ ‡è®°ä¸ºæ— æ•ˆ
                
                # âœ… æ–¹æ¡ˆAï¼šä¿ç•™å®Œæ•´çš„åŠ¨æ€bboxï¼Œä¸å†åªå–ä¸­é—´å¸§
                boxes_template_dynamic = boxes_template  # (T, max_anns, 4)
                
            else:
                # å›¾åƒæ¨¡å¼ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†ä¹Ÿè¿”å›mask
                boxes_template_dynamic = torch.zeros((1, total_num, 4), device=video_tensor.device)
                bbox_mask = torch.zeros((1, total_num), dtype=torch.bool, device=video_tensor.device)

                for i in range(total_num):
                    if i < valid_num:
                        region_item = region_data[i]
                        
                        if 'keyframes' in region_item and len(region_item['keyframes']) > 0:
                            mid_frame_idx = (region_item.get('start_frame', 0) + region_item.get('end_frame', 0)) // 2
                            box, is_valid = find_closest_keyframe_bbox(
                                region_item['keyframes'], 
                                mid_frame_idx, 
                                interpolate=True
                            )
                            boxes_template_dynamic[0, i] = torch.tensor(box[:4], dtype=torch.float32)
                            bbox_mask[0, i] = is_valid
                        else:
                            box = [0.0, 0.0, 1.0, 1.0]
                            boxes_template_dynamic[0, i] = torch.tensor(box[:4], dtype=torch.float32)
                            bbox_mask[0, i] = True
                    else:
                        boxes_template_dynamic[0, i] = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                        bbox_mask[0, i] = False
            
            # ========== Box caption å¤„ç† ==========
            # ğŸ”¬ å®éªŒå¼€å…³ï¼šæµ‹è¯•region texté•¿åº¦å¯¹æ”¶æ•›çš„å½±å“
            box_texts = []
            for i in range(total_num):
                if i < valid_num:
                    region_item = region_data[i]
                    
                    if self.data_args.use_simple_region_text:
                        # ğŸ”¬ å®éªŒæ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–textï¼ˆ"Region: " + global_captionï¼‰
                        # ç›®çš„ï¼šæµ‹è¯•æ˜¯å¦æ˜¯detailed region_captionså¤ªé•¿å¯¼è‡´æ”¶æ•›å›°éš¾
                        box_caption = f"Region: {global_caption}"  # âœ… ä¿®å¤ï¼šä½¿ç”¨global_captionå˜é‡
                    else:
                        # âœ… æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨åŸå§‹çš„detailed region caption
                        box_caption = region_item.get('caption', region_item.get('Caption', ''))
                else:
                    box_caption = ""
                
                # ç¼–ç  box caption
                box_text = torch.tensor(
                    self.tokenizer([box_caption], max_length=self.base_length, 
                                   padding="max_length", truncation=True).input_ids, 
                    dtype=torch.long, device=video_tensor.device
                )
                box_texts.append(box_text)

            box_texts = torch.cat(box_texts, dim=0)
            bbox_num = torch.tensor([valid_num], device=video_tensor.device)
                    
        if self.use_hard_neg:
            # ========== Hard Negativeç”Ÿæˆ ==========
            # ç­–ç•¥ï¼šè·¨ç±»åˆ«è¯­ä¹‰æ··æ·† + ç»†ç²’åº¦æè¿°å·®å¼‚
            # ä¸ºæ¯ä¸ªregion bboxç”Ÿæˆ10ä¸ªHard Negative captionsï¼ˆæ€»å…±11ä¸ªå€™é€‰ï¼š1æ­£+10è´Ÿï¼‰
            from fgclip.data.hard_negatives import generate_hard_negatives
            
            hard_texts = []
            hard_boxes = torch.zeros((self.max_anns, 4), device=video_tensor.device)
            valid_hard = 0
            
            # ä¸ºæ¯ä¸ªæœ‰æ•ˆbboxç”ŸæˆHard Negatives
            for bbox_idx in range(valid_num):
                # è·å–å½“å‰bboxçš„ç±»åˆ«å’Œcaption
                box_caption = region_data[bbox_idx].get('caption', region_data[bbox_idx].get('Caption', ''))
                
                # æ ¹æ®è§†é¢‘ç±»åˆ«ç”ŸæˆHard Negatives
                # UCF-Crimeç±»åˆ«ä»è§†é¢‘åç§°ä¸­æå–ï¼ˆå¦‚Abuse001_x264.mp4 â†’ Abuseï¼‰
                video_category = self._extract_category_from_filename(video_name, folder)
                
                # ç”Ÿæˆ11ä¸ªå€™é€‰captionï¼ˆ1æ­£æ ·æœ¬ + 10 hard negativesï¼‰
                # generate_hard_negativesè¿”å›: [æ­£æ ·æœ¬, neg1, neg2, ..., neg10]
                candidates = generate_hard_negatives(
                    category=video_category,
                    original_caption=box_caption,
                    num_negatives=10,
                    include_positive=True
                )
                
                # Tokenizeæ‰€æœ‰11ä¸ªå€™é€‰caption
                candidate_tokens = torch.tensor(
                    self.tokenizer(
                        candidates,
                        max_length=self.base_length,
                        padding="max_length",
                        truncation=True
                    ).input_ids,
                    dtype=torch.long,
                    device=video_tensor.device
                )  # Shape: (11, max_length)
                
                hard_texts.append(candidate_tokens)
                hard_boxes[valid_hard] = boxes_template_dynamic[0, bbox_idx]  # ä½¿ç”¨ç¬¬ä¸€å¸§çš„bboxï¼ˆç®€åŒ–ï¼‰
                valid_hard += 1
            
            # æ‹¼æ¥æ‰€æœ‰hard negative texts
            if len(hard_texts) > 0:
                hard_texts = torch.stack(hard_texts, dim=0)  # (num_regions, 11, max_length)
                # âš ï¸ æ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼æ˜¯(num_regions * 11, max_length)ï¼Œéœ€è¦reshape
                hard_texts = hard_texts.view(-1, hard_texts.shape[-1])  # (num_regions * 11, max_length)
            else:
                hard_texts = None
            
            valid_hard = torch.tensor([valid_hard], device=video_tensor.device)

        # ========== æ„å»ºè¿”å›å­—å…¸ ==========
        data_dict = {}
        data_dict['video'] = video_tensor
        data_dict['video_attention_mask'] = attention_mask  # æ”¹åä¸ºvideo_attention_mask
        data_dict['text'] = text
        data_dict['short_text'] = short_text
        data_dict['add_box_loss'] = self.add_box_loss
        data_dict['use_hard_neg'] = self.use_hard_neg

        if self.add_box_loss:
            data_dict['box_texts'] = box_texts
            data_dict['box_infos'] = boxes_template_dynamic  # âœ… ä½¿ç”¨åŠ¨æ€bbox (T, max_anns, 4)
            data_dict['bbox_mask'] = bbox_mask  # âœ… æ–°å¢ï¼šbboxæœ‰æ•ˆæ€§mask (T, max_anns)
            data_dict['box_nums'] = bbox_num
            data_dict['region_videos'] = region_videos  # âœ… æ–°å¢ï¼šregionç‹¬ç«‹é‡‡æ ·çš„è§†é¢‘ (max_anns, T, C, H, W)
        if self.use_hard_neg:
            data_dict['hard_texts'] = hard_texts
            data_dict['hard_infos'] = hard_boxes
            data_dict['hard_nums'] = valid_hard
            
        return data_dict

    def _extract_category_from_filename(self, filename: str, folder: str) -> str:
        """
        ä»æ–‡ä»¶åä¸­æå–ç±»åˆ«åç§°
        ä¾‹å¦‚: "Abuse001_x264.mp4" -> "Abuse"
              "Normal_Videos476_x264.mp4" -> "Training_Normal_Videos_Anomaly" (æ­£å¸¸è§†é¢‘)
        """
        if filename.startswith("Normal_Videos"):
            # æ­£å¸¸è§†é¢‘æ”¾åœ¨ç‰¹å®šæ–‡ä»¶å¤¹
            return folder
        else:
            # å¼‚å¸¸è§†é¢‘ï¼šæå–ç±»åˆ«åï¼ˆå»æ‰æ•°å­—å’Œåç¼€ï¼‰
            import re
            match = re.match(r"([A-Za-z]+)", filename)
            if match:
                return match.group(1)
            else:
                raise ValueError(f"Cannot extract category from filename: {filename}")




@dataclass
class DataCollatorForSupervisedDataset(object):
    """Collate examples for supervised fine-tuning."""

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        
        batch = {}
        
        # ========== ä¿®æ”¹ï¼šå¤„ç†è§†é¢‘å¼ é‡å’Œè§†é¢‘æ³¨æ„åŠ›æ©ç  ==========
        videos = [instance['video'] for instance in instances]
        batch['image'] = torch.stack(videos)  # (B, T, C, H, W) æˆ– (B, 1, C, H, W) - æ”¹åä¸ºimageä»¥åŒ¹é…æ¨¡å‹forward
        
        # æ–°å¢ï¼šå †å è§†é¢‘æ³¨æ„åŠ›æ©ç 
        if 'video_attention_mask' in instances[0]:
            masks = [instance['video_attention_mask'] for instance in instances]
            batch['video_attention_mask'] = torch.stack(masks)  # (B, T)
        
        # ========== æ–‡æœ¬å¤„ç†ï¼ˆä¸å˜ï¼‰==========
        texts = [instance['text'] for instance in instances]
        batch['text_long'] = torch.cat(texts,dim=0)
        short_texts = [instance['short_text'] for instance in instances]
        batch['text_short'] = torch.cat(short_texts,dim=0)
        
        batch["add_box_loss"] = instances[0]["add_box_loss"]
        batch["use_hard_neg"] = instances[0]["use_hard_neg"]
        
        if batch["add_box_loss"]:
            box_texts = [instance['box_texts'] for instance in instances]
            batch['box_texts'] = torch.cat(box_texts,dim=0)
            box_infos = [instance['box_infos'] for instance in instances]
            batch['box_infos'] = torch.stack(box_infos, dim=0)  # âœ… æ”¹ä¸ºstack: (B, T, max_anns, 4)
            # âœ… æ–°å¢ï¼šå †å bbox_mask
            bbox_masks = [instance['bbox_mask'] for instance in instances]
            batch['bbox_mask'] = torch.stack(bbox_masks, dim=0)  # (B, T, max_anns)
            box_nums = [instance['box_nums'] for instance in instances]
            batch['box_nums'] = torch.cat(box_nums, dim=0)
            # âœ… æ–°å¢ï¼šå †å  region_videos
            if 'region_videos' in instances[0] and instances[0]['region_videos'] is not None:
                region_videos_list = [instance['region_videos'] for instance in instances]
                batch['region_videos'] = torch.stack(region_videos_list, dim=0)  # (B, max_anns, T, C, H, W)
            else:
                batch['region_videos'] = None
        if batch["use_hard_neg"] :
            hard_texts = []
            for instance in instances:
                if instance['hard_texts'] != None:
                    hard_texts.append(instance['hard_texts'])

            batch['hard_texts'] = torch.cat(hard_texts,dim=0)
            hard_infos = [instance['hard_infos'] for instance in instances]
            batch['hard_infos'] = torch.cat(hard_infos,dim=0)
            hard_nums = [instance['hard_nums'] for instance in instances]
            batch['hard_nums'] = torch.cat(hard_nums, dim=0)                

        return batch

def make_supervised_data_module(data_args,img_preprocess,tokenizer) -> Dict:
    """Make dataset and collator for supervised fine-tuning."""

    train_dataset = LazySupervisedBboxDataset(
                                data_path=data_args.data_path,
                                data_args=data_args,
                                img_preprocess=img_preprocess,tokenizer=tokenizer,)
                     
    data_collator = DataCollatorForSupervisedDataset()
    return dict(train_dataset=train_dataset,
                eval_dataset=None,
                data_collator=data_collator)


def train():
    global local_rank

    parser = transformers.HfArgumentParser(
        (ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
    local_rank = training_args.local_rank

    assert training_args.fp16 == False
    # NOTE Use HF-Transformers to train FG-CLIP no support FP16, the loss will be NAN
    if training_args.per_device_train_batch_size % 2 != 0:
        raise ValueError("Stage1 balanced trainingè¦æ±‚ per_device_train_batch_size ä¸ºå¶æ•°ï¼Œä»¥ç¡®ä¿æ­£è´Ÿæ ·æœ¬æ•°é‡ç›¸åŒã€‚")

    compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))

    # âœ… ä½¿ç”¨æœ¬åœ°CLIPåŠ è½½å™¨ï¼ˆä¸ä¾èµ–åœ¨çº¿ä¸‹è½½ï¼‰
    rank0_print("=" * 60)
    rank0_print("Loading CLIP components (LOCAL MODE - No Internet Required)")
    rank0_print("=" * 60)
    
    try:
        # æ–¹å¼1: ä½¿ç”¨æœ¬åœ°CLIPåŒ…è£…å™¨ï¼ˆæ¨èï¼‰
        rank0_print(f"Loading tokenizer for: {model_args.base_model}")
        tokenizer = LocalCLIPWrapper.get_tokenizer()
        rank0_print("  âœ“ Tokenizer loaded (local CLIP)")
        
        rank0_print(f"Loading image processor for: {model_args.base_model}")
        image_processor = LocalCLIPWrapper.get_image_processor()
        rank0_print("  âœ“ Image processor loaded (local CLIP)")
        
    except Exception as e:
        # æ–¹å¼2: å›é€€åˆ°HuggingFaceï¼ˆéœ€è¦ç½‘ç»œï¼‰
        rank0_print(f"  âœ— Local CLIP loading failed: {e}")
        rank0_print("  â†’ Falling back to HuggingFace (requires internet)")
        tokenizer = AutoTokenizer.from_pretrained(model_args.base_model)
        image_processor = CLIPImageProcessor.from_pretrained(model_args.base_model)

    local_checkpoint_dir = None
    if model_args.model_name_or_path:
        potential_dir = Path(model_args.model_name_or_path)
        if potential_dir.is_dir() and (potential_dir / "config.json").exists():
            local_checkpoint_dir = potential_dir.resolve()

    if local_checkpoint_dir is not None:
        rank0_print(f"Loading FG-CLIP model from local checkpoint: {local_checkpoint_dir}")
        model = FGCLIPModel.from_pretrained(str(local_checkpoint_dir), torch_dtype=torch.float32)
        config = model.config
        rank0_print("  âœ“ Local FG-CLIP weights loaded")
    else:
        rank0_print(f"Initializing FG-CLIP model: {model_args.base_model}")
        
        from fgclip.model.clip_strc.configuration_clip import (
            CLIPConfig, CLIPTextConfig, CLIPVisionConfig
        )
        
        text_config = CLIPTextConfig(
            vocab_size=49408,
            hidden_size=512,
            intermediate_size=2048,
            num_hidden_layers=12,
            num_attention_heads=8,
            max_position_embeddings=77,
            hidden_act="quick_gelu",
            layer_norm_eps=1e-5,
            dropout=0.0,
            attention_dropout=0.0,
            initializer_range=0.02,
            initializer_factor=1.0,
        )
        
        vision_config = CLIPVisionConfig(
            hidden_size=768,
            intermediate_size=3072,
            num_hidden_layers=12,
            num_attention_heads=12,
            num_channels=3,
            image_size=224,
            patch_size=32,
            hidden_act="quick_gelu",
            layer_norm_eps=1e-5,
            dropout=0.0,
            attention_dropout=0.0,
            initializer_range=0.02,
            initializer_factor=1.0,
        )
        
        config = CLIPConfig(
            text_config=text_config.to_dict(),
            vision_config=vision_config.to_dict(),
            projection_dim=512,
            logit_scale_init_value=4.6052,  # âœ… ä¿®å¤: ln(100) ç¡®ä¿æ¸©åº¦=100ï¼ˆCLIPæ ‡å‡†ï¼‰
        )
        
        model = FGCLIPModel(config)
        rank0_print("  âœ“ Model initialized (random weights)")
        rank0_print("=" * 60)

        config = model.config
    
    # âœ… ä¿®å¤ï¼šlogit_scaleåº”è¯¥ç›´æ¥å¤åˆ¶å€¼ï¼Œè€Œä¸æ˜¯ä¹˜ä»¥Parameterå¯¹è±¡
    # model.logit_scaleæ˜¯nn.Parameter(ln(100))ï¼Œéœ€è¦æå–å…¶data
    # ç†è®ºä¾æ®ï¼šCLIPåŸå§‹å®ç°ä¸­logit_scale = ln(temperature)ï¼Œè®­ç»ƒæ—¶å­¦ä¹ 
    model.logit_scale_finegraind = torch.nn.Parameter(model.logit_scale.data.clone())
    model.logit_scale_hardneg = torch.nn.Parameter(model.logit_scale.data.clone())

    # NOTE If only the second phase is trained, from_openai must be set to True
    if training_args.from_openai and local_checkpoint_dir is None:
        print("=" * 60)
        print("Loading OpenAI CLIP pretrained weights...")
        print("=" * 60)
        
        # âœ… åŠ è½½å®Œæ•´çš„OpenAI CLIPæƒé‡ï¼ˆVision + Textï¼‰
        loaded_keys, missing_keys = model.load_openai_clip_weights(model_args.base_model)
        
        # æ‰©å±•position embeddingç”¨äºé•¿æ–‡æœ¬
        print("Resizing position embeddings for long text...")
        model.resize_postion_embeding()
        
        # å¤åˆ¶æŠ•å½±å±‚æƒé‡
        model.copy_weight()
        
        print("=" * 60)
        print("âœ“ OpenAI CLIP weights loaded successfully")
        print("=" * 60)

    if training_args.lora_enable:
        rank0_print("Applying LoRA adapters ...")
        lora_path = pathlib.Path(training_args.lora_weight_path) if training_args.lora_weight_path else None
        if lora_path and lora_path.exists():
            rank0_print(f"  â†’ Loading existing LoRA adapter from {lora_path}")
            model = PeftModel.from_pretrained(model, str(lora_path), is_trainable=True)
        else:
            target_modules = [
                "q_proj",
                "k_proj",
                "v_proj",
                "out_proj",
                "fc1",
                "fc2",
                "visual_projection",
                "text_projection",
                "text_filip_projection",
                "roi_projection",
            ]
            try:
                task_type = TaskType.MULTI_MODAL
            except AttributeError:
                task_type = TaskType.FEATURE_EXTRACTION
            lora_config = LoraConfig(
                task_type=task_type,
                r=training_args.lora_r,
                lora_alpha=training_args.lora_alpha,
                lora_dropout=training_args.lora_dropout,
                bias=training_args.lora_bias,
                target_modules=target_modules,
            )
            model = get_peft_model(model, lora_config)
        # è¦†ç›–PeftModelçš„forwardï¼Œä¿æŒè‡ªå®šä¹‰FGCLIPModelçš„å…¥å‚é£æ ¼
        base_forward = model.base_model.forward

        def fgclip_peft_forward(*args, **kwargs):
            return base_forward(*args, **kwargs)

        model.forward = fgclip_peft_forward
        try:
            rank0_print("  âœ“ LoRA enabled. Trainable parameters:")
            model.print_trainable_parameters()
        except AttributeError:
            pass

    model.disable_global_loss = getattr(training_args, "disable_global_loss", False)


    data_module = make_supervised_data_module(data_args=data_args,img_preprocess=image_processor,tokenizer=tokenizer,)
    
    model.to(dtype=compute_dtype, device=training_args.device)
    
    # âœ… éªŒè¯Projectionå±‚æ˜¯å¦åœ¨è®­ç»ƒä¸­
    print("\n" + "=" * 80)
    print("ğŸ“Š Projectionå±‚è®­ç»ƒçŠ¶æ€æ£€æŸ¥")
    print("=" * 80)
    for name, param in model.named_parameters():
        if 'projection' in name or 'logit_scale' in name:
            print(f"  {name:50s}: requires_grad={param.requires_grad}, shape={tuple(param.shape)}")
    print("=" * 80 + "\n")

    # æ ¹æ®ç¯å¢ƒå˜é‡å¯ç”¨è¿è¡Œæ—¶è¯Šæ–­ï¼ˆé»˜è®¤å…³é—­ï¼‰
    enable_diag, diag_interval = parse_runtime_diag_env()
    if enable_diag:
        try:
            model.enable_runtime_diagnostics = True
            model.diagnostics_interval = diag_interval
            rank0_print(f"[RUNTIME DIAG] Enabled. Interval={diag_interval}")
        except Exception as e:
            rank0_print(f"[RUNTIME DIAG] Failed to enable diagnostics: {e}")

    # NOTE Set up for two front-passes
    training_args.gradient_checkpointing_kwargs = {"use_reentrant":False}

    trainer = BalancedCLIPTrainer(model=model,
                        args=training_args,
                        **data_module)

    checkpoint_paths = sorted(pathlib.Path(training_args.output_dir).glob("checkpoint-*"))
    if training_args.auto_resume and checkpoint_paths:
        latest_checkpoint = checkpoint_paths[-1]
        print(f"Resuming from checkpoint: {latest_checkpoint}")
        trainer.train(resume_from_checkpoint=str(latest_checkpoint))
    else:
        trainer.train()
    trainer.save_state()
    
    safe_save_model_for_hf_trainer(trainer=trainer,output_dir=training_args.output_dir)




if __name__ == "__main__":
    train()
