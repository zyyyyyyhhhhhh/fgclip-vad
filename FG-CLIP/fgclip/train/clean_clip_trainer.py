import os
import torch
import time
from datetime import datetime

from torch.utils.data import Sampler
from torch.utils.tensorboard import SummaryWriter

from transformers import Trainer
from transformers.trainer import (
    is_sagemaker_mp_enabled,
    get_parameter_names,
    has_length,
    ALL_LAYERNORM_LAYERS,
    logger,
)
from typing import List, Optional
# import torch.distributed.nn as nn_dist
import torch.nn.functional as F

class CLIPTrainer(Trainer):
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # åˆå§‹åŒ–batchçº§åˆ«çš„lossç›‘æ§
        self.batch_losses = []
        self.batch_step = 0
        self.total_steps = 0
        self.epoch = 0
        
        # âœ… æ–°å¢ï¼šç‹¬ç«‹çš„ç´¯ç§¯æ­¥æ•°è®¡æ•°å™¨ï¼ˆæ¯æ¬¡optimizer.step()åé€’å¢ï¼‰
        self.accumulated_step = 0  # ç”¨äºè®°å½•çœŸæ­£çš„è®­ç»ƒæ­¥æ•°
        
        # âœ… åˆå§‹åŒ–TensorBoard
        if self.args.local_rank in [-1, 0]:  # åªåœ¨ä¸»è¿›ç¨‹è®°å½•
            tensorboard_dir = os.path.join(self.args.output_dir, "tensorboard")
            os.makedirs(tensorboard_dir, exist_ok=True)
            self.tb_writer = SummaryWriter(log_dir=tensorboard_dir)
            print(f"[TensorBoard] æ—¥å¿—ç›®å½•: {tensorboard_dir}")
            print(f"[TensorBoard] å¯åŠ¨å‘½ä»¤: tensorboard --logdir {tensorboard_dir} --port 6006")
        else:
            self.tb_writer = None
        
        # åˆ›å»ºè¯¦ç»†çš„lossæ—¥å¿—æ–‡ä»¶
        if self.args.local_rank in [-1, 0]:  # åªåœ¨ä¸»è¿›ç¨‹è®°å½•
            self.loss_log_file = os.path.join(self.args.output_dir, "batch_losses.log")
            with open(self.loss_log_file, "w") as f:
                f.write("=" * 100 + "\n")
                f.write("FG-CLIP Training - Detailed Batch Loss Log\n")
                f.write("=" * 100 + "\n")
                f.write(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Batch Size: {self.args.per_device_train_batch_size}\n")
                f.write(f"Gradient Accumulation Steps: {self.args.gradient_accumulation_steps}\n")
                f.write(f"Effective Batch Size: {self.args.per_device_train_batch_size * self.args.gradient_accumulation_steps}\n")
                f.write("=" * 100 + "\n\n")
                f.write(f"{'Epoch':<6} {'Global_Step':<12} {'Batch':<8} {'Loss':<12} {'Loss_Global':<12} {'Loss_Region':<12} {'Loss_Hard':<12} {'LR':<12} {'Time':<10}\n")
                f.write("-" * 100 + "\n")

    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
        if self.train_dataset is None or not has_length(self.train_dataset):
            return None

        return super()._get_train_sampler()

        
    def create_optimizer(self):
        """
        Setup the optimizer.

        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
        Trainer's init through `optimizers`, or subclass and override this method in a subclass.
        """
        if is_sagemaker_mp_enabled():
            return super().create_optimizer()

        opt_model = self.model

        if self.optimizer is None:
            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)
            decay_parameters = [name for name in decay_parameters if "bias" not in name]

            # print(decay_parameters)

            if self.args.text_model_lr is not None:
                text_model_parameters = [name for name, _ in opt_model.named_parameters() if "text_model" in name]
                optimizer_grouped_parameters = [
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n not in text_model_parameters and p.requires_grad)
                        ],
                        "weight_decay": self.args.weight_decay,
                    },
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n not in text_model_parameters and p.requires_grad)
                        ],
                        "weight_decay": 0.0,
                    },
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and n in text_model_parameters and p.requires_grad)
                        ],
                        "weight_decay": self.args.weight_decay,
                        "lr": self.args.text_model_lr,
                    },
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and n in text_model_parameters and p.requires_grad)
                        ],
                        "weight_decay": 0.0,
                        "lr": self.args.text_model_lr,
                    },
                ]
            else:
                optimizer_grouped_parameters = [
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)
                        ],
                        "weight_decay": self.args.weight_decay,
                    },
                    {
                        "params": [
                            p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)
                        ],
                        "weight_decay": 0.0,
                    },
                ]
            # print(optimizer_grouped_parameters)

            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)
            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)

        return self.optimizer

    def _save_checkpoint(self, model, trial, metrics=None):
        super(CLIPTrainer, self)._save_checkpoint(model, trial)

    def _save(self, output_dir: Optional[str] = None, state_dict=None):
        super(CLIPTrainer, self)._save(output_dir, state_dict)
    
    def training_step(self, model, inputs, num_items_in_batch=None):
        """
        é‡å†™training_stepæ¥ç›‘æ§æ¯ä¸ªbatchçš„è¯¦ç»†loss
        
        Args:
            model: è®­ç»ƒçš„æ¨¡å‹
            inputs: è¾“å…¥æ•°æ®batch
            num_items_in_batch: batchä¸­çš„æ ·æœ¬æ•°(æ–°ç‰ˆtransformerséœ€è¦)
        """
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å‰å‘ä¼ æ’­
        with self.compute_loss_context_manager():
            loss_dict = self.compute_loss(model, inputs, return_outputs=True)
            
            if isinstance(loss_dict, tuple):
                loss = loss_dict[0]
                outputs = loss_dict[1]
            else:
                loss = loss_dict
                outputs = None
        
        # âœ… åœ¨åå‘ä¼ æ’­å‰ç«‹å³æå–lossè¯¦æƒ…(å› ä¸ºåå‘ä¼ æ’­åtensorå¯èƒ½è¢«æ¸…ç©º)
        loss_value = loss.item() if hasattr(loss, 'item') else float(loss)
        loss_global = 0.0
        loss_region = 0.0
        loss_hard = 0.0
        
        # âœ… è°ƒè¯•: æ£€æŸ¥outputså’Œloss_dict
        if outputs is not None:
            has_loss_dict = hasattr(outputs, 'loss_dict')
            if has_loss_dict:
                loss_dict_details = outputs.loss_dict
                loss_global = float(loss_dict_details.get('loss_global', 0.0))
                loss_region = float(loss_dict_details.get('loss_region', 0.0))
                loss_hard = float(loss_dict_details.get('loss_hard_neg', 0.0))
            else:
                # å¦‚æœæ²¡æœ‰loss_dict,æ‰“å°ä¸€æ¬¡è­¦å‘Š
                if self.batch_step == 0:
                    print(f"âš ï¸ Warning: outputs has no 'loss_dict' attribute. Type: {type(outputs)}, Dir: {dir(outputs)}")
        else:
            if self.batch_step == 0:
                print(f"âš ï¸ Warning: outputs is None!")
        
        # æ¢¯åº¦ç¼©æ”¾(å¦‚æœä½¿ç”¨æ··åˆç²¾åº¦)
        if self.args.n_gpu > 1:
            loss = loss.mean()
        
        if self.use_apex:
            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            self.accelerator.backward(loss)
        
        # è®°å½•batchä¿¡æ¯
        batch_time = time.time() - start_time
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # ğŸ“Š Memory BankçŠ¶æ€ç›‘æ§
        mb_status = ""
        if hasattr(model, 'module'):  # DDPæ¨¡å¼
            actual_model = model.module
        else:
            actual_model = model
        
        if hasattr(actual_model, 'use_memory_bank') and actual_model.use_memory_bank:
            queue_ptr = int(actual_model.queue_ptr.item())
            queue_full = actual_model.queue_is_full.item()
            effective_size = actual_model.memory_bank_size if queue_full else queue_ptr
            mb_status = f" | MB: {effective_size}/128 {'âœ“' if queue_full else '...'}"
        
        self.batch_step += 1
        
        # âœ… ä¿®å¤ï¼šåœ¨ç´¯ç§¯æœ€åä¸€æ­¥æ—¶æ›´æ–°accumulated_step
        # åˆ¤æ–­æ˜¯å¦æ˜¯æ¢¯åº¦ç´¯ç§¯çš„æœ€åä¸€æ­¥ï¼ˆå³å°†æ‰§è¡Œoptimizer.step()ï¼‰
        is_last_accumulation_step = (self.batch_step % self.args.gradient_accumulation_steps == 0)
        if is_last_accumulation_step:
            self.accumulated_step += 1
        
        self.total_steps = self.state.global_step  # ä¿ç•™åŸæœ‰é€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰
        
        # æ¯ä¸ªbatchéƒ½æ‰“å°åˆ°ç»ˆç«¯
        if self.args.local_rank in [-1, 0]:
            self.batch_losses.append(loss_value)
            
            # âœ… TensorBoardè®°å½•ï¼ˆä½¿ç”¨accumulated_stepç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
            if self.tb_writer is not None:
                # ä½¿ç”¨accumulated_stepä½œä¸ºxè½´ï¼ˆçœŸå®çš„è®­ç»ƒæ­¥æ•°ï¼‰
                self.tb_writer.add_scalar('Loss/Total', loss_value, self.accumulated_step)
                self.tb_writer.add_scalar('Loss/Global', loss_global, self.accumulated_step)
                self.tb_writer.add_scalar('Loss/Region', loss_region, self.accumulated_step)
                self.tb_writer.add_scalar('Loss/HardNeg', loss_hard, self.accumulated_step)
                
                # è®°å½•å­¦ä¹ ç‡
                self.tb_writer.add_scalar('Training/LearningRate', current_lr, self.accumulated_step)
                
                # è®°å½•è®­ç»ƒé€Ÿåº¦
                self.tb_writer.add_scalar('Training/BatchTime', batch_time, self.accumulated_step)
                
                # âœ… ä¿®å¤ï¼šä»è®­ç»ƒå¼€å§‹å°±è®°å½•Memory BankçŠ¶æ€ï¼ˆè€Œä¸æ˜¯ç­‰use_memory_bank=Trueï¼‰
                # è¿™æ ·å¯ä»¥æ¸…æ™°çœ‹åˆ°MBä»ç¦ç”¨â†’å¯ç”¨â†’å¡«å……çš„å®Œæ•´è¿‡ç¨‹
                if hasattr(actual_model, 'queue_ptr') and hasattr(actual_model, 'queue_is_full'):
                    queue_ptr = int(actual_model.queue_ptr.item())
                    queue_full = actual_model.queue_is_full.item()
                    mb_enabled = getattr(actual_model, 'use_memory_bank', False)
                    
                    # è®¡ç®—æœ‰æ•ˆé˜Ÿåˆ—å¤§å°
                    if mb_enabled:
                        # MBå·²å¯ç”¨ï¼šè®°å½•å®é™…å¡«å……å¤§å°
                        effective_size = actual_model.memory_bank_size if queue_full else queue_ptr
                    else:
                        # MBæœªå¯ç”¨ï¼šè®°å½•ä¸º0ï¼ˆè¡¨ç¤ºæœªæ¿€æ´»çŠ¶æ€ï¼‰
                        effective_size = 0
                    
                    # è®°å½•åˆ°TensorBoardï¼ˆä½¿ç”¨accumulated_stepï¼‰
                    self.tb_writer.add_scalar('MemoryBank/Size', effective_size, self.accumulated_step)
                    self.tb_writer.add_scalar('MemoryBank/Full', int(queue_full), self.accumulated_step)
                    self.tb_writer.add_scalar('MemoryBank/Enabled', int(mb_enabled), self.accumulated_step)  # æ–°å¢ï¼šæ˜¯å¦å¯ç”¨
                    
                    # å¦‚æœMBå·²å¯ç”¨ï¼Œé¢å¤–è®°å½•é˜Ÿåˆ—æŒ‡é’ˆä½ç½®ï¼ˆè°ƒè¯•ç”¨ï¼‰
                    if mb_enabled:
                        self.tb_writer.add_scalar('MemoryBank/QueuePtr', queue_ptr, self.accumulated_step)
                
                # æ¯10ä¸ªbatchè®¡ç®—ç§»åŠ¨å¹³å‡
                if self.batch_step % 10 == 0 and len(self.batch_losses) >= 10:
                    avg_loss = sum(self.batch_losses[-10:]) / 10
                    self.tb_writer.add_scalar('Loss/MovingAvg10', avg_loss, self.accumulated_step)
            
            # ç»ˆç«¯å®æ—¶è¾“å‡º(æ¯ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥) - æ˜¾ç¤ºç´¯ç§¯å¹³å‡loss
            # âœ… ä½¿ç”¨accumulated_stepæ˜¾ç¤ºçœŸå®çš„è®­ç»ƒæ­¥æ•°ï¼ˆä¸TensorBoardä¸€è‡´ï¼‰
            print(f"[ACCUMULATED][Epoch {self.epoch}][Step {self.accumulated_step}][Batch {self.batch_step}] "
                  f"AvgLoss: {loss_value:.6f} | Global: {loss_global:.6f} | Region: {loss_region:.6f} | "
                  f"Hard: {loss_hard:.6f}{mb_status} | LR: {current_lr:.2e} | Time: {batch_time:.2f}s")
            
            # å†™å…¥è¯¦ç»†æ—¥å¿—æ–‡ä»¶
            with open(self.loss_log_file, "a") as f:
                f.write(f"{self.epoch:<6} {self.total_steps:<12} {self.batch_step:<8} "
                       f"{loss_value:<12.6f} {loss_global:<12.6f} {loss_region:<12.6f} "
                       f"{loss_hard:<12.6f} {current_lr:<12.2e} {batch_time:<10.2f}\n")
            
            # æ¯10ä¸ªbatchç»Ÿè®¡ä¸€æ¬¡
            if self.batch_step % 10 == 0:
                avg_loss = sum(self.batch_losses[-10:]) / min(10, len(self.batch_losses))
                print(f"\n{'='*100}")
                print(f"[ç»Ÿè®¡] æœ€è¿‘10ä¸ªbatchå¹³å‡Loss: {avg_loss:.6f}")
                print(f"{'='*100}\n")
        
        return loss.detach() / self.args.gradient_accumulation_steps
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """
        é‡å†™compute_lossæ¥è¿”å›è¯¦ç»†çš„lossä¿¡æ¯
        """
        outputs = model(**inputs)
        
        if hasattr(outputs, "loss"):
            loss = outputs.loss
        else:
            # å¤„ç†å¯èƒ½çš„å…¶ä»–è¾“å‡ºæ ¼å¼
            if isinstance(outputs, dict) and "loss" in outputs:
                loss = outputs["loss"]
            else:
                raise ValueError("Model output doesn't contain 'loss' field")
        
        return (loss, outputs) if return_outputs else loss
    
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒ - å°†TensorBoard writeræ³¨å…¥åˆ°æ¨¡å‹ä¸­"""
        if self.tb_writer is not None and args.local_rank in [-1, 0]:
            # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå¤„ç†DDPåŒ…è£…ï¼‰
            if hasattr(self.model, 'module'):
                actual_model = self.model.module
            else:
                actual_model = self.model
            
            # å°†TensorBoard writerä¼ é€’ç»™æ¨¡å‹ï¼ˆç”¨äºmicro-batchçº§åˆ«çš„è®°å½•ï¼‰
            actual_model.tb_writer = self.tb_writer
            print(f"[TensorBoard] âœ… TensorBoard writerå·²æ³¨å…¥åˆ°æ¨¡å‹ï¼Œå¯ç”¨micro-batchçº§åˆ«è®°å½•")
            print(f"[TensorBoard] ğŸ“Š ä½ å°†çœ‹åˆ°ä¸¤å¥—æ›²çº¿ï¼š")
            print(f"             - MicroBatch/* : æ¯ä¸ªforwardçš„åŸå§‹lossï¼ˆé«˜é¢‘æ³¢åŠ¨ï¼‰")
            print(f"             - Loss/*        : æ¢¯åº¦ç´¯ç§¯åçš„å¹³å‡lossï¼ˆå¹³æ»‘æ›²çº¿ï¼‰")
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """Epochå¼€å§‹æ—¶çš„å›è°ƒ"""
        self.epoch = state.epoch if state.epoch is not None else 0
        self.batch_step = 0
        
        if args.local_rank in [-1, 0]:
            print(f"\n{'='*100}")
            print(f"ğŸš€ å¼€å§‹ Epoch {self.epoch + 1}")
            print(f"{'='*100}\n")
            
            with open(self.loss_log_file, "a") as f:
                f.write(f"\n{'='*100}\n")
                f.write(f"Epoch {self.epoch + 1} Started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"{'='*100}\n\n")
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶çš„å›è°ƒ - å…³é—­TensorBoard"""
        if self.tb_writer is not None:
            print(f"\n[TensorBoard] å…³é—­ TensorBoard writer...")
            self.tb_writer.close()
            print(f"[TensorBoard] âœ“ å·²å…³é—­")
        
        if args.local_rank in [-1, 0]:
            print(f"\n{'='*100}")
            print(f"âœ… è®­ç»ƒå®Œæˆï¼")
            print(f"{'='*100}\n")
            
            with open(self.loss_log_file, "a") as f:
                f.write(f"\n{'='*100}\n")
                f.write(f"Training Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"{'='*100}\n")
    
    def __del__(self):
        """ææ„å‡½æ•° - ç¡®ä¿TensorBoard writerè¢«å…³é—­"""
        if hasattr(self, 'tb_writer') and self.tb_writer is not None:
            try:
                self.tb_writer.close()
            except:
                pass
