import torch
import torch.nn as nn
import math

from transformers import CLIPConfig,AutoConfig
from typing import Any, Optional, Tuple, Union
import torch.distributed.nn as nn_dist
import torch.nn.functional as F
import numpy as np
from collections import OrderedDict
from typing import Tuple, Union
from dataclasses import dataclass
from .modeling_clip import CLIPModel, CLIPTextTransformer, CLIPVisionTransformer, CLIPOutput, CLIPAttention, CLIPMLP

import torch.distributed as dist
from torch.nn import AvgPool2d
from transformers import (
    AutoImageProcessor,
    AutoModel,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    TrainingArguments,
)

from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
from torch import nn

import math
from torchvision.ops import roi_align

# âœ… è‡ªå®šä¹‰è¾“å‡ºç±»,ç»§æ‰¿CLIPOutputå¹¶æ·»åŠ loss_dictå­—æ®µ
@dataclass
class FGCLIPOutput(CLIPOutput):
    """
    æ‰©å±•çš„CLIPè¾“å‡º,åŒ…å«è¯¦ç»†çš„lossä¿¡æ¯
    """
    loss_dict: Optional[dict] = None

class FGCLIPModel(CLIPModel):
    config_class = CLIPConfig
    main_input_name = "text_long"

    def __init__(self, config):
        super(CLIPModel, self).__init__(config)

        if not isinstance(config.text_config, CLIPTextConfig):
            raise ValueError(
                "config.text_config is expected to be of type CLIPTextConfig but is of type"
                f" {type(config.text_config)}."
            )

        if not isinstance(config.vision_config, CLIPVisionConfig):
            raise ValueError(
                "config.vision_config is expected to be of type CLIPVisionConfig but is of type"
                f" {type(config.vision_config)}."
            )

        text_config = config.text_config
        vision_config = config.vision_config
        text_config.eos_token_id = 49407
        text_config.pad_token_id = 49407
        text_config.bos_token_id = 49406

        self.projection_dim = config.projection_dim
        self.text_embed_dim = text_config.hidden_size
        self.vision_embed_dim = vision_config.hidden_size

        self.text_model = CLIPTextTransformer(text_config)

        self.vision_model = CLIPVisionTransformer(vision_config)
        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)


        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)
        self.text_filip_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)


        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))
        self.logit_scale_finegraind = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))
        self.logit_scale_hardneg = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))

 
        self.embed_dim = text_config.hidden_size
        self.world_size = 0

        # ========== æ–°å¢: æ—¶åºå»ºæ¨¡æ¨¡å— ==========
        # è½»é‡çº§ Transformer ç”¨äºæ•æ‰å¸§é—´æ—¶åºå…³ç³»
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.projection_dim,
            nhead=8,
            dim_feedforward=self.projection_dim * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.temporal_transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        
        # æ³¨æ„åŠ›åŠ æƒæ± åŒ–: å­¦ä¹ å“ªäº›å¸§æ›´é‡è¦
        self.temporal_attention = nn.Sequential(
            nn.Linear(self.projection_dim, self.projection_dim // 2),
            nn.ReLU(),
            nn.Linear(self.projection_dim // 2, 1)
        )
        
        # âœ… ROIç‰¹å¾æŠ•å½±å±‚ - å¤„ç†ç»´åº¦åŒ¹é…
        # ä½¿ç”¨ViT-B/32: vision_config.hidden_size=768
        # hidden_states[-2]çš„ç‰¹å¾ç»´åº¦ä¹Ÿæ˜¯768ï¼ˆä¸é…ç½®ä¸€è‡´ï¼‰
        # 
        # âœ… å…³é”®ä¿®å¤ï¼šä½¿ç”¨visual_projectionçš„æƒé‡åˆå§‹åŒ–roi_projection
        # ç†è®ºä¾æ®ï¼šROIç‰¹å¾å’Œå…¨å±€ç‰¹å¾éƒ½æ¥è‡ªåŒä¸€ä¸ªViTï¼Œåº”è¯¥ä½¿ç”¨ç›¸åŒçš„æŠ•å½±ç©ºé—´
        # è¿™æ ·Regionç‰¹å¾å’ŒTextç‰¹å¾æ‰èƒ½åœ¨é¢„è®­ç»ƒå¯¹é½çš„ç©ºé—´ä¸­å¯¹æ¯”å­¦ä¹ 
        self.roi_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)
        
        # âš ï¸ æ³¨æ„ï¼šæ­¤æ—¶visual_projectionè¿˜æœªåˆå§‹åŒ–ï¼Œå®é™…å¤åˆ¶åœ¨load_openai_clip_weights()ä¹‹å
        # åœ¨train_fgclip.pyä¸­è°ƒç”¨copy_weight()æ¥å®Œæˆæƒé‡å¤åˆ¶
        print(f"[INFO] Initialized roi_projection: {self.vision_embed_dim} -> {self.projection_dim} (ViT-B/32)")

        # è¿è¡Œæ—¶è¯Šæ–­å¼€å…³ï¼ˆé»˜è®¤å…³é—­ï¼‰ã€‚å¼€å¯åä¼šåœ¨æœ‰regionçš„batchæ‰“å°embeddingã€logitsã€MBçŠ¶æ€ç­‰ç»Ÿè®¡ä¿¡æ¯
        # ç”¨æ³•ï¼šåœ¨è®­ç»ƒè„šæœ¬ä¸­è®¾ç½® model.enable_runtime_diagnostics = True
        self.enable_runtime_diagnostics = False
        # æ‰“å°é¢‘ç‡ï¼ˆå•ä½ï¼šæœ‰regionçš„forwardè°ƒç”¨æ¬¡æ•°ï¼‰ï¼Œé»˜è®¤1è¡¨ç¤ºæ¯æ¬¡æ‰“å°
        self.diagnostics_interval = 1

        # ========== Memory Bank for Region Contrastive Learning ==========
        # Queue-based memory bank (MoCo v2 style)
        # ç”¨äºå¢å¼ºRegionå¯¹æ¯”å­¦ä¹ çš„è´Ÿæ ·æœ¬æ•°é‡
        # âœ… è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨detach()åˆ‡æ–­æ¢¯åº¦ä¾èµ– + clone()åˆ›å»ºå®‰å…¨å‰¯æœ¬
        # ç†è®ºä¾æ®ï¼šMoCo v2è®ºæ–‡Section 3.2 "Dictionary as a queue"
        # 
        # âœ… ä¿®å¤ï¼šè®­ç»ƒåˆæœŸæš‚æ—¶ç¦ç”¨Memory Bankï¼Œç­‰ROIç‰¹å¾ç©ºé—´ç¨³å®šåå†å¯ç”¨
        # åŸå› ï¼šéšæœºåˆå§‹åŒ–çš„roi_projectionéœ€è¦å…ˆå­¦ä¹ åŸºç¡€å¯¹é½ï¼Œå¦åˆ™é˜Ÿåˆ—ä¸­çš„ç‰¹å¾éƒ½æ˜¯å™ªå£°
        # âœ… ç­–ç•¥ï¼šå‰50æ­¥ç¦ç”¨MBï¼Œç­‰Region lossåˆæ­¥æ”¶æ•›åè‡ªåŠ¨å¯ç”¨
        self.use_memory_bank = False  # åˆå§‹ç¦ç”¨ï¼Œç­‰step 50åè‡ªåŠ¨å¯ç”¨
        self.memory_bank_size = 128  # æ¯ä¸ªæ¨¡æ€å­˜å‚¨128ä¸ªå†å²æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬æ•°é‡ä»Nå¢åŠ åˆ°N+128ï¼‰
        self.memory_bank_warmup_steps = 50  # âœ… ä¿®å¤: 50æ¬¡forwardè°ƒç”¨ â‰ˆ 6ä¸ªTrainer step (gradient_accumulation=8)
        
        # æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼Œä½†ä¼šè¢«ä¿å­˜åˆ°checkpointï¼‰
        self.register_buffer("region_image_queue", torch.randn(self.projection_dim, self.memory_bank_size))
        self.register_buffer("region_text_queue", torch.randn(self.projection_dim, self.memory_bank_size))
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))
        self.register_buffer("training_steps", torch.zeros(1, dtype=torch.long))  # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        
        # åˆå§‹åŒ–é˜Ÿåˆ—ï¼šå½’ä¸€åŒ–åˆ°å•ä½å‘é‡
        self.region_image_queue = F.normalize(self.region_image_queue, dim=0)
        self.region_text_queue = F.normalize(self.region_text_queue, dim=0)
        
        # æ ‡å¿—ä½ï¼šè®­ç»ƒåˆæœŸé˜Ÿåˆ—æœªå¡«æ»¡æ—¶ä½¿ç”¨
        self.register_buffer("queue_is_full", torch.zeros(1, dtype=torch.bool))

        # Initialize weights and apply final processing
        self.post_init()


    def resize_postion_embeding(self, newsize=248):

        old_position_embedding = self.text_model.embeddings.position_embedding
        old_position_embedding_res = self.text_model.embeddings.position_embedding_res
        old_position_embedding_ori = self.text_model.embeddings.position_embedding_ori
        
        positional_embedding_pre = self.text_model.embeddings.position_embedding.weight.data
    
        length, dim = positional_embedding_pre.shape
        keep_len = 20
        posisitonal_embedding_new = torch.zeros([4*length-3*keep_len, dim], dtype=positional_embedding_pre.dtype)
        for i in range(keep_len):
            posisitonal_embedding_new[i] = positional_embedding_pre[i]
        for i in range(length-1-keep_len):
            posisitonal_embedding_new[4*i + keep_len] = positional_embedding_pre[i + keep_len]
            posisitonal_embedding_new[4*i + 1 + keep_len] = 3*positional_embedding_pre[i + keep_len]/4 + 1*positional_embedding_pre[i+1+keep_len]/4
            posisitonal_embedding_new[4*i + 2+keep_len] = 2*positional_embedding_pre[i+keep_len]/4 + 2*positional_embedding_pre[i+1+keep_len]/4
            posisitonal_embedding_new[4*i + 3+keep_len] = 1*positional_embedding_pre[i+keep_len]/4 + 3*positional_embedding_pre[i+1+keep_len]/4

        posisitonal_embedding_new[4*length -3*keep_len - 4] = positional_embedding_pre[length-1] + 0*(positional_embedding_pre[length-1] - positional_embedding_pre[length-2])/4
        posisitonal_embedding_new[4*length -3*keep_len - 3] = positional_embedding_pre[length-1] + 1*(positional_embedding_pre[length-1] - positional_embedding_pre[length-2])/4
        posisitonal_embedding_new[4*length -3*keep_len - 2] = positional_embedding_pre[length-1] + 2*(positional_embedding_pre[length-1] - positional_embedding_pre[length-2])/4
        posisitonal_embedding_new[4*length -3*keep_len - 1] = positional_embedding_pre[length-1] + 3*(positional_embedding_pre[length-1] - positional_embedding_pre[length-2])/4
                
        positional_embedding_res = posisitonal_embedding_new.clone()

        self.text_model.embeddings.position_embedding_ori.weight.data = posisitonal_embedding_new
        self.text_model.embeddings.position_embedding_ori.num_embeddings = posisitonal_embedding_new.shape[0]
        
        self.text_model.embeddings.position_embedding_res.weight.data = positional_embedding_res
        self.text_model.embeddings.position_embedding_res.num_embeddings = positional_embedding_res.shape[0]

        old_position_embedding_ori_requires_grad = old_position_embedding_ori.weight.requires_grad
        self.text_model.embeddings.position_embedding_ori.requires_grad_(old_position_embedding_ori_requires_grad)

        old_position_embedding_res_requires_grad = old_position_embedding_res.weight.requires_grad
        self.text_model.embeddings.position_embedding_res.requires_grad_(old_position_embedding_res_requires_grad)



    def copy_weight(self,):
        """
        å¤åˆ¶æŠ•å½±å±‚æƒé‡ï¼Œç¡®ä¿ä¸åŒåˆ†æ”¯ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾ç©ºé—´
        
        âœ… å…³é”®ä¿®å¤ï¼šå°†visual_projectionçš„æƒé‡å¤åˆ¶åˆ°roi_projection
        ç†è®ºä¾æ®ï¼š
        1. ROIç‰¹å¾å’Œå…¨å±€ç‰¹å¾éƒ½æ¥è‡ªåŒä¸€ä¸ªViTç¼–ç å™¨
        2. åº”è¯¥æŠ•å½±åˆ°ç›¸åŒçš„CLIPç‰¹å¾ç©ºé—´æ‰èƒ½ä¸æ–‡æœ¬å¯¹æ¯”
        3. ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–å¯ä»¥é¿å…Regionåˆ†æ”¯ä»é›¶å¼€å§‹å­¦ä¹ 
        """
        with torch.no_grad():
            # åŸæœ‰é€»è¾‘ï¼štext_filip_projectionå¤åˆ¶text_projectionæƒé‡
            self.text_filip_projection.weight.data.copy_(self.text_projection.weight.data)
            
            # âœ… æ–°å¢ï¼šroi_projectionå¤åˆ¶visual_projectionæƒé‡
            # ç¡®ä¿ROIç‰¹å¾æŠ•å½±åˆ°ä¸å…¨å±€ç‰¹å¾ç›¸åŒçš„CLIPç©ºé—´
            self.roi_projection.weight.data.copy_(self.visual_projection.weight.data)
            print("[FG-CLIP] âœ… Copied visual_projection weights to roi_projection")
    
    def load_openai_clip_weights(self, model_name="ViT-B/32"):
        """
        ä»æœ¬åœ°OpenAI CLIPæƒé‡æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæƒé‡
        
        å­¦ä¹ è‡ª Vadclip-iccv/src/model52.py çš„å®ç°ï¼š
        ç›´æ¥ä½¿ç”¨ clip.load() åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¤åˆ¶æƒé‡åˆ°FG-CLIP
        
        è¿™ä¸ªæ–¹æ³•æ¯”æ‰‹åŠ¨æ˜ å°„æƒé‡ç®€å•å¾—å¤šï¼Œå› ä¸ºOpenAI CLIPä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰æ¶æ„ç»†èŠ‚
        
        Args:
            model_name: CLIPæ¨¡å‹åç§°,ä¾‹å¦‚ "ViT-B/32"
        
        Returns:
            loaded_keys: æˆåŠŸåŠ è½½çš„æƒé‡é”®åˆ—è¡¨
            missing_keys: ç¼ºå¤±çš„æƒé‡é”®åˆ—è¡¨
        """
        import os
        from fgclip.model import clip
        
        print(f"[FG-CLIP] ğŸ”„ Loading OpenAI CLIP weights: {model_name}")
        print(f"[FG-CLIP] Cache: ~/.cache/clip/")
        
        try:
            # âœ… ç›´æ¥ä½¿ç”¨ clip.load() åŠ è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹
            # è¿™ä¼šè‡ªåŠ¨ä»æœ¬åœ°ç¼“å­˜åŠ è½½ ViT-B-32.pt
            clipmodel, _ = clip.load(model_name, device='cpu', jit=False, 
                                    download_root=os.path.expanduser("~/.cache/clip"))
            
            # è·å–OpenAI CLIPçš„state_dict
            openai_state_dict = clipmodel.state_dict()
            
            # è·å–å½“å‰FG-CLIPæ¨¡å‹çš„state_dict
            fgclip_state_dict = self.state_dict()
            
            loaded_keys = []
            missing_keys = []
            
            # âœ… å¤åˆ¶Vision Transformeræƒé‡ï¼ˆæ™ºèƒ½æ˜ å°„ï¼‰
            print("[FG-CLIP] Copying Vision Transformer weights...")
            vision_count = 0
            for key in openai_state_dict.keys():
                if not key.startswith('visual.'):
                    continue
                
                # æ˜ å°„è§„åˆ™:
                # visual.class_embedding â†’ vision_model.embeddings.class_embedding
                # visual.positional_embedding â†’ vision_model.embeddings.position_embedding.weight  
                # visual.conv1.weight â†’ vision_model.embeddings.patch_embedding.weight
                # visual.ln_pre â†’ vision_model.pre_layrnorm
                # visual.transformer.resblocks.X â†’ vision_model.encoder.layers.X
                # visual.ln_post â†’ vision_model.post_layernorm
                
                if 'class_embedding' in key:
                    fgclip_key = 'vision_model.embeddings.class_embedding'
                elif 'positional_embedding' in key and 'visual.' in key:
                    fgclip_key = 'vision_model.embeddings.position_embedding.weight'
                elif 'conv1' in key:
                    fgclip_key = key.replace('visual.conv1', 'vision_model.embeddings.patch_embedding')
                elif 'ln_pre' in key:
                    fgclip_key = key.replace('visual.ln_pre', 'vision_model.pre_layrnorm')
                elif 'ln_post' in key:
                    fgclip_key = key.replace('visual.ln_post', 'vision_model.post_layernorm')
                elif 'resblocks' in key:
                    # visual.transformer.resblocks.X â†’ vision_model.encoder.layers.X
                    fgclip_key = key.replace('visual.transformer.resblocks', 'vision_model.encoder.layers')
                    # in_proj_weight/bias â†’ q/k/v_proj (OpenAIåˆå¹¶çš„æƒé‡ï¼Œéœ€è¦åˆ†è§£)
                    if 'in_proj_weight' in key or 'in_proj_bias' in key:
                        # è·³è¿‡ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                        continue
                    fgclip_key = fgclip_key.replace('.attn.', '.self_attn.')
                    fgclip_key = fgclip_key.replace('.ln_1', '.layer_norm1')
                    fgclip_key = fgclip_key.replace('.ln_2', '.layer_norm2')
                    fgclip_key = fgclip_key.replace('.c_fc', '.fc1')
                    fgclip_key = fgclip_key.replace('.c_proj', '.fc2')
                else:
                    continue
                
                if fgclip_key in fgclip_state_dict:
                    if openai_state_dict[key].shape == fgclip_state_dict[fgclip_key].shape:
                        fgclip_state_dict[fgclip_key].copy_(openai_state_dict[key])
                        loaded_keys.append(fgclip_key)
                        vision_count += 1
            
            print(f"[FG-CLIP]   Copied {vision_count} vision weights")
            
            # âœ… å¤åˆ¶Text Transformeræƒé‡ï¼ˆæ™ºèƒ½æ˜ å°„ï¼‰
            print("[FG-CLIP] Copying Text Transformer weights...")
            text_count = 0
            for key in openai_state_dict.keys():
                if not (key.startswith('transformer.') or key == 'positional_embedding' or key == 'token_embedding.weight'):
                    continue
                
                # æ˜ å°„è§„åˆ™:
                # positional_embedding â†’ text_model.embeddings.position_embedding.weight
                # token_embedding.weight â†’ text_model.embeddings.token_embedding.weight
                # transformer.resblocks.X â†’ text_model.encoder.layers.X
                
                if key == 'positional_embedding':
                    fgclip_key = 'text_model.embeddings.position_embedding.weight'
                elif key == 'token_embedding.weight':
                    fgclip_key = 'text_model.embeddings.token_embedding.weight'
                elif 'resblocks' in key:
                    fgclip_key = key.replace('transformer.resblocks', 'text_model.encoder.layers')
                    if 'in_proj_weight' in key or 'in_proj_bias' in key:
                        # è·³è¿‡åˆå¹¶çš„QKVæƒé‡
                        continue
                    fgclip_key = fgclip_key.replace('.attn.', '.self_attn.')
                    fgclip_key = fgclip_key.replace('.ln_1', '.layer_norm1')
                    fgclip_key = fgclip_key.replace('.ln_2', '.layer_norm2')
                    fgclip_key = fgclip_key.replace('.c_fc', '.fc1')
                    fgclip_key = fgclip_key.replace('.c_proj', '.fc2')
                elif 'ln_final' in key:
                    fgclip_key = key.replace('transformer.ln_final', 'text_model.final_layer_norm')
                else:
                    continue
                
                if fgclip_key in fgclip_state_dict:
                    if openai_state_dict[key].shape == fgclip_state_dict[fgclip_key].shape:
                        fgclip_state_dict[fgclip_key].copy_(openai_state_dict[key])
                        loaded_keys.append(fgclip_key)
                        text_count += 1
            
            print(f"[FG-CLIP]   Copied {text_count} text weights")
            
            # âœ… å¤åˆ¶æŠ•å½±å±‚æƒé‡
            projection_mappings = {
                'visual.proj': 'visual_projection.weight',
                'text_projection': 'text_projection.weight',
            }
            
            print("[FG-CLIP] Copying projection layers...")
            for openai_key, fgclip_key in projection_mappings.items():
                if openai_key in openai_state_dict and fgclip_key in fgclip_state_dict:
                    # OpenAIçš„projæ˜¯(D_out, D_in), HuggingFaceæ˜¯(D_in, D_out)ï¼Œéœ€è¦è½¬ç½®
                    if 'visual.proj' in openai_key:
                        fgclip_state_dict[fgclip_key].copy_(openai_state_dict[openai_key].T)
                    else:
                        fgclip_state_dict[fgclip_key].copy_(openai_state_dict[openai_key])
                    loaded_keys.append(fgclip_key)
            
            # âœ… å…³é”®ä¿®å¤ï¼šå¤åˆ¶logit_scaleå‚æ•°ï¼ˆè®­ç»ƒæ¸©åº¦æ§åˆ¶ï¼‰
            # OpenAI CLIPçš„logit_scaleåˆå§‹å€¼ä¸ºln(100)=4.6052
            # è¿™ä¸ªå‚æ•°æ§åˆ¶å¯¹æ¯”å­¦ä¹ çš„æ¸©åº¦ï¼Œå¯¹æ”¶æ•›è‡³å…³é‡è¦
            if 'logit_scale' in openai_state_dict:
                self.logit_scale.data.copy_(openai_state_dict['logit_scale'])
                loaded_keys.append('logit_scale')
                print(f"[FG-CLIP]   âœ… Loaded logit_scale = {self.logit_scale.data.item():.4f} (exp={self.logit_scale.exp().item():.1f})")
            
            # åŠ è½½å›æ¨¡å‹
            self.load_state_dict(fgclip_state_dict, strict=False)
            
            print(f"[FG-CLIP] âœ… Successfully loaded {len(loaded_keys)} weight tensors")
            print(f"[FG-CLIP] âš ï¸  Missing {len(missing_keys)} weight tensors (expected for new modules)")
            
            return loaded_keys, missing_keys
            
        except Exception as e:
            print(f"[FG-CLIP] âŒ Failed to load OpenAI CLIP weights: {e}")
            print(f"[FG-CLIP] Continuing with random initialization...")
            return [], []
  

    def get_image_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> torch.FloatTensor:

        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = vision_outputs[1]  # pooled_output
        image_features = self.visual_projection(pooled_output)

        return image_features
    
    def get_image_box_roi_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        box_info=None,
    ) -> torch.FloatTensor:


        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=True,
            return_dict=return_dict
        )

        bs = pixel_values.shape[0]
        length = vision_outputs[0].shape[1]-1
        h = int(math.sqrt(length))
        w = h

        feature_map = vision_outputs.hidden_states[-2]#[:, 1:, :]
        feature_map = self.forward_without_attn(feature_map)[:, 1:]

        feature_map = self.vision_model.post_layernorm(feature_map)
        feature_map = self.visual_projection(feature_map)

        feature_map = feature_map.view(bs, h, w, -1).permute(0, 3, 1, 2)
        x_rois = roi_align(feature_map.type(torch.float32),box_info, (1, 1), 1.0, -1, True)[..., 0, 0]

        # x_rois = x_rois / x_rois.norm(p=2, dim=-1, keepdim=True)

        return x_rois

    def get_text_features(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        walk_short_pos: Optional[bool] = True,
        use_bbox: Optional[bool] = False
    ) -> torch.FloatTensor:

        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        pos_flag = walk_short_pos or use_bbox

        text_outputs = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            walk_short_pos=pos_flag,
        )
        pooled_output = text_outputs[1]

        if walk_short_pos:
            text_features = self.text_projection(pooled_output)
        else:
            text_features = self.text_filip_projection(pooled_output)           

        return text_features

    @staticmethod
    def _denormalize_boxes(normed_boxes, x):
        h, w = x.shape[-2:]
        denormed_boxes = []
        # print("normed_boxes, ", normed_boxes.shape)
        for boxes in normed_boxes:
            # print("boxes, ", boxes)
            new_boxes = boxes.clone()   # FIXME: do not change the value in normed_boxes!
            new_boxes[:, [0, 2]] *= w
            new_boxes[:, [1, 3]] *= h
            denormed_boxes.append(new_boxes.type(torch.float32))
        return denormed_boxes

    @torch.no_grad()
    def _update_memory_bank(self, image_feats, text_feats):
        """
        æ›´æ–°Memory Bank (FIFOé˜Ÿåˆ—)
        
        Args:
            image_feats: (N, D) å½“å‰batchçš„region imageç‰¹å¾
            text_feats: (N, D) å½“å‰batchçš„region textç‰¹å¾
        
        æ³¨æ„:
            - ä½¿ç”¨@torch.no_grad()ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦
            - FIFOç­–ç•¥ï¼šæ–°æ ·æœ¬æ›¿æ¢æœ€è€çš„æ ·æœ¬
            - é˜Ÿåˆ—æœªæ»¡æ—¶ï¼Œqueue_is_full=False
        """
        batch_size = image_feats.shape[0]
        
        # å½“å‰é˜Ÿåˆ—æŒ‡é’ˆä½ç½®
        ptr = int(self.queue_ptr)
        
        # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºé˜Ÿåˆ—å®¹é‡
        if ptr + batch_size <= self.memory_bank_size:
            # æƒ…å†µ1: é˜Ÿåˆ—æœ‰è¶³å¤Ÿç©ºé—´ï¼Œç›´æ¥å†™å…¥
            self.region_image_queue[:, ptr:ptr + batch_size] = image_feats.T
            self.region_text_queue[:, ptr:ptr + batch_size] = text_feats.T
        else:
            # æƒ…å†µ2: é˜Ÿåˆ—ç©ºé—´ä¸è¶³ï¼Œéœ€è¦å¾ªç¯å†™å…¥ï¼ˆwrap aroundï¼‰
            # å…ˆå¡«å……é˜Ÿåˆ—æœ«å°¾å‰©ä½™ç©ºé—´
            remain_space = self.memory_bank_size - ptr
            self.region_image_queue[:, ptr:] = image_feats[:remain_space].T
            self.region_text_queue[:, ptr:] = text_feats[:remain_space].T
            
            # å†ä»é˜Ÿåˆ—å¼€å¤´å†™å…¥å‰©ä½™æ ·æœ¬
            overflow_size = batch_size - remain_space
            self.region_image_queue[:, :overflow_size] = image_feats[remain_space:].T
            self.region_text_queue[:, :overflow_size] = text_feats[remain_space:].T
        
        # æ›´æ–°é˜Ÿåˆ—æŒ‡é’ˆï¼ˆå¾ªç¯ï¼‰
        old_ptr = ptr  # ä¿å­˜æ—§æŒ‡é’ˆç”¨äºæ—¥å¿—
        ptr = (ptr + batch_size) % self.memory_bank_size
        self.queue_ptr[0] = ptr
        
        # æ ‡è®°é˜Ÿåˆ—å·²æ»¡ï¼ˆè‡³å°‘å®Œæ•´å¾ªç¯ä¸€æ¬¡ï¼‰
        was_full = self.queue_is_full.item()  # ä¿å­˜ä¹‹å‰çš„çŠ¶æ€
        if not self.queue_is_full and ptr < batch_size:
            self.queue_is_full[0] = True
            print(f"[MEMORY BANK] ğŸ‰ é˜Ÿåˆ—é¦–æ¬¡å¡«æ»¡ï¼Ptr: {old_ptr}â†’{ptr}, è´Ÿæ ·æœ¬æ•°é‡: {batch_size}â†’128")
        
        # å‘¨æœŸæ€§æ—¥å¿—ï¼šæ˜¾ç¤ºé˜Ÿåˆ—å¡«å……è¿›åº¦
        if not was_full and old_ptr % 32 == 0:  # è®­ç»ƒåˆæœŸæ¯32ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡
            fill_ratio = (old_ptr / self.memory_bank_size) * 100
            print(f"[MEMORY BANK] ğŸ“Š ç§¯ç´¯ä¸­... Ptr: {old_ptr}/{self.memory_bank_size} ({fill_ratio:.1f}%), å½“å‰è´Ÿæ ·æœ¬: {old_ptr}")

    def forward_without_attn(self, x):
        # get last layer 
        residual = x
        x = self.vision_model.encoder.layers[-1].layer_norm1(x)

        x = F.linear(input=x, weight=self.vision_model.encoder.layers[-1].self_attn.v_proj.weight, bias=self.vision_model.encoder.layers[-1].self_attn.v_proj.bias)
        x = self.vision_model.encoder.layers[-1].self_attn.out_proj(x)
        x = residual+x

        residual = x
        x = self.vision_model.encoder.layers[-1].layer_norm2(x)
        x = self.vision_model.encoder.layers[-1].mlp(x)
        x = residual + x

        return x

    def get_image_dense_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        box_info=None,
    ) -> torch.FloatTensor:

        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        vision_outputs = self.vision_model(
            pixel_values=pixel_values,
            output_attentions=output_attentions,
            output_hidden_states=True,
            return_dict=return_dict,
        )


        bs = pixel_values.shape[0]
        length = vision_outputs[0].shape[1]-1
        h = int(math.sqrt(length))
        w = h

        feature_map = vision_outputs.hidden_states[-2]#[:, 1:, :]
        feature_map = self.forward_without_attn(feature_map)[:, 1:]

        feature_map = self.vision_model.post_layernorm(feature_map)
        feature_map = self.visual_projection(feature_map)

        return feature_map

    def forward(
        self,
        text_short: Optional[torch.LongTensor] = None,
        text_long: Optional[torch.LongTensor] = None,
        image: Optional[torch.FloatTensor] = None,
        box_infos: Optional[torch.FloatTensor] = None,
        bbox_mask: Optional[torch.BoolTensor] = None,  # âœ… æ–°å¢ï¼šbboxæœ‰æ•ˆæ€§mask (B, T, max_anns)
        box_texts: Optional[torch.LongTensor] = None,
        box_nums: Optional[torch.LongTensor] = None,
        hard_infos: Optional[torch.FloatTensor] = None,
        hard_texts: Optional[torch.LongTensor] = None,
        hard_nums: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,  # æ–‡æœ¬çš„attention_mask
        video_attention_mask: Optional[torch.Tensor] = None,  # æ–°å¢ï¼šè§†é¢‘çš„attention_mask
        region_videos: Optional[torch.FloatTensor] = None,  # âœ… æ–°å¢ï¼šregionç‹¬ç«‹é‡‡æ ·çš„è§†é¢‘ (B, max_anns, T, C, H, W)
        position_ids: Optional[torch.LongTensor] = None,
        return_loss: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        add_box_loss: bool = False,
        use_hard_neg: bool = False,
    ) -> Union[Tuple, CLIPOutput]:

        # ========== é˜²æ­¢å±æ€§ä¸¢å¤±ï¼Œè‡ªåŠ¨åˆå§‹åŒ–ç´¯ç§¯lossç›¸å…³å˜é‡ ==========
        if not hasattr(self, 'accum_buffer'):
            self.accum_buffer = {'total': 0.0, 'global': 0.0, 'region': 0.0, 'hard_neg': 0.0, 'count': 0}
        if not hasattr(self, 'accum_step_counter'):
            self.accum_step_counter = 0
        if not hasattr(self, 'gradient_accumulation_steps'):
            self.gradient_accumulation_steps = 8  # å¯æ ¹æ®å®é™…ä¼ å…¥configè°ƒæ•´

        # Use CLIP model's config for some fields (if specified) instead of those of vision & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # ========== Memory Bankè‡ªåŠ¨å¯ç”¨é€»è¾‘ ==========
        # è·å–rankï¼Œå¦‚æœæ²¡æœ‰åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒåˆ™ä½¿ç”¨0
        try:
            rank = dist.get_rank()
        except (RuntimeError, ValueError):
            rank = 0
            
        # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œçš„training_stepsæ˜¯forwardè°ƒç”¨æ¬¡æ•°ï¼Œä¸æ˜¯Trainerçš„global_step
        # Trainerçš„stepè€ƒè™‘äº†gradient_accumulation_stepsï¼ˆæ¯accumulation_stepsä¸ªforwardæ‰æ›´æ–°ä¸€æ¬¡ï¼‰
        # æ‰€ä»¥å¦‚æœgradient_accumulation=8ï¼Œé‚£ä¹ˆTrainerçš„step 50 = è¿™é‡Œçš„training_steps 400
        # 
        # ä¸ºäº†å¯¹é½ç”¨æˆ·ç†è§£ï¼Œæˆ‘ä»¬æ”¹ä¸ºï¼šåœ¨Trainerçš„step 50æ—¶å¯ç”¨ï¼ˆå³training_steps=50*8=400ï¼‰
        if self.training and add_box_loss:
            self.training_steps += 1
            
            # âœ… è‡ªåŠ¨å¯ç”¨Memory Bankï¼ˆ200æ­¥å - ç¡®ä¿region_videosç‰¹å¾ç¨³å®šï¼‰
            # ç†ç”±ï¼šè®­ç»ƒåˆæœŸregionç‰¹å¾å¿«é€Ÿæ¼”åŒ–ï¼Œè¿‡æ—©å¯ç”¨ä¼šæ±¡æŸ“é˜Ÿåˆ—
            if not self.use_memory_bank and self.training_steps >= 200:  # 50 â†’ 200
                self.use_memory_bank = True
                if rank == 0:
                    print(f"\n{'='*80}")
                    print(f"[Memory Bank] âœ… å·²å¯ç”¨ @ training_step {self.training_steps.item()}")
                    print(f"[Memory Bank] é˜Ÿåˆ—å¤§å°: {self.memory_bank_size}, å½“å‰æŒ‡é’ˆ: {self.queue_ptr.item()}")
                    print(f"[Memory Bank] âš ï¸  è®­ç»ƒå‰200æ­¥çš„æ—§ç‰¹å¾å·²è¢«ä¸¢å¼ƒ")
                    print(f"{'='*80}\n")
         
        # ========== ä¿®æ”¹: æ”¯æŒè§†é¢‘è¾“å…¥ (B, T, C, H, W) ==========
        is_video = (image.dim() == 5)  # åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘è¾“å…¥
        
        if is_video:
            bs, num_frames, c, h, w = image.shape
            # å°†è§†é¢‘å±•å¹³ä¸º (B*T, C, H, W) è¿›è¡Œé€å¸§ç¼–ç 
            image_flat = image.view(bs * num_frames, c, h, w)
            
            vision_outputs = self.vision_model(
                    pixel_values=image_flat,
                    output_attentions=output_attentions,
                    output_hidden_states=True,
                    return_dict=return_dict,
            )
            
            # æå–å…¨å±€ç‰¹å¾ (B*T, D)
            image_embeds_flat = vision_outputs[1]
            image_embeds_flat = self.visual_projection(image_embeds_flat)
            
            # é‡å¡‘ä¸º (B, T, D)
            image_embeds_temporal = image_embeds_flat.view(bs, num_frames, -1)
            
            # æ—¶åºå»ºæ¨¡: Transformeræ•æ‰å¸§é—´å…³ç³»
            # å¦‚æœæœ‰video_attention_mask,ä½¿ç”¨å®ƒæ¥å¤„ç†å¡«å……å¸§
            if video_attention_mask is not None and video_attention_mask.dim() == 2:
                # video_attention_mask: (B, T), Trueè¡¨ç¤ºæœ‰æ•ˆå¸§
                # Transformeréœ€è¦çš„mask: (B, T), Falseè¡¨ç¤ºæœ‰æ•ˆä½ç½®
                temporal_mask = ~video_attention_mask  # åè½¬
                image_embeds_temporal = self.temporal_transformer(
                    image_embeds_temporal, 
                    src_key_padding_mask=temporal_mask
                )
            else:
                image_embeds_temporal = self.temporal_transformer(image_embeds_temporal)
            
            # æ³¨æ„åŠ›åŠ æƒèšåˆ
            attn_weights = self.temporal_attention(image_embeds_temporal)  # (B, T, 1)
            if video_attention_mask is not None and video_attention_mask.dim() == 2:
                # å°†å¡«å……å¸§çš„æƒé‡è®¾ä¸ºæå°å€¼
                attn_weights = attn_weights.masked_fill(~video_attention_mask.unsqueeze(-1), -1e9)
            attn_weights = torch.softmax(attn_weights, dim=1)  # (B, T, 1)
            
            # åŠ æƒæ±‚å’Œå¾—åˆ°è§†é¢‘çº§ç‰¹å¾
            image_embeds = (image_embeds_temporal * attn_weights).sum(dim=1)  # (B, D)
            # âœ… é›¶èŒƒæ•°ä¿æŠ¤
            image_norm = image_embeds.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-8)
            image_embeds = image_embeds / image_norm
        else:
            # åŸå§‹å›¾åƒå¤„ç†é€»è¾‘
            vision_outputs = self.vision_model(
                    pixel_values=image,
                    output_attentions=output_attentions,
                    output_hidden_states=True,
                    return_dict=return_dict,
            )
            image_embeds = vision_outputs[1]
            image_embeds = self.visual_projection(image_embeds)
            # âœ… é›¶èŒƒæ•°ä¿æŠ¤
            image_norm = image_embeds.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-8)
            image_embeds = image_embeds / image_norm


        long_text_outputs = self.text_model(
                input_ids=text_long,
                attention_mask=attention_mask,
                position_ids=position_ids,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
            )

        # âœ… æ³¨é‡Šï¼šshort_textä¸å†ä½¿ç”¨ï¼Œä¿ç•™ç¼–ç åªæ˜¯ä¸ºäº†å‘åå…¼å®¹
        # åœ¨VADä»»åŠ¡ä¸­ï¼Œshort_text(region_caption)åº”è¯¥ä¸regionè§†è§‰ç‰¹å¾å¯¹æ¯”
        # è€Œä¸æ˜¯ä¸å…¨å±€è§†è§‰ç‰¹å¾å¯¹æ¯”
        short_text_outputs = self.text_model(
                input_ids=text_short,
                attention_mask=attention_mask,
                position_ids=position_ids,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                walk_short_pos=True,
            )

        long_text_embeds = long_text_outputs[1]
        long_text_embeds = self.text_filip_projection(long_text_embeds)
        # âœ… é›¶èŒƒæ•°ä¿æŠ¤
        text_norm = long_text_embeds.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-8)
        long_text_embeds = long_text_embeds / text_norm

        # âœ… ä¿®æ”¹ï¼šç¦ç”¨short_textå¯¹æ¯”å­¦ä¹ ï¼Œåªä¿ç•™å…¨å±€å¯¹æ¯”
        # short_textåœ¨VADä»»åŠ¡ä¸­è¯­ä¹‰ä¸åŒ¹é…ï¼ˆå…¨å±€è§†è§‰ vs regionæ–‡æœ¬ï¼‰
        # short_text_embeds = short_text_outputs[1]
        # short_text_embeds = self.text_projection(short_text_embeds)
        # short_text_embeds = short_text_embeds / short_text_embeds.norm(p=2, dim=-1, keepdim=True)

        # âœ… åªä½¿ç”¨å…¨å±€å¯¹æ¯”å­¦ä¹  (image_embeds â†” long_text_embeds)
        loss_itcl = self.clip_loss_global_only(image_embeds, long_text_embeds, rank, image)

        if getattr(self, "disable_global_loss", False):
            loss_itcl = loss_itcl * 0.0

        # åˆå§‹åŒ–lossï¼ˆåªåŒ…å«å…¨å±€å¯¹æ¯”ï¼‰
        loss = loss_itcl
        
        # ç”¨äºæ‰“å°çš„è¯¦ç»†losså­—å…¸
        loss_dict = {
            'loss_global': loss_itcl.item() if hasattr(loss_itcl, 'item') else float(loss_itcl),
            'loss_region': 0.0,
            'loss_hard_neg': 0.0,
        }

        if add_box_loss or use_hard_neg:
            # ========== ä¿®æ”¹: æ”¯æŒè§†é¢‘çš„ feature map æå– ==========
            if is_video:
                # å¯¹äºè§†é¢‘è¾“å…¥,feature_map å·²ç»æ˜¯ (B*T, ...) å½¢å¼
                feature_map = vision_outputs.hidden_states[-2]
                feature_map = self.forward_without_attn(feature_map)[:, 1:]
                feature_map = self.vision_model.post_layernorm(feature_map)
                # âŒ ç§»é™¤ï¼šä¸è¦å¯¹hidden_statesåº”ç”¨visual_projection
                # å› ä¸ºvisual_projectionæœŸæœ›768ç»´ï¼Œä½†hidden_statesæ˜¯1024ç»´
                # ROIç‰¹å¾çš„æŠ•å½±ç”±roi_projectionå•ç‹¬å¤„ç†
                # feature_map = self.visual_projection(feature_map)
                
                # è®¡ç®—ç©ºé—´ç»´åº¦
                length = vision_outputs[0].shape[1]-1
                h = int(math.sqrt(length))
                w = h
                # feature_map: (B*T, h*w, D) -> (B*T, D, h, w)
                feature_map = feature_map.view(bs * num_frames, h, w, -1).permute(0, 3, 1, 2)
            else:
                # åŸå§‹å›¾åƒå¤„ç†
                feature_map = vision_outputs.hidden_states[-2]
                feature_map = self.forward_without_attn(feature_map)[:, 1:]
                feature_map = self.vision_model.post_layernorm(feature_map)
                # âŒ ç§»é™¤ï¼šåŒæ ·ä¸è¦å¯¹hidden_statesåº”ç”¨visual_projection
                # feature_map = self.visual_projection(feature_map)
                bs = image.shape[0]
                length = vision_outputs[0].shape[1]-1
                h = int(math.sqrt(length))
                w = h
                feature_map = feature_map.view(bs, h, w, -1).permute(0, 3, 1, 2)

        if add_box_loss:
            # ========== ä¿®æ”¹: ä½¿ç”¨ region_videos ç›´æ¥ç¼–ç  Region ç‰¹å¾ ==========
            if region_videos is not None:
                # region_videos: (B, max_anns, T, C, H, W)
                bs_reg, num_anns, num_frames_reg, c, h_img, w_img = region_videos.shape
                
                # å±•å¹³ä¸º (B*max_anns*T, C, H, W)
                region_videos_flat = region_videos.view(bs_reg * num_anns * num_frames_reg, c, h_img, w_img)
                
                # é€šè¿‡ Vision Encoder ç¼–ç 
                region_vision_outputs = self.vision_model(
                    pixel_values=region_videos_flat,
                    output_attentions=output_attentions,
                    output_hidden_states=True,
                    return_dict=return_dict,
                )
                
                # æå–ç‰¹å¾ (B*max_anns*T, D)
                region_embeds_flat = region_vision_outputs[1]
                region_embeds_flat = self.visual_projection(region_embeds_flat)
                
                # é‡å¡‘ä¸º (B, max_anns, T, D)
                region_embeds_temporal = region_embeds_flat.view(bs_reg, num_anns, num_frames_reg, -1)
                
                # å¯¹æ¯ä¸ª region å•ç‹¬è¿›è¡Œæ—¶åºå»ºæ¨¡
                # éœ€è¦å¤„ç†ç»´åº¦: (B*max_anns, T, D)
                region_embeds_for_transformer = region_embeds_temporal.view(bs_reg * num_anns, num_frames_reg, -1)
                
                # æ—¶åº Transformer
                region_embeds_temporal_transformed = self.temporal_transformer(region_embeds_for_transformer)
                
                # æ³¨æ„åŠ›åŠ æƒèšåˆ
                region_attn_weights = self.temporal_attention(region_embeds_temporal_transformed)  # (B*max_anns, T, 1)
                region_attn_weights = torch.softmax(region_attn_weights, dim=1)
                
                # åŠ æƒæ±‚å’Œ (B*max_anns, D)
                bbox_image_embeds = (region_embeds_temporal_transformed * region_attn_weights).sum(dim=1)
                
                # å½’ä¸€åŒ–
                bbox_image_embeds = F.normalize(bbox_image_embeds, p=2, dim=-1, eps=1e-8)
            
            # ========== Region æ–‡æœ¬ç¼–ç ï¼ˆå¿…é¡»åœ¨è§†è§‰ç¼–ç åç«‹å³æ‰§è¡Œï¼‰==========
            # æ— è®ºä½¿ç”¨ region_videos è¿˜æ˜¯ ROI Alignï¼Œéƒ½éœ€è¦ç¼–ç  box_texts
            bbox_text_outputs = self.text_model(
                input_ids=box_texts,
                attention_mask=attention_mask,
                position_ids=position_ids,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                walk_short_pos=True,
            )
            bbox_text_embeds = bbox_text_outputs[1]
            bbox_text_embeds = self.text_projection(bbox_text_embeds)
            # âœ… é›¶èŒƒæ•°ä¿æŠ¤
            text_norm = bbox_text_embeds.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-8)
            bbox_text_embeds = bbox_text_embeds / text_norm
                
            if region_videos is None:
                # ========== å›é€€: ä½¿ç”¨æ—§é€»è¾‘ï¼ˆä» feature_map æå– ROIï¼‰==========
                box_size = box_infos.shape[-1]  # åº”è¯¥æ˜¯ 4 (x1, y1, x2, y2)
                
                if is_video:
                    # box_infos: (B, T, max_anns, 4)
                    if box_infos.dim() == 4:
                        num_anns = box_infos.shape[2]
                        # å±•å¹³ä¸º (B*T, max_anns, 4)
                        box_infos_expanded = box_infos.view(bs * num_frames, num_anns, box_size)
                        
                        # åå½’ä¸€åŒ– bbox
                        original_bboxes = self._denormalize_boxes(box_infos_expanded, feature_map)
                        
                        # RoI Align
                        x_rois = roi_align(feature_map.type(torch.float32), original_bboxes, (1, 1), 1.0, -1, True)[..., 0, 0]
                        
                        # é‡å¡‘ä¸º (B, T, max_anns, D)
                        x_rois = x_rois.view(bs, num_frames, num_anns, -1)
                        
                        # æ—¶åºèšåˆ
                        if bbox_mask is not None:
                            mask_expanded = bbox_mask.unsqueeze(-1).float()
                            x_rois_masked = x_rois * mask_expanded
                            valid_frames = bbox_mask.sum(dim=1).unsqueeze(-1).float()
                            bbox_vision_outputs = x_rois_masked.sum(dim=1) / valid_frames.clamp(min=1)
                        else:
                            if video_attention_mask is not None and video_attention_mask.dim() == 2:
                                mask_expanded = video_attention_mask.unsqueeze(-1).unsqueeze(-1)
                                x_rois_masked = x_rois * mask_expanded
                                valid_frames = video_attention_mask.sum(dim=1, keepdim=True).unsqueeze(-1).unsqueeze(-1)
                                bbox_vision_outputs = x_rois_masked.sum(dim=1) / valid_frames.clamp(min=1)
                            else:
                                bbox_vision_outputs = x_rois.mean(dim=1)
                        
                        # å±•å¹³ä¸º (B*max_anns, D)
                        bbox_vision_outputs = bbox_vision_outputs.view(bs * num_anns, -1).type(torch.bfloat16)
                    elif box_infos.dim() == 3:
                        raise ValueError("3D box_infos is deprecated. Please use 4D box_infos (B, T, max_anns, 4)")
                    else:
                        raise ValueError(f"Unexpected box_infos shape for video: {box_infos.shape}")
                else:
                    # å›¾åƒå¤„ç†é€»è¾‘
                    if box_infos.dim() == 4:
                        box_infos = box_infos.squeeze(1)
                    elif box_infos.dim() == 2:
                        box_infos = box_infos.reshape(bs, -1, box_size)
                    else:
                        raise ValueError(f"Unexpected box_infos shape for image: {box_infos.shape}")
                    
                    original_bboxes = self._denormalize_boxes(box_infos, feature_map)
                    x_rois = roi_align(feature_map.type(torch.float32), original_bboxes, (1, 1), 1.0, -1, True)[..., 0, 0]
                    bbox_vision_outputs = x_rois.type(torch.bfloat16)
                
                # æŠ•å½±åˆ°æœ€ç»ˆç»´åº¦
                bbox_image_embeds = self.roi_projection(bbox_vision_outputs)
                bbox_image_embeds = F.normalize(bbox_image_embeds, p=2, dim=-1, eps=1e-8)
            
        if use_hard_neg:
            box_size = hard_infos.shape[-1]
            hard_infos = hard_infos.reshape(bs, -1, box_size)
           
            original_bboxes = self._denormalize_boxes(hard_infos, feature_map)
            x_rois = roi_align(feature_map.type(torch.float32), original_bboxes, (1, 1), 1.0, -1, True)[..., 0, 0]

            hard_bbox_image_embeds = x_rois.type(torch.bfloat16)
            
            # âœ… å…³é”®ä¿®å¤ï¼šHard Negativeçš„ROIç‰¹å¾ä¹Ÿéœ€è¦ç»è¿‡projectionï¼ˆä¸Regionåˆ†æ”¯ä¿æŒä¸€è‡´ï¼‰
            # ç¡®ä¿ç»´åº¦ä¸bbox_image_embedsä¸€è‡´ï¼ˆ512ç»´ï¼‰
            if self.roi_projection is not None and not isinstance(self.roi_projection, nn.Identity):
                hard_bbox_image_embeds = self.roi_projection(hard_bbox_image_embeds)
            
            # âœ… é›¶èŒƒæ•°ä¿æŠ¤ - ä½¿ç”¨F.normalizeé¿å…inplaceæ“ä½œ
            hard_bbox_image_embeds = F.normalize(hard_bbox_image_embeds, p=2, dim=-1, eps=1e-8)

            hard_bbox_text_outputs = self.text_model(
                    input_ids=hard_texts,
                    attention_mask=attention_mask,
                    position_ids=position_ids,
                    output_attentions=None,
                    output_hidden_states=None,
                    return_dict=return_dict,
                    walk_short_pos=True,
                )
            hard_bbox_text_embeds = hard_bbox_text_outputs[1]
            hard_bbox_text_embeds = self.text_projection(hard_bbox_text_embeds)
            # âœ… é›¶èŒƒæ•°ä¿æŠ¤
            hard_text_norm = hard_bbox_text_embeds.norm(p=2, dim=-1, keepdim=True).clamp(min=1e-8)
            hard_bbox_text_embeds = hard_bbox_text_embeds / hard_text_norm

        

        if add_box_loss:

            # âœ… ä¿®å¤ï¼šæå‡Region lossæƒé‡ï¼Œå¢å¼ºå­¦ä¹ ä¿¡å·
            # åŸå€¼0.1å¯¼è‡´Regionæ¢¯åº¦è¿‡å¼±ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ 
            # ç†è®ºä¾æ®ï¼šRegionå¯¹æ¯”å­¦ä¹ å¯¹VADä»»åŠ¡æ›´å…³é”®ï¼Œåº”è¯¥ä¸GlobalåŒç­‰é‡è¦
            box_loss_weight = 0.5  # ä»0.1æå‡åˆ°0.5
            bs = box_nums.shape[0]
            bbox_size = int(bbox_text_embeds.shape[0]/bs)
            # åˆå§‹åŸºäº box_nums æ ‡è®°æœ‰æ•ˆregionï¼ˆæŒ‰region_dataæ•°é‡ï¼‰
            box_weight = torch.zeros([bs, bbox_size], device=bbox_text_embeds.device)
            for i in range(bs):
                valid_count = int(box_nums[i])
                box_weight[i][:valid_count] = 1

            # é¢å¤–å®‰å…¨è¿‡æ»¤ï¼šå¦‚æœ bbox_mask å­˜åœ¨ï¼Œå‰”é™¤é‚£äº›åœ¨æ‰€æœ‰å¸§ä¸Šéƒ½æ— æœ‰æ•ˆbboxçš„region
            # è¿™ç§æƒ…å†µä¼šåœ¨ temporal aggregation æ—¶äº§ç”Ÿå…¨é›¶çš„è§†è§‰ç‰¹å¾ã€‚
            # å¦‚æœä¸å‰”é™¤ï¼Œå®ƒä»¬ä¼šä»¥é›¶å‘é‡è¢«é€‰ä¸­å¹¶è¿›å…¥ memory bankï¼Œæ±¡æŸ“è´Ÿæ ·æœ¬é›†åˆã€‚
            if bbox_mask is not None:
                try:
                    # bbox_mask: (B, T, max_anns) -> (B, max_anns) indicatingæ¯regionæ˜¯å¦åœ¨ä»»æ„å¸§ä¸Šæœ‰æ•ˆ
                    valid_region_mask = (bbox_mask.sum(dim=1) > 0)  # (B, max_anns), bool
                    # å¯¹é½dtypeå¹¶é€å…ƒç´ ç›¸ä¹˜
                    box_weight = box_weight * valid_region_mask.to(box_weight.dtype)
                except Exception:
                    # è‹¥ä»»ä½•ç»´åº¦ä¸åŒ¹é…ï¼Œä¿ç•™åŸæœ‰è¡Œä¸ºï¼Œé¿å…å¼•å…¥è¿è¡Œæ—¶é”™è¯¯
                    pass

            # flattenå¹¶é€‰å–éé›¶ç´¢å¼•
            box_weight = box_weight.reshape(1, bbox_text_embeds.shape[0]).squeeze()
            select_index = box_weight.nonzero()
            
            # âœ… ä¿®å¤ï¼šå®‰å…¨çš„squeezeï¼Œé¿å…å•æ ·æœ¬ç»´åº¦å¡Œé™·
            if select_index.numel() > 0:
                valid_count = select_index.shape[0]
                bbox_text_embeds = bbox_text_embeds[select_index, :].view(valid_count, -1)
                bbox_image_embeds = bbox_image_embeds[select_index, :].view(valid_count, -1)
                
                # ========== Regionå¯¹æ¯”å­¦ä¹  with Memory Bank ==========
                # ç›®æ ‡ï¼šå°†è´Ÿæ ·æœ¬æ•°é‡ä»batchå†…çš„Nä¸ªæ‰©å±•åˆ°N+128ä¸ªï¼ˆMemory Bankï¼‰
                # æ¶æ„ï¼šMoCo v2é£æ ¼çš„Queue-based Memory Bank
                # è§£å†³æ–¹æ¡ˆï¼šDetach + Cloneæœºåˆ¶ç¡®ä¿æ¢¯åº¦å®‰å…¨
                
                logit_scale = self.logit_scale_finegraind.exp()
                
                if self.use_memory_bank:
                    # âœ… åŠ¨æ€Queueç­–ç•¥ï¼šè®­ç»ƒåˆæœŸåªç”¨batchå†…å¯¹æ¯”ï¼Œé˜Ÿåˆ—å¡«æ»¡åå†ä½¿ç”¨Memory Bank
                    # ç†è®ºä¾æ®ï¼šMoCo v2è®ºæ–‡Algorithm 1ç¬¬7-8è¡Œ
                    
                    # Step 1: è®¡ç®—å½“å‰æœ‰æ•ˆé˜Ÿåˆ—å¤§å°
                    if self.queue_is_full:
                        # é˜Ÿåˆ—å·²æ»¡ï¼šä½¿ç”¨å…¨éƒ¨128ä¸ªæ ·æœ¬
                        effective_queue_size = self.memory_bank_size
                    else:
                        # é˜Ÿåˆ—æœªæ»¡ï¼šåªä½¿ç”¨å·²å¡«å……çš„éƒ¨åˆ†ï¼ˆqueue_ptræŒ‡å‘ä¸‹ä¸€ä¸ªç©ºä½ï¼‰
                        effective_queue_size = int(self.queue_ptr)
                    
                    # Step 2: æ ¹æ®é˜Ÿåˆ—çŠ¶æ€é€‰æ‹©å¯¹æ¯”ç­–ç•¥
                    if effective_queue_size > 0:
                        # æœ‰å†å²æ ·æœ¬ï¼šä½¿ç”¨Memory Bankå¢å¼ºå¯¹æ¯”å­¦ä¹ 
                        queue_image = self.region_image_queue[:, :effective_queue_size].detach().clone()  # (D, K)
                        queue_text = self.region_text_queue[:, :effective_queue_size].detach().clone()    # (D, K)
                        
                        # âœ… è´¨é‡ç›‘æ§ï¼šæ£€æµ‹é˜Ÿåˆ—æ ·æœ¬çš„èŒƒæ•°åˆ†å¸ƒ
                        with torch.no_grad():
                            queue_img_norms = queue_image.norm(p=2, dim=0)  # (K,)
                            queue_txt_norms = queue_text.norm(p=2, dim=0)   # (K,)
                            curr_img_norms = bbox_image_embeds.norm(p=2, dim=1)  # (N,)
                            
                            # æ£€æµ‹å¼‚å¸¸ï¼šé˜Ÿåˆ—æ ·æœ¬çš„èŒƒæ•°è¿œå°äºå½“å‰batch
                            queue_norm_mean = queue_img_norms.mean().item()
                            curr_norm_mean = curr_img_norms.mean().item()
                            norm_ratio = curr_norm_mean / (queue_norm_mean + 1e-8)
                            
                            # å¦‚æœé˜Ÿåˆ—æ ·æœ¬è´¨é‡æ˜æ˜¾ä½äºå½“å‰batchï¼Œæ‰“å°è­¦å‘Š
                            if norm_ratio > 1.5 and self.training_steps % 50 == 0:
                                print(f"âš ï¸  [MB Quality] Queueæ ·æœ¬èŒƒæ•°åä½ï¼šQueue={queue_norm_mean:.4f} vs Curr={curr_norm_mean:.4f} (ratio={norm_ratio:.2f})")
                        
                        # è®­ç»ƒç›‘æ§ï¼šæ˜¾ç¤ºMemory BankçŠ¶æ€ï¼ˆè®­ç»ƒåˆæœŸï¼‰
                        if not self.queue_is_full and effective_queue_size % 32 == 0:
                            print(f"[MB-FORWARD] ä½¿ç”¨{effective_queue_size}ä¸ªå†å²è´Ÿæ ·æœ¬ | Batchå†…: {valid_count} | æ€»è´Ÿæ ·æœ¬: {valid_count + effective_queue_size}")
                        
                        # æ‹¼æ¥batch + queueä½œä¸ºè´Ÿæ ·æœ¬
                        text_with_queue = torch.cat([bbox_text_embeds.T, queue_text], dim=1)  # (D, N+K)
                        logits_i2t = torch.matmul(bbox_image_embeds, text_with_queue) * logit_scale  # (N, N+K)
                        
                        image_with_queue = torch.cat([bbox_image_embeds.T, queue_image], dim=1)  # (D, N+K)
                        logits_t2i = torch.matmul(bbox_text_embeds, image_with_queue) * logit_scale  # (N, N+K)
                        
                        # æ ‡ç­¾ï¼šæ­£æ ·æœ¬åœ¨å‰Nä¸ªä½ç½®çš„å¯¹è§’çº¿
                        labels = torch.arange(valid_count, device=bbox_image_embeds.device, dtype=torch.long)
                    else:
                        # è®­ç»ƒæœ€å¼€å§‹ï¼šé˜Ÿåˆ—ä¸ºç©ºï¼Œåªç”¨batchå†…å¯¹æ¯”ï¼ˆé¿å…éšæœºå™ªå£°å¹²æ‰°ï¼‰
                        logits_i2t = torch.matmul(bbox_image_embeds, bbox_text_embeds.T) * logit_scale
                        logits_t2i = torch.matmul(bbox_text_embeds, bbox_image_embeds.T) * logit_scale
                        labels = torch.arange(valid_count, device=bbox_image_embeds.device, dtype=torch.long)
                    
                    # Step 4: InfoNCE Loss
                    loss_i2t = F.cross_entropy(logits_i2t, labels)
                    loss_t2i = F.cross_entropy(logits_t2i, labels)
                    loss_bbox_itcl = (loss_i2t + loss_t2i) / 2.0
                    
                    # Step 5: å®‰å…¨æ›´æ–°Memory Bankï¼ˆä½¿ç”¨detachedç‰¹å¾ï¼‰
                    with torch.no_grad():
                        # clone()ç¡®ä¿ç‰¹å¾å‰¯æœ¬ä¸ä¼šåå‘ä¼ æ’­
                        self._update_memory_bank(
                            bbox_image_embeds.detach().clone(), 
                            bbox_text_embeds.detach().clone()
                        )
                else:
                    # Fallback: åªä½¿ç”¨batchå†…å¯¹æ¯”ï¼ˆåŸå§‹å®ç°ï¼‰
                    logits_i2t = torch.matmul(bbox_image_embeds, bbox_text_embeds.T) * logit_scale
                    logits_t2i = torch.matmul(bbox_text_embeds, bbox_image_embeds.T) * logit_scale
                    
                    labels = torch.arange(valid_count, device=bbox_image_embeds.device, dtype=torch.long)
                    loss_i2t = F.cross_entropy(logits_i2t, labels)
                    loss_t2i = F.cross_entropy(logits_t2i, labels)
                    loss_bbox_itcl = (loss_i2t + loss_t2i) / 2.0
                
                # ç´¯åŠ åˆ°æ€»æŸå¤±
                # âœ… æå‡Region lossæƒé‡ï¼šä»0.2â†’0.5ï¼ˆä¸GlobalåŒç­‰é‡è¦ï¼‰
                # ç†ç”±ï¼šRegionå¯¹VADä»»åŠ¡æ›´å…³é”®ï¼Œéœ€è¦æ›´å¼ºçš„å­¦ä¹ ä¿¡å·
                box_loss_weight = 0.5
                loss = loss + box_loss_weight * loss_bbox_itcl
                loss_dict['loss_region'] = loss_bbox_itcl.item() if hasattr(loss_bbox_itcl, 'item') else float(loss_bbox_itcl)
                # ========== è¿è¡Œæ—¶è¯Šæ–­è¾“å‡º ==========
                try:
                    if getattr(self, 'enable_runtime_diagnostics', False):
                        # è®¡æ•°å™¨ï¼ˆåªåœ¨æœ‰regionçš„forwardä¸­é€’å¢ï¼‰
                        if not hasattr(self, '_diag_counter'):
                            self._diag_counter = 0
                        self._diag_counter += 1
                        if (self._diag_counter % max(1, getattr(self, 'diagnostics_interval', 1))) == 0:
                            # åŸºæœ¬ç»Ÿè®¡
                            img_norms = bbox_image_embeds.norm(p=2, dim=-1)
                            txt_norms = bbox_text_embeds.norm(p=2, dim=-1)
                            logits = None
                            try:
                                logits = logits_i2t.detach() if 'logits_i2t' in locals() else None
                            except Exception:
                                logits = None

                            def stats(tensor):
                                if tensor is None:
                                    return {'min': None, 'max': None, 'mean': None, 'std': None, 'has_nan': None}
                                t = tensor.detach()
                                return {
                                    'min': float(t.min().item()),
                                    'max': float(t.max().item()),
                                    'mean': float(t.mean().item()),
                                    'std': float(t.std().item()),
                                    'has_nan': bool(torch.isnan(t).any().item()),
                                }

                            img_stats = stats(img_norms)
                            txt_stats = stats(txt_norms)
                            logits_stats = stats(logits)

                            mb_ptr = int(self.queue_ptr.item()) if hasattr(self, 'queue_ptr') else None
                            mb_full = bool(self.queue_is_full.item()) if hasattr(self, 'queue_is_full') else None
                            mb_size = int(self.memory_bank_size) if hasattr(self, 'memory_bank_size') else None

                            print("[DIAG] Region batch diagnostics:")
                            print(f"  - valid_count={valid_count} | bbox_batch_size={bbox_text_embeds.shape[0]}")
                            print(f"  - img_norms: min={img_stats['min']:.4f}, max={img_stats['max']:.4f}, mean={img_stats['mean']:.4f}, std={img_stats['std']:.4f}, has_nan={img_stats['has_nan']}")
                            print(f"  - txt_norms: min={txt_stats['min']:.4f}, max={txt_stats['max']:.4f}, mean={txt_stats['mean']:.4f}, std={txt_stats['std']:.4f}, has_nan={txt_stats['has_nan']}")
                            if logits_stats['min'] is not None:
                                print(f"  - logits_i2t: min={logits_stats['min']:.4f}, max={logits_stats['max']:.4f}, mean={logits_stats['mean']:.4f}, std={logits_stats['std']:.4f}, has_nan={logits_stats['has_nan']}")
                            print(f"  - logit_scale (finegrained): {self.logit_scale_finegraind.item():.6f} (exp={self.logit_scale_finegraind.exp().item():.4f})")
                            print(f"  - memory_bank: ptr={mb_ptr}, full={mb_full}, size={mb_size}")
                            # æ£€æŸ¥NaN/Inf
                            any_nan = torch.isnan(bbox_image_embeds).any().item() or torch.isnan(bbox_text_embeds).any().item()
                            any_inf = torch.isinf(bbox_image_embeds).any().item() or torch.isinf(bbox_text_embeds).any().item()
                            print(f"  - any_nan={any_nan}, any_inf={any_inf}")
                            # é¢å¤–ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„bboxæ–‡æœ¬å’Œä¸€äº›logitsä½ç½®
                            try:
                                print(f"  - sample_logits_row0_first5: {logits[0,:5].cpu().numpy().tolist() if logits is not None else 'N/A'}")
                            except Exception:
                                pass
                except Exception as e:
                    print(f"[DIAG] Failed to emit diagnostics: {e}")
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆregionï¼Œè·³è¿‡region loss
                pass

        if use_hard_neg:
            hard_box_loss_weight = 0.5

            bs = hard_nums.shape[0]
            bbox_size = int(hard_bbox_image_embeds.shape[0]/bs)
            box_weight = torch.zeros([bs, bbox_size], device=hard_bbox_image_embeds.device)
            for i in range(bs):
                valid_count = int(hard_nums[i])
                box_weight[i][:valid_count] = 1
            box_weight = box_weight.reshape(1, hard_bbox_image_embeds.shape[0]).squeeze()
            select_index = box_weight.nonzero()
            hard_bbox_image_embeds = hard_bbox_image_embeds[select_index,:].squeeze()
            loss_bbox_hitc= self.hard_contrastive_loss(hard_bbox_image_embeds, hard_bbox_text_embeds, hard_bbox_text_embeds.device, self.logit_scale_hardneg)
            loss = loss + hard_box_loss_weight*loss_bbox_hitc
            loss_dict['loss_hard_neg'] = loss_bbox_hitc.item() if hasattr(loss_bbox_hitc, 'item') else float(loss_bbox_hitc)
        
        # ========== åªè®°å½•ç´¯ç§¯æ­¥ï¼ˆaccumulatedï¼‰lossåˆ°TensorBoard ==========
        if rank == 0:
            # ç´¯ç§¯lossç»Ÿè®¡
            self.accum_buffer['total'] += loss.item()
            self.accum_buffer['global'] += loss_dict['loss_global']
            self.accum_buffer['region'] += loss_dict['loss_region']
            self.accum_buffer['hard_neg'] += loss_dict['loss_hard_neg']
            self.accum_buffer['count'] += 1
            if self.accum_buffer['count'] == self.gradient_accumulation_steps:
                avg_total = self.accum_buffer['total'] / self.accum_buffer['count']
                avg_global = self.accum_buffer['global'] / self.accum_buffer['count']
                avg_region = self.accum_buffer['region'] / self.accum_buffer['count']
                avg_hard_neg = self.accum_buffer['hard_neg'] / self.accum_buffer['count']
                self.accum_step_counter += 1
                print(f"[ACCUMULATED] Total: {avg_total:.4f} | Global: {avg_global:.4f} | Region: {avg_region:.4f} | HardNeg: {avg_hard_neg:.4f}", flush=True)
                if hasattr(self, 'tb_writer') and self.tb_writer is not None:
                    self.tb_writer.add_scalar('Loss/Total', avg_total, self.accum_step_counter)
                    self.tb_writer.add_scalar('Loss/Global', avg_global, self.accum_step_counter)
                    self.tb_writer.add_scalar('Loss/Region', avg_region, self.accum_step_counter)
                    self.tb_writer.add_scalar('Loss/HardNeg', avg_hard_neg, self.accum_step_counter)
                self.accum_buffer = {'total': 0.0, 'global': 0.0, 'region': 0.0, 'hard_neg': 0.0, 'count': 0}
        
        # âœ… ä½¿ç”¨FGCLIPOutputè¿”å›ç»“æœ,åŒ…å«loss_dict
        return FGCLIPOutput(
            loss=loss,
            loss_dict=loss_dict
        )

    

    def clip_loss(self,image_features_long, text_features_long, text_features_short,rank,image):


        image_feat_all_long = torch.cat(nn_dist.all_gather(image_features_long), dim=0)#gather with grad

        if text_features_long is not None:
            text_feat_all_long = torch.cat(nn_dist.all_gather(text_features_long), dim=0)

        text_feat_all_short = torch.cat(nn_dist.all_gather(text_features_short), dim=0)
        
        if text_features_long is not None:
            sim_i2tl = torch.matmul(image_features_long, text_feat_all_long.T)
            sim_tl2i = torch.matmul(image_feat_all_long, text_features_long.T)
            sim_tl2i = sim_tl2i.T

        sim_i2ts = torch.matmul(image_features_long, text_feat_all_short.T)
        sim_ts2i = torch.matmul(image_feat_all_long, text_features_short.T)
        sim_ts2i = sim_ts2i.T

        
        if text_features_long is not None:
            sim_i2tl = self.logit_scale.exp() * sim_i2tl
            sim_tl2i = self.logit_scale.exp() * sim_tl2i


        sim_i2ts = self.logit_scale.exp() * sim_i2ts
        sim_ts2i = self.logit_scale.exp() * sim_ts2i

        
        bs = image_features_long.size(0)
        targets = torch.linspace(rank * bs,rank * bs + bs - 1, bs, dtype=torch.long).to(image.device)

        loss_itcl = None
        if text_features_long is not None:
            loss_itcl = (
                    F.cross_entropy(sim_i2tl, targets, label_smoothing=0.0)
                    + F.cross_entropy(sim_tl2i, targets, label_smoothing=0.0)
                ) / 2
        
        loss_itcs = (
                F.cross_entropy(sim_i2ts, targets, label_smoothing=0.0)
                + F.cross_entropy(sim_ts2i, targets, label_smoothing=0.0)
            ) / 2

        return loss_itcl, loss_itcs

    
    def clip_loss_global_only(self, image_features, text_features, rank, image):
        """
        âœ… ä¿®æ”¹ï¼šåªè¿›è¡Œå…¨å±€å¯¹æ¯”å­¦ä¹  (image_embeds â†” global_caption)
        
        Args:
            image_features: å…¨å±€è§†è§‰ç‰¹å¾ [CLS] token (B, D)
            text_features: å…¨å±€æ–‡æœ¬ç‰¹å¾ global_caption (B, D)
            rank: å½“å‰GPUçš„rank
            image: ç”¨äºè·å–device
            
        Returns:
            loss_itcl: å…¨å±€å¯¹æ¯”å­¦ä¹ loss
        """
        # âœ… ä¿®å¤ï¼šæ­£ç¡®å¤„ç†åˆ†å¸ƒå¼å’Œå•GPUç¯å¢ƒ
        if dist.is_available() and dist.is_initialized():
            # åˆ†å¸ƒå¼è®­ç»ƒï¼šè·¨GPU gather
            image_feat_all = torch.cat(nn_dist.all_gather(image_features), dim=0)
            text_feat_all = torch.cat(nn_dist.all_gather(text_features), dim=0)
            world_size = dist.get_world_size()
        else:
            # âœ… å•GPUè®­ç»ƒï¼šä¸éœ€è¦all_gatherï¼Œrankå›ºå®šä¸º0
            image_feat_all = image_features
            text_feat_all = text_features
            rank = 0
            world_size = 1
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_i2t = torch.matmul(image_features, text_feat_all.T)  # (B, world_sizeÃ—B)
        sim_t2i = torch.matmul(image_feat_all, text_features.T)  # (world_sizeÃ—B, B)
        sim_t2i = sim_t2i.T  # (B, world_sizeÃ—B)
        
        # âœ… æ¸©åº¦ç¼©æ”¾ + æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤
        logit_scale_clamped = torch.clamp(self.logit_scale, max=4.6052)  # exp(4.6) â‰ˆ 100
        scale = logit_scale_clamped.exp()
        sim_i2t = scale * sim_i2t
        sim_t2i = scale * sim_t2i
        
        # âœ… æ„å»ºæ ‡ç­¾ï¼ˆå•GPUæ—¶rank=0ï¼Œtargetså°±æ˜¯[0,1,2,...,B-1]ï¼‰
        bs = image_features.size(0)
        targets = torch.linspace(rank * bs, rank * bs + bs - 1, bs, dtype=torch.long).to(image.device)
        
        # InfoNCE Lossï¼ˆåŒå‘å¯¹æ¯”ï¼‰
        loss_i2t = F.cross_entropy(sim_i2t, targets, label_smoothing=0.0)
        loss_t2i = F.cross_entropy(sim_t2i, targets, label_smoothing=0.0)
        
        loss_itcl = (loss_i2t + loss_t2i) / 2
        
        return loss_itcl


    def pairwise_contrastive_loss(self, image_features_long, text_features_long, device, logit_scale=1.0):
        batch_size, c = image_features_long.shape
        
        # âœ… ä¿®å¤ï¼šä½¿ç”¨ç±»åˆ«ç´¢å¼•è€Œéone-hotçŸ©é˜µ
        labels = torch.arange(batch_size, device=device, dtype=torch.long)
        
        # âœ… æ•°å€¼ç¨³å®šæ€§ï¼šclamp logit_scale
        logit_scale_clamped = torch.clamp(logit_scale, max=4.6052)  # exp(4.6) â‰ˆ 100
        scale = logit_scale_clamped.exp()
        
        # ğŸ” è¯¦ç»†è°ƒè¯•ï¼šæ£€æŸ¥çŸ©é˜µä¹˜æ³•è¾“å…¥
        # print(f"    [pairwise] scale={scale.item():.4f}, batch_size={batch_size}", flush=True)
        
        logits_per_image = scale * image_features_long @ text_features_long.T
        logits_per_text = scale * text_features_long @ image_features_long.T
        
        # ğŸ” æ£€æŸ¥ç›¸ä¼¼åº¦çŸ©é˜µ
        # print(f"    [pairwise] logits_per_image: min={logits_per_image.min().item():.4f}, "
        #       f"max={logits_per_image.max().item():.4f}, "
        #       f"has_nan={torch.isnan(logits_per_image).any().item()}", flush=True)
        
        temp1 = F.cross_entropy(logits_per_text, labels)
        temp2 = F.cross_entropy(logits_per_image, labels)

        loss = (temp1+temp2)/2
        return loss
    
    def hard_contrastive_loss(self, image_features_long, text_features_long, device, logit_scale=1.0):
        batch_size, c = image_features_long.shape
        text_features_long = text_features_long.reshape(batch_size, 11, -1)
        labels = torch.zeros(batch_size, device=device, dtype=torch.long)#.repeat(batch_size, 1)
        predict = logit_scale.exp() * torch.einsum('bp,bdp->bd', image_features_long, text_features_long)
        loss = F.cross_entropy(predict, labels)
        return loss
