"""
å®Œæ•´æµ‹è¯•ï¼šåŠ è½½é¢„è®­ç»ƒCLIPæƒé‡ï¼Œæµ‹è¯•è§†é¢‘æ—¶åºå»ºæ¨¡çš„å®Œæ•´pipeline
"""
import torch
import sys
sys.path.insert(0, '/data/zyy/wsvad/2026CVPR/FG-CLIP')

def test_load_pretrained_clip():
    """æµ‹è¯•åŠ è½½é¢„è®­ç»ƒCLIPæ¨¡å‹"""
    print("=" * 60)
    print("æµ‹è¯•1: åŠ è½½é¢„è®­ç»ƒCLIPæƒé‡")
    print("=" * 60)
    
    try:
        # ä½¿ç”¨OpenAI CLIPåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        import fgclip.model.clip.clip as clip
        
        print("æ­£åœ¨ä¸‹è½½/åŠ è½½ CLIP ViT-B/32 æ¨¡å‹...")
        model, preprocess = clip.load("ViT-B/32", device="cpu")
        
        print("âœ… é¢„è®­ç»ƒCLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   - æ¨¡å‹ç±»å‹: {type(model)}")
        print(f"   - è§†è§‰ç¼–ç å™¨: {type(model.visual)}")
        print(f"   - æ–‡æœ¬ç¼–ç å™¨: {type(model.transformer)}")
        print()
        return True, model, preprocess
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, None, None


def test_convert_to_fgclip():
    """æµ‹è¯•å°†OpenAI CLIPè½¬æ¢ä¸ºFGCLIPModel"""
    print("=" * 60)
    print("æµ‹è¯•2: è½¬æ¢ä¸ºFGCLIPModelå¹¶æ·»åŠ æ—¶åºæ¨¡å—")
    print("=" * 60)
    
    try:
        from fgclip.model.clip_strc.fgclip import FGCLIPModel
        from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
        from transformers import AutoTokenizer
        
        # åˆ›å»ºé…ç½®ï¼ˆåŒ¹é…ViT-B/32ï¼‰
        text_config = CLIPTextConfig(
            vocab_size=49408,
            hidden_size=512,
            intermediate_size=2048,
            num_hidden_layers=12,
            num_attention_heads=8,
            max_position_embeddings=77,
        )
        
        vision_config = CLIPVisionConfig(
            hidden_size=768,
            intermediate_size=3072,
            num_hidden_layers=12,
            num_attention_heads=12,
            image_size=224,
            patch_size=32,
        )
        
        config = CLIPConfig(
            text_config=text_config.to_dict(),
            vision_config=vision_config.to_dict(),
            projection_dim=512,
        )
        
        # åˆ›å»ºFGCLIPModelï¼ˆåŒ…å«æˆ‘ä»¬çš„æ—¶åºæ¨¡å—ï¼‰
        print("åˆ›å»ºFGCLIPModel...")
        model = FGCLIPModel(config)
        
        # æ£€æŸ¥æ—¶åºæ¨¡å—
        assert hasattr(model, 'temporal_transformer'), "âŒ ç¼ºå°‘ temporal_transformer"
        assert hasattr(model, 'temporal_attention'), "âŒ ç¼ºå°‘ temporal_attention"
        
        print("âœ… FGCLIPModelåˆ›å»ºæˆåŠŸï¼ŒåŒ…å«æ—¶åºæ¨¡å—")
        print(f"   - temporal_transformer: {model.temporal_transformer}")
        print(f"   - temporal_attention: {model.temporal_attention}")
        print()
        
        return True, model
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False, None


def test_video_forward_with_pretrained():
    """æµ‹è¯•ä½¿ç”¨é¢„è®­ç»ƒæƒé‡çš„è§†é¢‘å‰å‘ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•3: è§†é¢‘å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæƒé‡ç»“æ„ï¼‰")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    # åˆ›å»ºé…ç½®
    text_config = CLIPTextConfig(
        vocab_size=49408,
        hidden_size=512,
        intermediate_size=2048,
        num_hidden_layers=12,
        num_attention_heads=8,
        max_position_embeddings=77,
    )
    
    vision_config = CLIPVisionConfig(
        hidden_size=768,
        intermediate_size=3072,
        num_hidden_layers=12,
        num_attention_heads=12,
        image_size=224,
        patch_size=32,
    )
    
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    
    model = FGCLIPModel(config).eval()
    
    # å‡†å¤‡è§†é¢‘è¾“å…¥
    batch_size = 2
    num_frames = 8
    video = torch.randn(batch_size, num_frames, 3, 224, 224)
    video_attention_mask = torch.ones(batch_size, num_frames, dtype=torch.bool)
    
    print(f"è§†é¢‘è¾“å…¥: {video.shape}")
    print(f"è§†é¢‘æ³¨æ„åŠ›æ©ç : {video_attention_mask.shape}")
    
    try:
        with torch.no_grad():
            # åªæµ‹è¯•è§†è§‰ç¼–ç éƒ¨åˆ†ï¼ˆé¿å…æ–‡æœ¬ä½ç½®ç¼–ç é—®é¢˜ï¼‰
            # æ¨¡æ‹Ÿè§†é¢‘å¤„ç†æµç¨‹
            bs, T, c, h, w = video.shape
            video_flat = video.view(bs * T, c, h, w)
            
            # 1. é€å¸§è§†è§‰ç¼–ç 
            vision_outputs = model.vision_model(pixel_values=video_flat)
            image_embeds_flat = vision_outputs[1]
            image_embeds_flat = model.visual_projection(image_embeds_flat)
            
            # 2. é‡å¡‘ä¸ºæ—¶åº
            image_embeds_temporal = image_embeds_flat.view(bs, T, -1)
            print(f"   âœ“ é€å¸§ç¼–ç : {image_embeds_flat.shape} -> {image_embeds_temporal.shape}")
            
            # 3. æ—¶åºTransformer
            temporal_mask = ~video_attention_mask
            image_embeds_temporal = model.temporal_transformer(
                image_embeds_temporal,
                src_key_padding_mask=temporal_mask
            )
            print(f"   âœ“ æ—¶åºå»ºæ¨¡: {image_embeds_temporal.shape}")
            
            # 4. æ³¨æ„åŠ›åŠ æƒèšåˆ
            attn_weights = model.temporal_attention(image_embeds_temporal)
            attn_weights = attn_weights.masked_fill(~video_attention_mask.unsqueeze(-1), -1e9)
            attn_weights = torch.softmax(attn_weights, dim=1)
            
            # 5. åŠ æƒæ±‚å’Œ
            video_features = (image_embeds_temporal * attn_weights).sum(dim=1)
            video_features = video_features / video_features.norm(p=2, dim=-1, keepdim=True)
            
            print(f"   âœ“ æ³¨æ„åŠ›èšåˆ: {video_features.shape}")
            print(f"   âœ“ ç¬¬ä¸€ä¸ªè§†é¢‘çš„æ³¨æ„åŠ›æƒé‡: {attn_weights[0].squeeze().tolist()}")
            
            print("âœ… è§†é¢‘å‰å‘ä¼ æ’­æˆåŠŸ")
            print()
            return True
            
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_tokenizer():
    """æµ‹è¯•tokenizer"""
    print("=" * 60)
    print("æµ‹è¯•4: CLIP Tokenizer")
    print("=" * 60)
    
    try:
        import fgclip.model.clip.clip as clip
        
        texts = [
            "A person walking on the street",
            "An abnormal event with violence"
        ]
        
        print(f"æµ‹è¯•æ–‡æœ¬: {texts}")
        tokens = clip.tokenize(texts)
        print(f"âœ… TokenizationæˆåŠŸ")
        print(f"   - Token shape: {tokens.shape}")
        print(f"   - TokenèŒƒä¾‹: {tokens[0][:10]}")
        print()
        return True
        
    except Exception as e:
        print(f"âŒ Tokenizationå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_with_mask_variations():
    """æµ‹è¯•ä¸åŒmaskæƒ…å†µ"""
    print("=" * 60)
    print("æµ‹è¯•5: ä¸åŒAttention Maskåœºæ™¯")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    # åˆ›å»ºæ¨¡å‹
    text_config = CLIPTextConfig(vocab_size=49408, hidden_size=512, max_position_embeddings=77)
    vision_config = CLIPVisionConfig(hidden_size=768, image_size=224, patch_size=32)
    config = CLIPConfig(
        text_config=text_config.to_dict(),
        vision_config=vision_config.to_dict(),
        projection_dim=512,
    )
    model = FGCLIPModel(config).eval()
    
    # æµ‹è¯•åœºæ™¯
    test_cases = [
        ("å…¨éƒ¨æœ‰æ•ˆå¸§", torch.ones(2, 8, dtype=torch.bool)),
        ("éƒ¨åˆ†å¡«å……", torch.tensor([[True]*5 + [False]*3, [True]*8], dtype=torch.bool)),
        ("å¤§é‡å¡«å……", torch.tensor([[True]*2 + [False]*6, [True]*3 + [False]*5], dtype=torch.bool)),
    ]
    
    all_passed = True
    for name, mask in test_cases:
        try:
            video = torch.randn(2, 8, 3, 224, 224)
            
            with torch.no_grad():
                bs, T = mask.shape
                video_flat = video.view(bs * T, 3, 224, 224)
                
                vision_outputs = model.vision_model(pixel_values=video_flat)
                features = vision_outputs[1]
                features = model.visual_projection(features)
                features = features.view(bs, T, -1)
                
                temporal_mask = ~mask
                features = model.temporal_transformer(features, src_key_padding_mask=temporal_mask)
                
                attn_weights = model.temporal_attention(features)
                attn_weights = attn_weights.masked_fill(~mask.unsqueeze(-1), -1e9)
                attn_weights = torch.softmax(attn_weights, dim=1)
                
                video_features = (features * attn_weights).sum(dim=1)
                
                print(f"âœ… {name}: æˆåŠŸ")
                print(f"   - Mask: Video0={mask[0].sum().item()}å¸§, Video1={mask[1].sum().item()}å¸§")
                print(f"   - è¾“å‡ºshape: {video_features.shape}")
                
        except Exception as e:
            print(f"âŒ {name}: å¤±è´¥ - {str(e)}")
            all_passed = False
    
    print()
    return all_passed


def test_data_compatibility():
    """æµ‹è¯•ä¸train_fgclip.pyçš„æ•°æ®å…¼å®¹æ€§"""
    print("=" * 60)
    print("æµ‹è¯•6: æ•°æ®åŠ è½½å…¼å®¹æ€§")
    print("=" * 60)
    
    try:
        # æ¨¡æ‹Ÿä»train_fgclip.pyæ¥çš„æ•°æ®æ ¼å¼
        batch = {
            'video': torch.randn(4, 16, 3, 224, 224),  # (B, T, C, H, W)
            'video_attention_mask': torch.ones(4, 16, dtype=torch.bool),
            'text_long': torch.randint(0, 1000, (4, 77)),
            'text_short': torch.randint(0, 1000, (4, 77)),
        }
        
        print(f"æ¨¡æ‹Ÿbatchæ•°æ®:")
        print(f"   - video: {batch['video'].shape}")
        print(f"   - video_attention_mask: {batch['video_attention_mask'].shape}")
        print(f"   - text_long: {batch['text_long'].shape}")
        print(f"   - text_short: {batch['text_short'].shape}")
        
        # æ£€æŸ¥å…¼å®¹æ€§
        from fgclip.model.clip_strc.fgclip import FGCLIPModel
        from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
        
        text_config = CLIPTextConfig(vocab_size=49408, hidden_size=512, max_position_embeddings=77)
        vision_config = CLIPVisionConfig(hidden_size=768, image_size=224, patch_size=32)
        config = CLIPConfig(
            text_config=text_config.to_dict(),
            vision_config=vision_config.to_dict(),
            projection_dim=512,
        )
        model = FGCLIPModel(config).eval()
        
        with torch.no_grad():
            # åªæµ‹è¯•è§†è§‰éƒ¨åˆ†
            video = batch['video']
            mask = batch['video_attention_mask']
            
            bs, T, c, h, w = video.shape
            video_flat = video.view(bs * T, c, h, w)
            
            vision_outputs = model.vision_model(pixel_values=video_flat)
            features = vision_outputs[1]
            features = model.visual_projection(features)
            features = features.view(bs, T, -1)
            
            temporal_mask = ~mask
            features = model.temporal_transformer(features, src_key_padding_mask=temporal_mask)
            
            attn_weights = model.temporal_attention(features)
            attn_weights = attn_weights.masked_fill(~mask.unsqueeze(-1), -1e9)
            attn_weights = torch.softmax(attn_weights, dim=1)
            
            video_features = (features * attn_weights).sum(dim=1)
            
            print("âœ… æ•°æ®æ ¼å¼å…¼å®¹")
            print(f"   - è§†é¢‘ç‰¹å¾è¾“å‡º: {video_features.shape}")
            print()
            return True
            
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("FG-CLIP è§†é¢‘æ—¶åºå»ºæ¨¡å®Œæ•´æµ‹è¯•")
    print("åŒ…å«é¢„è®­ç»ƒæƒé‡åŠ è½½")
    print("=" * 60 + "\n")
    
    results = []
    
    # æµ‹è¯•1: åŠ è½½é¢„è®­ç»ƒCLIP
    success, clip_model, preprocess = test_load_pretrained_clip()
    results.append(("åŠ è½½é¢„è®­ç»ƒCLIP", success))
    
    # æµ‹è¯•2: è½¬æ¢ä¸ºFGCLIPModel
    success, fgclip_model = test_convert_to_fgclip()
    results.append(("è½¬æ¢ä¸ºFGCLIPModel", success))
    
    # æµ‹è¯•3: è§†é¢‘å‰å‘ä¼ æ’­
    results.append(("è§†é¢‘å‰å‘ä¼ æ’­", test_video_forward_with_pretrained()))
    
    # æµ‹è¯•4: Tokenizer
    results.append(("CLIP Tokenizer", test_tokenizer()))
    
    # æµ‹è¯•5: Maskå˜åŒ–
    results.append(("Attention Maskåœºæ™¯", test_with_mask_variations()))
    
    # æµ‹è¯•6: æ•°æ®å…¼å®¹æ€§
    results.append(("æ•°æ®æ ¼å¼å…¼å®¹æ€§", test_data_compatibility()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:<25}: {status}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ¨ å®Œæ•´éªŒè¯ç»“æœ:")
        print("  âœ“ é¢„è®­ç»ƒCLIPæƒé‡å¯æ­£å¸¸åŠ è½½")
        print("  âœ“ FGCLIPModelç»“æ„æ­£ç¡®ï¼ŒåŒ…å«æ—¶åºæ¨¡å—")
        print("  âœ“ è§†é¢‘è¾“å…¥å¤„ç†æ­£å¸¸")
        print("  âœ“ æ—¶åºTransformerå·¥ä½œæ­£å¸¸")
        print("  âœ“ æ³¨æ„åŠ›åŠ æƒèšåˆæ­£ç¡®")
        print("  âœ“ Attention Maskå¤„ç†æ­£ç¡®")
        print("  âœ“ ä¸train_fgclip.pyæ•°æ®æ ¼å¼å…¼å®¹")
        print("\nğŸš€ æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œå®é™…è®­ç»ƒï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    print("=" * 60 + "\n")
