"""
æµ‹è¯•è§†é¢‘æ¨¡å‹çš„æ—¶åºå»ºæ¨¡å’ŒåŠ¨æ€ bbox åŠŸèƒ½
"""
import torch
import sys
sys.path.insert(0, '/data/zyy/wsvad/2026CVPR/FG-CLIP')

def create_test_config():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„é…ç½®"""
    from fgclip.model.clip_strc.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig
    
    # åˆ›å»ºæ–‡æœ¬é…ç½® - ä½¿ç”¨é»˜è®¤çš„77ä¸ªä½ç½®ç¼–ç 
    text_config = CLIPTextConfig(
        vocab_size=49408,
        hidden_size=512,
        intermediate_size=2048,
        num_hidden_layers=12,
        num_attention_heads=8,
        max_position_embeddings=77,  # æ ‡å‡†CLIPé•¿åº¦
        hidden_act="quick_gelu",
        layer_norm_eps=1e-5,
        dropout=0.0,
        attention_dropout=0.0,
        initializer_range=0.02,
        initializer_factor=1.0,
    )
    
    # åˆ›å»ºè§†è§‰é…ç½®
    vision_config = CLIPVisionConfig(
        hidden_size=768,
        intermediate_size=3072,
        num_hidden_layers=12,
        num_attention_heads=12,
        image_size=224,
        patch_size=32,
        hidden_act="quick_gelu",
        layer_norm_eps=1e-5,
        dropout=0.0,
        attention_dropout=0.0,
        initializer_range=0.02,
        initializer_factor=1.0,
    )
    
    # åˆ›å»ºCLIPé…ç½® - éœ€è¦ä¼ å…¥dictæ ¼å¼
    config = CLIPConfig(
        text_config=text_config.to_dict(),  # è½¬ä¸ºdict
        vision_config=vision_config.to_dict(),  # è½¬ä¸ºdict
        projection_dim=512,
        logit_scale_init_value=2.6592,
    )
    
    return config


def create_test_model():
    """åˆ›å»ºå¹¶åˆå§‹åŒ–æµ‹è¯•æ¨¡å‹"""
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config)
    
    # æ³¨æ„ï¼šFG-CLIPéœ€è¦è°ƒç”¨resize_postion_embedingå’Œcopy_weight
    # ä½†è¿™éœ€è¦å·²ç»åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œåœ¨æµ‹è¯•ç¯å¢ƒä¸­æˆ‘ä»¬è·³è¿‡
    # model.resize_postion_embeding()
    # model.copy_weight()
    
    return model


def test_temporal_modeling():
    """æµ‹è¯•æ—¶åºå»ºæ¨¡æ¨¡å—"""
    print("=" * 60)
    print("æµ‹è¯•1: æ—¶åºå»ºæ¨¡æ¨¡å—åˆå§‹åŒ–")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config)
    
    # æ£€æŸ¥æ—¶åºæ¨¡å—æ˜¯å¦æ­£ç¡®æ·»åŠ 
    assert hasattr(model, 'temporal_transformer'), "âŒ ç¼ºå°‘ temporal_transformer"
    assert hasattr(model, 'temporal_attention'), "âŒ ç¼ºå°‘ temporal_attention"
    
    print("âœ… æ—¶åºæ¨¡å—å·²æ­£ç¡®æ·»åŠ :")
    print(f"   - temporal_transformer: {type(model.temporal_transformer)}")
    print(f"   - temporal_attention: {type(model.temporal_attention)}")
    print()


def test_video_forward():
    """æµ‹è¯•è§†é¢‘è¾“å…¥çš„å‰å‘ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•2: è§†é¢‘è¾“å…¥å‰å‘ä¼ æ’­")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config).eval()
    
    # æ¨¡æ‹Ÿè§†é¢‘è¾“å…¥
    batch_size = 2
    num_frames = 8
    channels = 3
    height = 224
    width = 224
    
    video_tensor = torch.randn(batch_size, num_frames, channels, height, width)
    text_short = torch.randint(0, 1000, (batch_size, 77))
    text_long = torch.randint(0, 1000, (batch_size, 77))
    video_attention_mask = torch.ones(batch_size, num_frames, dtype=torch.bool)
    
    print(f"è¾“å…¥è§†é¢‘å½¢çŠ¶: {video_tensor.shape}")
    print(f"è§†é¢‘æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {video_attention_mask.shape}")
    
    try:
        with torch.no_grad():
            output = model(
                image=video_tensor,
                text_short=text_short,
                text_long=text_long,
                video_attention_mask=video_attention_mask,
                add_box_loss=False,
                use_hard_neg=False,
                return_loss=True
            )
        
        print("âœ… è§†é¢‘å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - è¾“å‡º loss: {output.loss}")
        print()
        return True
    except Exception as e:
        print(f"âŒ è§†é¢‘å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_image_backward_compatibility():
    """æµ‹è¯•å›¾åƒè¾“å…¥çš„å‘åå…¼å®¹æ€§"""
    print("=" * 60)
    print("æµ‹è¯•3: å›¾åƒè¾“å…¥å‘åå…¼å®¹æ€§")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config).eval()
    
    # æ¨¡æ‹Ÿå›¾åƒè¾“å…¥ (B, C, H, W)
    batch_size = 2
    image_tensor = torch.randn(batch_size, 3, 224, 224)
    text_short = torch.randint(0, 1000, (batch_size, 77))
    text_long = torch.randint(0, 1000, (batch_size, 77))
    
    print(f"è¾“å…¥å›¾åƒå½¢çŠ¶: {image_tensor.shape}")
    
    try:
        with torch.no_grad():
            output = model(
                image=image_tensor,
                text_short=text_short,
                text_long=text_long,
                add_box_loss=False,
                use_hard_neg=False,
                return_loss=True
            )
        
        print("âœ… å›¾åƒå‰å‘ä¼ æ’­æˆåŠŸ (å‘åå…¼å®¹)")
        print(f"   - è¾“å‡º loss: {output.loss}")
        print()
        return True
    except Exception as e:
        print(f"âŒ å›¾åƒå‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_video_with_bbox():
    """æµ‹è¯•è§†é¢‘ + bbox çš„å‰å‘ä¼ æ’­"""
    print("=" * 60)
    print("æµ‹è¯•4: è§†é¢‘ + BBox Loss")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config).eval()
    
    batch_size = 2
    num_frames = 8
    max_anns = 4
    
    video_tensor = torch.randn(batch_size, num_frames, 3, 224, 224)
    text_short = torch.randint(0, 1000, (batch_size, 77))
    text_long = torch.randint(0, 1000, (batch_size, 77))
    video_attention_mask = torch.ones(batch_size, num_frames, dtype=torch.bool)
    
    # BBox ä¿¡æ¯: (B, max_anns, 4) - å½’ä¸€åŒ–åæ ‡
    box_infos = torch.rand(batch_size, max_anns, 4)
    box_texts = torch.randint(0, 1000, (batch_size * max_anns, 77))
    box_nums = torch.tensor([2, 1], dtype=torch.long)  # ç¬¬ä¸€ä¸ªè§†é¢‘2ä¸ªboxï¼Œç¬¬äºŒä¸ª1ä¸ªbox
    
    print(f"è¾“å…¥è§†é¢‘å½¢çŠ¶: {video_tensor.shape}")
    print(f"BBox ä¿¡æ¯å½¢çŠ¶: {box_infos.shape}")
    print(f"BBox æ•°é‡: {box_nums}")
    
    try:
        with torch.no_grad():
            output = model(
                image=video_tensor,
                text_short=text_short,
                text_long=text_long,
                video_attention_mask=video_attention_mask,
                box_infos=box_infos,
                box_texts=box_texts,
                box_nums=box_nums,
                add_box_loss=True,
                use_hard_neg=False,
                return_loss=True
            )
        
        print("âœ… è§†é¢‘ + BBox å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - è¾“å‡º loss: {output.loss}")
        print()
        return True
    except Exception as e:
        print(f"âŒ è§†é¢‘ + BBox å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_attention_mask():
    """æµ‹è¯•æ³¨æ„åŠ›æ©ç çš„æ­£ç¡®æ€§"""
    print("=" * 60)
    print("æµ‹è¯•5: æ³¨æ„åŠ›æ©ç å¤„ç†")
    print("=" * 60)
    
    from fgclip.model.clip_strc.fgclip import FGCLIPModel
    
    config = create_test_config()
    model = FGCLIPModel(config).eval()
    
    batch_size = 2
    num_frames = 8
    
    video_tensor = torch.randn(batch_size, num_frames, 3, 224, 224)
    text_short = torch.randint(0, 1000, (batch_size, 77))
    text_long = torch.randint(0, 1000, (batch_size, 77))
    
    # æ¨¡æ‹Ÿå¡«å……: ç¬¬ä¸€ä¸ªè§†é¢‘æœ‰5å¸§,ç¬¬äºŒä¸ªè§†é¢‘æœ‰8å¸§
    video_attention_mask = torch.zeros(batch_size, num_frames, dtype=torch.bool)
    video_attention_mask[0, :5] = True
    video_attention_mask[1, :] = True
    
    print(f"è§†é¢‘æ³¨æ„åŠ›æ©ç :")
    print(f"  Video 0: {video_attention_mask[0].sum().item()} æœ‰æ•ˆå¸§ / {num_frames} æ€»å¸§")
    print(f"  Video 1: {video_attention_mask[1].sum().item()} æœ‰æ•ˆå¸§ / {num_frames} æ€»å¸§")
    
    try:
        with torch.no_grad():
            output = model(
                image=video_tensor,
                text_short=text_short,
                text_long=text_long,
                video_attention_mask=video_attention_mask,
                add_box_loss=False,
                use_hard_neg=False,
                return_loss=True
            )
        
        print("âœ… æ³¨æ„åŠ›æ©ç å¤„ç†æˆåŠŸ")
        print(f"   - è¾“å‡º loss: {output.loss}")
        print()
        return True
    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›æ©ç å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("è§†é¢‘æ¨¡å‹æ—¶åºå»ºæ¨¡åŠŸèƒ½æµ‹è¯•")
    print("=" * 60 + "\n")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("æ—¶åºæ¨¡å—åˆå§‹åŒ–", test_temporal_modeling()))
    results.append(("è§†é¢‘å‰å‘ä¼ æ’­", test_video_forward()))
    results.append(("å›¾åƒå‘åå…¼å®¹", test_image_backward_compatibility()))
    results.append(("è§†é¢‘+BBox", test_video_with_bbox()))
    results.append(("æ³¨æ„åŠ›æ©ç ", test_attention_mask()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:<20}: {status}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹ä¿®æ”¹æ­£ç¡®ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    print("=" * 60 + "\n")
